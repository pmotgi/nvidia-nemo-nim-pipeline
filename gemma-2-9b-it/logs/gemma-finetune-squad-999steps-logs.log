/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[NeMo W 2025-10-06 22:49:18 nemo_logging:405] /opt/venv/lib/python3.12/site-packages/lhotse/recipes/iwslt22_ta.py:323: SyntaxWarning: invalid escape sequence '\s'
      text = re.sub("\s+", " ", text)
    
[NeMo W 2025-10-06 22:49:18 nemo_logging:405] /opt/venv/lib/python3.12/site-packages/lhotse/recipes/iwslt22_ta.py:324: SyntaxWarning: invalid escape sequence '\s'
      text = re.sub("\s+\.\s+", ".", text)
    
Configuring global options
Dry run for task nemo.collections.llm.api:finetune
Resolved Arguments
┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Argument Name    ┃ Resolved Value                                            ┃
┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ data             │ SquadDataModule(seq_length=4096, micro_batch_size=2,      │
│                  │ global_batch_size=16)                                     │
│ log              │ NeMoLogger(                                               │
│                  │   name='default',                                         │
│                  │   log_dir='/data/nvidia-pipeline/finetuned-models/gemma2… │
│                  │   ckpt=ModelCheckpoint(                                   │
│                  │     save_last='link',                                     │
│                  │     save_top_k=2,                                         │
│                  │     every_n_train_steps=50,                               │
│                  │     filename='{model_name}--{val_loss:.2f}-{step}-{consu… │
│                  │   tensorboard=TensorBoardLogger(save_dir='tb_logs',       │
│                  │ name='default'),                                          │
│                  │   wandb=None)                                             │
│ model            │ Gemma2Model(config=Gemma2Config9B())                      │
│ optim            │ MegatronOptimizerModule(                                  │
│                  │   config=OptimizerConfig(                                 │
│                  │     optimizer='adam',                                     │
│                  │     lr=0.0001,                                            │
│                  │     weight_decay=0.1,                                     │
│                  │     fp16=False,                                           │
│                  │     bf16=True,                                            │
│                  │     adam_beta1=0.9,                                       │
│                  │     adam_beta2=0.98,                                      │
│                  │     adam_eps=1e-05,                                       │
│                  │     use_distributed_optimizer=True,                       │
│                  │     clip_grad=1.0),                                       │
│                  │   lr_scheduler=CosineAnnealingScheduler(warmup_steps=50,  │
│                  │ constant_steps=0, min_lr=0))                              │
│ peft             │ LoRA()                                                    │
│ resume           │ AutoResume(                                               │
│                  │   restore_config=RestoreConfig(path='nemo://google/gemma… │
│ tokenizer        │ 'model'                                                   │
│ trainer          │ Trainer(                                                  │
│                  │   accelerator='gpu',                                      │
│                  │   strategy=MegatronStrategy(                              │
│                  │     tensor_model_parallel_size=1,                         │
│                  │     pipeline_model_parallel_size=1,                       │
│                  │     virtual_pipeline_model_parallel_size=None,            │
│                  │     context_parallel_size=1,                              │
│                  │     sequence_parallel=False,                              │
│                  │     pipeline_dtype=torch.bfloat16,                        │
│                  │     ckpt_load_strictness='log_all',                       │
│                  │     gradient_as_bucket_view=True),                        │
│                  │   devices='1',                                            │
│                  │   num_nodes=1,                                            │
│                  │   callbacks=[TimingCallback()],                           │
│                  │   max_steps=999,                                          │
│                  │   limit_val_batches=None,                                 │
│                  │   limit_test_batches=None,                                │
│                  │   val_check_interval=30,                                  │
│                  │   log_every_n_steps=1,                                    │
│                  │   accumulate_grad_batches=1,                              │
│                  │   use_distributed_sampler=False,                          │
│                  │   plugins=MegatronMixedPrecision(                         │
│                  │     precision='bf16-mixed',                               │
│                  │     params_dtype=torch.bfloat16,                          │
│                  │     pipeline_dtype=torch.bfloat16,                        │
│                  │     autocast_enabled=False,                               │
│                  │     grad_reduce_in_fp32=True))                            │
└──────────────────┴───────────────────────────────────────────────────────────┘
Launching finetune...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[NeMo I 2025-10-06 22:49:22 nemo_logging:393] Disabling try_restore_best_ckpt restoration for adapters
[NeMo I 2025-10-06 22:49:22 nemo_logging:393] Experiments will be logged at /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22
[NeMo W 2025-10-06 22:49:22 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/tb_logs
[NeMo W 2025-10-06 22:49:22 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.
[NeMo W 2025-10-06 22:49:22 nemo_logging:405] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 999. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has data parallel group : [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All context parallel group ranks: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has model parallel group: [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All model parallel group ranks: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All tensor model parallel group ranks: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has embedding group: [0]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All pipeline model parallel group ranks: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] All embedding group ranks: [[0]]
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Rank 0 has embedding rank: 0
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Bootstrap: Using eth0:10.236.1.13<0>
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO cudaDriverVersion 13000
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Comm config Blocking set to 1
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v10 (v10)
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/Plugin: Loaded collnet plugin NCCL RDMA Plugin v10 (v10)
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Successfully loaded external plugin libnccl-net.so
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO P2P plugin v10 IBext_v10
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/IB : No device found.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.236.1.13<0>
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/IB : No device found.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.236.1.13<0>
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO NET/Socket : Using [0]eth0:10.236.1.13<0>
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Initialized NET plugin Socket
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Assigned NET plugin Socket to comm
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Using network Socket
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO ncclCommInitRankConfig comm 0x3c5f1210 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x650117bbca506285 - Init START
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Bootstrap timings total 0.012473 (create 0.000019, send 0.012160, recv 0.000076, ring 0.000001, delay 0.000000)
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Setting affinity for GPU 0 to 0-47
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO comm 0x3c5f1210 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 00/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 01/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 02/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 03/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 04/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 05/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 06/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 07/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 08/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 09/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 10/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 11/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 12/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 13/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 14/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 15/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 16/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 17/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 18/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 19/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 20/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 21/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 22/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 23/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 24/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 25/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 26/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 27/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 28/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 29/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 30/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 31/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 32/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 33/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 34/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 35/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 36/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 37/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 38/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 39/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 40/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 41/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 42/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 43/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 44/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 45/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 46/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 47/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 48/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 49/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 50/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 51/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 52/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 53/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 54/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 55/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 56/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 57/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 58/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 59/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 60/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 61/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 62/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Channel 63/64 : 0
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO P2P Chunksize set to 524288
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
nemo-gpu-lora-job-worker-0-0:76:458 [0] NCCL INFO [Proxy Service] Device 0 CPU core 26
nemo-gpu-lora-job-worker-0-0:76:459 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 32
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO CC Off, workFifoBytes 1048576
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO ncclCommInitRankConfig comm 0x3c5f1210 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x650117bbca506285 - Init COMPLETE
nemo-gpu-lora-job-worker-0-0:76:456 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.20 (kernels 0.10, alloc 0.08, bootstrap 0.01, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[NeMo I 2025-10-06 22:49:24 nemo_logging:393] Downloading SquadDataModule...
Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 1223218.23 examples/s]
Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 1174498.46 examples/s]
[NeMo I 2025-10-06 22:49:28 nemo_logging:393] Preprocessing SquadDataModule to jsonl format and splitting...
[NeMo I 2025-10-06 22:49:32 nemo_logging:393] training split saved to /root/.cache/nemo/datasets/squad/training.jsonl
[NeMo I 2025-10-06 22:49:32 nemo_logging:393] validation split saved to /root/.cache/nemo/datasets/squad/validation.jsonl
[NeMo I 2025-10-06 22:49:32 nemo_logging:393] test split saved to /root/.cache/nemo/datasets/squad/test.jsonl
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.FITTING
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Found model_transform attribute on pl_module
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x7b4de5ad42c0>
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Use preset vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Copying Trainer's 'max_steps' (999) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-06 22:49:34 num_microbatches_calculator:228] setting number of microbatches to constant 8
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Doing selective restore from RestoreConfig(path='/data/nvidia-pipeline/nemo-models/google/gemma-2-9b', load_model_state=True, load_optim_state=False, load_artifacts=True)
[NeMo W 2025-10-06 22:49:34 serialization:184] DEPRECATED: Passing 'checkpoint_dir' as a Path object in load_common_state_dict will no longer be supported in a future release. Please pass it as a string instead.
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Loaded sharded_state_dict_metadata from checkpoint: {'distrib_optim_sharding_type': 'fully_sharded_model_space'}
[NeMo I 2025-10-06 22:49:34 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7b4d0c0d3590> dist-ckpt load strategy.
[NeMo I 2025-10-06 22:55:20 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1759790974.529s : Time spent in load_checkpoint: 346.158s
[NeMo I 2025-10-06 22:55:20 nemo_logging:393] Restoring model weights from RestoreConfig(path='/data/nvidia-pipeline/nemo-models/google/gemma-2-9b', load_model_state=True, load_optim_state=False, load_artifacts=True)
[NeMo I 2025-10-06 22:55:20 nemo_logging:393] Finished restoring from RestoreConfig(path='/data/nvidia-pipeline/nemo-models/google/gemma-2-9b', load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.

  | Name   | Type     | Params | Mode 
--------------------------------------------
0 | module | GPTModel | 9.2 B  | train
--------------------------------------------
9.2 B     Trainable params
0         Non-trainable params
9.2 B     Total params
36,966.824Total estimated model params size (MB)
723       Modules in train mode
0         Modules in eval mode
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.32.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.32.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.32.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.32.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.33.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.33.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.33.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.33.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.34.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.34.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.34.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.34.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.35.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.35.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.35.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.35.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.36.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.36.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.36.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.36.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.37.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.37.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.37.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.37.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.38.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.38.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.38.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.38.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.39.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.39.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.39.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.39.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.40.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.40.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.40.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.40.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.41.self_attention.linear_proj
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.41.self_attention.linear_qkv
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.41.mlp.linear_fc1
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Adding lora to: module.decoder.layers.41.mlp.linear_fc2
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] After applying model_transform:
    
      | Name   | Type     | Params | Mode 
    --------------------------------------------
    0 | module | GPTModel | 9.3 B  | train
    --------------------------------------------
    93.6 M    Trainable params
    9.2 B     Non-trainable params
    9.3 B     Total params
    37,341.166Total estimated model params size (MB)
    1563      Modules in train mode
    0         Modules in eval mode
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Initializing model parallel
[NeMo I 2025-10-06 22:55:21 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 9335291392
[NeMo I 2025-10-06 22:55:21 nemo_logging:393]  > number of trainable parameters: 93585408 (1.00% of total)
[NeMo I 2025-10-06 22:55:21 utils:673] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_megatron_fsdp=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache=False, nccl_ub=False, fsdp_double_buffer=False, outer_dp_sharding_strategy='no_shard', delay_wgrad_compute=False)
[NeMo I 2025-10-06 22:55:21 utils:694] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (93585408 elements, 93585408 padded size):
    	module.decoder.layers.25.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.5.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.41.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.20.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.36.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.16.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.8.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.0.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.3.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.30.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.18.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.33.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.13.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.7.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.41.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.20.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.36.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.28.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.16.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.8.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.0.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.23.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.3.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.38.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.30.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.18.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.10.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.34.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.26.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.21.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.36.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.28.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.15.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.23.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.38.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.31.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.18.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.10.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.33.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.26.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.5.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.41.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.21.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.15.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.8.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.3.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.39.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.31.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.18.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.11.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.26.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.13.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.5.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.41.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.36.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.28.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.8.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.3.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.23.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.39.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.19.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.11.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.26.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.6.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.21.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.36.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.16.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.23.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.39.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.31.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.19.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.11.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.6.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.34.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.26.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.13.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.41.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.21.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.29.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.8.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.1.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.0.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.24.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.3.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.39.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.31.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.18.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.11.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.34.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.27.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.13.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.6.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.41.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.21.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.36.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.29.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.16.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.8.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.1.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.24.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.3.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.18.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.34.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.26.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.14.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.6.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.21.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.1.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.36.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.29.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.16.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.9.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.4.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.39.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.31.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.11.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.26.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.14.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.22.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.2.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.37.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.29.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.9.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.0.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.24.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.4.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.39.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.19.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.11.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.34.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.14.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.6.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.22.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.1.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.37.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.29.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.16.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.9.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.0.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.24.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.4.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.32.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.6.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.34.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.26.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.14.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.0.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.21.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.37.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.30.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.16.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.9.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.1.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.24.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.4.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.39.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.32.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.19.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.11.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.27.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.13.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.21.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.37.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.29.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.17.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.24.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.39.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.32.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.19.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.11.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.34.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.27.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.6.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.7.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.29.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.17.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.9.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.25.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.4.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.40.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.32.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.12.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.34.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.27.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.14.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.6.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.22.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.1.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.37.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.17.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.9.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.25.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.4.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.32.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.40.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.19.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.12.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.35.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.14.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.7.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.2.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.37.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.29.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.17.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.24.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.33.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.40.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.19.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.12.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.35.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.27.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.7.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.22.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.2.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.30.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.16.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.9.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.4.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.24.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.40.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.32.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.20.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.12.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.35.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.27.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.14.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.7.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.22.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.2.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.9.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.37.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.30.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.4.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.32.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.20.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.35.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.14.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.7.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.37.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.30.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.17.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.13.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.5.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.25.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.40.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.20.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.12.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.28.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.15.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.22.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.2.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.38.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.17.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.10.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.25.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.5.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.40.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.32.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.20.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.12.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.35.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.28.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.15.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.7.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.22.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.2.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.38.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.30.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.10.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.25.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.5.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.33.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.19.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.35.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.28.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.23.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.38.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.30.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.17.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.10.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.5.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.40.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.33.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.12.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.27.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.15.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.0.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.23.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.2.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.38.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.17.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.10.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.25.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.40.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.33.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.20.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.12.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.35.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.27.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.15.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.7.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.23.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.2.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.31.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.18.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.25.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.41.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.33.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.20.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.13.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.35.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.28.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.15.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.8.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.1.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.23.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.3.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.38.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.31.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.18.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.10.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.5.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.41.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.33.self_attention.linear_qkv.adapter.linear_in.weight
    	module.decoder.layers.13.self_attention.linear_qkv.adapter.linear_out.weight
    	module.decoder.layers.36.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.28.mlp.linear_fc2.adapter.linear_in.weight
    	module.decoder.layers.15.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.8.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.1.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.0.mlp.linear_fc1.adapter.linear_in.weight
    	module.decoder.layers.22.mlp.linear_fc2.adapter.linear_out.weight
    	module.decoder.layers.3.self_attention.linear_proj.adapter.linear_out.weight
    	module.decoder.layers.38.mlp.linear_fc1.adapter.linear_out.weight
    	module.decoder.layers.31.self_attention.linear_proj.adapter.linear_in.weight
    	module.decoder.layers.10.mlp.linear_fc1.adapter.linear_out.weight
[NeMo I 2025-10-06 22:55:21 nemo_logging:393] Setting up optimizers
[NeMo I 2025-10-06 22:55:21 utils:673] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, use_megatron_fsdp=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Comm config Blocking set to 1
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Assigned NET plugin Socket to comm
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Using network Socket
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO ncclCommInitRankConfig comm 0x61887a80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x958bc5ca2e0677cf - Init START
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Bootstrap timings total 0.000259 (create 0.000029, send 0.000088, recv 0.000074, ring 0.000001, delay 0.000000)
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Setting affinity for GPU 0 to 0-47
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO comm 0x61887a80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 00/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 01/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 02/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 03/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 04/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 05/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 06/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 07/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 08/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 09/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 10/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 11/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 12/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 13/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 14/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 15/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 16/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 17/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 18/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 19/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 20/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 21/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 22/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 23/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 24/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 25/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 26/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 27/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 28/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 29/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 30/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 31/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 32/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 33/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 34/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 35/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 36/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 37/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 38/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 39/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 40/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 41/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 42/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 43/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 44/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 45/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 46/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 47/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 48/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 49/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 50/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 51/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 52/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 53/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 54/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 55/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 56/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 57/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 58/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 59/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 60/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 61/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 62/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Channel 63/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO P2P Chunksize set to 524288
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
nemo-gpu-lora-job-worker-0-0:76:1244 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
nemo-gpu-lora-job-worker-0-0:76:1245 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO CC Off, workFifoBytes 1048576
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO ncclCommInitRankConfig comm 0x61887a80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x958bc5ca2e0677cf - Init COMPLETE
nemo-gpu-lora-job-worker-0-0:76:1243 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00)
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Comm config Blocking set to 1
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Assigned NET plugin Socket to comm
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Using network Socket
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO ncclCommInitRankConfig comm 0x62827e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x8311089e66643265 - Init START
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Bootstrap timings total 0.000393 (create 0.000032, send 0.000098, recv 0.000175, ring 0.000001, delay 0.000000)
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Setting affinity for GPU 0 to 0-47
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO comm 0x62827e60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 00/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 01/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 02/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 03/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 04/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 05/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 06/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 07/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 08/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 09/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 10/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 11/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 12/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 13/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 14/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 15/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 16/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 17/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 18/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 19/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 20/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 21/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 22/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 23/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 24/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 25/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 26/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 27/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 28/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 29/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 30/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 31/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 32/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 33/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 34/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 35/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 36/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 37/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 38/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 39/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 40/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 41/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 42/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 43/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 44/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 45/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 46/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 47/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 48/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 49/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 50/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 51/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 52/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 53/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 54/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 55/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 56/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 57/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 58/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 59/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 60/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 61/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 62/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Channel 63/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO P2P Chunksize set to 524288
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
nemo-gpu-lora-job-worker-0-0:76:1347 [0] NCCL INFO [Proxy Service] Device 0 CPU core 46
nemo-gpu-lora-job-worker-0-0:76:1348 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 44
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO CC Off, workFifoBytes 1048576
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO ncclCommInitRankConfig comm 0x62827e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x8311089e66643265 - Init COMPLETE
nemo-gpu-lora-job-worker-0-0:76:1346 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00)
[NeMo W 2025-10-06 22:55:27 nemo_logging:405] /opt/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-10-06 22:55:27 nemo_logging:405] /opt/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Comm config Blocking set to 1
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Assigned NET plugin Socket to comm
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Using network Socket
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO ncclCommInitRankConfig comm 0x62be01d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x10fcbafed2149d8a - Init START
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Bootstrap timings total 0.000328 (create 0.000041, send 0.000105, recv 0.000090, ring 0.000001, delay 0.000000)
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Setting affinity for GPU 0 to 0-47
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO comm 0x62be01d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 00/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 01/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 02/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 03/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 04/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 05/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 06/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 07/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 08/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 09/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 10/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 11/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 12/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 13/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 14/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 15/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 16/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 17/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 18/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 19/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 20/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 21/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 22/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 23/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 24/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 25/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 26/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 27/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 28/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 29/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 30/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 31/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 32/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 33/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 34/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 35/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 36/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 37/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 38/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 39/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 40/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 41/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 42/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 43/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 44/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 45/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 46/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 47/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 48/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 49/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 50/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 51/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 52/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 53/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 54/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 55/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 56/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 57/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 58/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 59/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 60/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 61/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 62/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Channel 63/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO P2P Chunksize set to 524288
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
nemo-gpu-lora-job-worker-0-0:76:1464 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
nemo-gpu-lora-job-worker-0-0:76:1465 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO CC Off, workFifoBytes 1048576
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO ncclCommInitRankConfig comm 0x62be01d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0x10fcbafed2149d8a - Init COMPLETE
nemo-gpu-lora-job-worker-0-0:76:1463 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00)
[NeMo W 2025-10-06 22:55:31 rerun_state_machine:1300] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-06 22:55:31 rerun_state_machine:238] RerunStateMachine initialized in mode RerunMode.DISABLED
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO Comm config Blocking set to 1
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Assigned NET plugin Socket to comm
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Using network Socket
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO ncclCommInitRankConfig comm 0x64491bb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0xce8ba138ca921dcc - Init START
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Bootstrap timings total 0.000386 (create 0.000030, send 0.000091, recv 0.000153, ring 0.000001, delay 0.000000)
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Setting affinity for GPU 0 to 0-47
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO comm 0x64491bb0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 00/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 01/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 02/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 03/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 04/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 05/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 06/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 07/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 08/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 09/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 10/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 11/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 12/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 13/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 14/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 15/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 16/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 17/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 18/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 19/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 20/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 21/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 22/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 23/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 24/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 25/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 26/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 27/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 28/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 29/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 30/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 31/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 32/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 33/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 34/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 35/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 36/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 37/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 38/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 39/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 40/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 41/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 42/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 43/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 44/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 45/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 46/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 47/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 48/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 49/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 50/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 51/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 52/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 53/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 54/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 55/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 56/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 57/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 58/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 59/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 60/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 61/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 62/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Channel 63/64 : 0
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO P2P Chunksize set to 524288
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
nemo-gpu-lora-job-worker-0-0:76:1941 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 17
nemo-gpu-lora-job-worker-0-0:76:1940 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO CC Off, workFifoBytes 1048576
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO ncclCommInitRankConfig comm 0x64491bb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 5000 commId 0xce8ba138ca921dcc - Init COMPLETE
nemo-gpu-lora-job-worker-0-0:76:1939 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00)
Training epoch 0, iteration 0/998 | lr: 1.961e-06 | global_batch_size: 16 | global_step: 0 | reduced_train_loss: 4.457 | train_step_timing in s: 3.165
Training epoch 0, iteration 1/998 | lr: 3.922e-06 | global_batch_size: 16 | global_step: 1 | reduced_train_loss: 3.938 | train_step_timing in s: 2.739 | consumed_samples: 32
Training epoch 0, iteration 2/998 | lr: 5.882e-06 | global_batch_size: 16 | global_step: 2 | reduced_train_loss: 2.742 | train_step_timing in s: 1.285 | consumed_samples: 48
Training epoch 0, iteration 3/998 | lr: 7.843e-06 | global_batch_size: 16 | global_step: 3 | reduced_train_loss: 3.636 | train_step_timing in s: 1.292 | consumed_samples: 64
Training epoch 0, iteration 4/998 | lr: 9.804e-06 | global_batch_size: 16 | global_step: 4 | reduced_train_loss: 3.416 | train_step_timing in s: 1.242 | consumed_samples: 80
Training epoch 0, iteration 5/998 | lr: 1.176e-05 | global_batch_size: 16 | global_step: 5 | reduced_train_loss: 4.303 | train_step_timing in s: 1.283 | consumed_samples: 96
Training epoch 0, iteration 6/998 | lr: 1.373e-05 | global_batch_size: 16 | global_step: 6 | reduced_train_loss: 2.225 | train_step_timing in s: 1.28 | consumed_samples: 112
Training epoch 0, iteration 7/998 | lr: 1.569e-05 | global_batch_size: 16 | global_step: 7 | reduced_train_loss: 2.248 | train_step_timing in s: 1.329 | consumed_samples: 128
Training epoch 0, iteration 8/998 | lr: 1.765e-05 | global_batch_size: 16 | global_step: 8 | reduced_train_loss: 2.397 | train_step_timing in s: 1.238 | consumed_samples: 144
Training epoch 0, iteration 9/998 | lr: 1.961e-05 | global_batch_size: 16 | global_step: 9 | reduced_train_loss: 1.446 | train_step_timing in s: 1.35 | consumed_samples: 160
Training epoch 0, iteration 10/998 | lr: 2.157e-05 | global_batch_size: 16 | global_step: 10 | reduced_train_loss: 1.146 | train_step_timing in s: 1.292 | consumed_samples: 176
Training epoch 0, iteration 11/998 | lr: 2.353e-05 | global_batch_size: 16 | global_step: 11 | reduced_train_loss: 1.523 | train_step_timing in s: 1.742 | consumed_samples: 192
Training epoch 0, iteration 12/998 | lr: 2.549e-05 | global_batch_size: 16 | global_step: 12 | reduced_train_loss: 1.084 | train_step_timing in s: 1.287 | consumed_samples: 208
Training epoch 0, iteration 13/998 | lr: 2.745e-05 | global_batch_size: 16 | global_step: 13 | reduced_train_loss: 0.9261 | train_step_timing in s: 1.272 | consumed_samples: 224
Training epoch 0, iteration 14/998 | lr: 2.941e-05 | global_batch_size: 16 | global_step: 14 | reduced_train_loss: 0.8549 | train_step_timing in s: 1.231 | consumed_samples: 240
Training epoch 0, iteration 15/998 | lr: 3.137e-05 | global_batch_size: 16 | global_step: 15 | reduced_train_loss: 0.6158 | train_step_timing in s: 1.287 | consumed_samples: 256
Training epoch 0, iteration 16/998 | lr: 3.333e-05 | global_batch_size: 16 | global_step: 16 | reduced_train_loss: 0.5439 | train_step_timing in s: 1.234 | consumed_samples: 272
Training epoch 0, iteration 17/998 | lr: 3.529e-05 | global_batch_size: 16 | global_step: 17 | reduced_train_loss: 0.4544 | train_step_timing in s: 1.243 | consumed_samples: 288
Training epoch 0, iteration 18/998 | lr: 3.725e-05 | global_batch_size: 16 | global_step: 18 | reduced_train_loss: 0.576 | train_step_timing in s: 1.228 | consumed_samples: 304
Training epoch 0, iteration 19/998 | lr: 3.922e-05 | global_batch_size: 16 | global_step: 19 | reduced_train_loss: 0.6682 | train_step_timing in s: 1.748 | consumed_samples: 320
Training epoch 0, iteration 20/998 | lr: 4.118e-05 | global_batch_size: 16 | global_step: 20 | reduced_train_loss: 0.6049 | train_step_timing in s: 1.281 | consumed_samples: 336
Training epoch 0, iteration 21/998 | lr: 4.314e-05 | global_batch_size: 16 | global_step: 21 | reduced_train_loss: 0.4133 | train_step_timing in s: 1.233 | consumed_samples: 352
Training epoch 0, iteration 22/998 | lr: 4.51e-05 | global_batch_size: 16 | global_step: 22 | reduced_train_loss: 0.8986 | train_step_timing in s: 1.836 | consumed_samples: 368
Training epoch 0, iteration 23/998 | lr: 4.706e-05 | global_batch_size: 16 | global_step: 23 | reduced_train_loss: 0.5915 | train_step_timing in s: 2.15 | consumed_samples: 384
Training epoch 0, iteration 24/998 | lr: 4.902e-05 | global_batch_size: 16 | global_step: 24 | reduced_train_loss: 0.4527 | train_step_timing in s: 1.2 | consumed_samples: 400
Training epoch 0, iteration 25/998 | lr: 5.098e-05 | global_batch_size: 16 | global_step: 25 | reduced_train_loss: 0.6371 | train_step_timing in s: 1.989 | consumed_samples: 416
Training epoch 0, iteration 26/998 | lr: 5.294e-05 | global_batch_size: 16 | global_step: 26 | reduced_train_loss: 0.4217 | train_step_timing in s: 1.224 | consumed_samples: 432
Training epoch 0, iteration 27/998 | lr: 5.49e-05 | global_batch_size: 16 | global_step: 27 | reduced_train_loss: 0.465 | train_step_timing in s: 1.232 | consumed_samples: 448
Training epoch 0, iteration 28/998 | lr: 5.686e-05 | global_batch_size: 16 | global_step: 28 | reduced_train_loss: 0.3236 | train_step_timing in s: 1.332 | consumed_samples: 464
Training epoch 0, iteration 29/998 | lr: 5.882e-05 | global_batch_size: 16 | global_step: 29 | reduced_train_loss: 0.175 | train_step_timing in s: 1.691 | consumed_samples: 480
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:02:03 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[NeMo I 2025-10-06 23:02:13 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 30 : Start time: 1759791723.382s : Save duration: 10.114s
[NeMo I 2025-10-06 23:02:17 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=29-consumed_samples=480.0-last.ckpt
Training epoch 0, iteration 30/998 | lr: 6.078e-05 | global_batch_size: 16 | global_step: 30 | reduced_train_loss: 0.2923 | train_step_timing in s: 1.839 | consumed_samples: 496 | val_loss: 0.3666
[NeMo I 2025-10-06 23:02:19 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 31/998 | lr: 6.275e-05 | global_batch_size: 16 | global_step: 31 | reduced_train_loss: 0.2372 | train_step_timing in s: 1.982 | consumed_samples: 512 | val_loss: 0.3666
[NeMo I 2025-10-06 23:02:23 nemo_logging:393] Successfully saved checkpoint from iteration      30 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=29-consumed_samples=480.0-last.ckpt
[NeMo I 2025-10-06 23:02:24 nemo_logging:393] Async checkpoint save for step 30 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=29-consumed_samples=480.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:02:24 nemo_logging:393] Async finalization time took 0.989 s
Training epoch 0, iteration 32/998 | lr: 6.471e-05 | global_batch_size: 16 | global_step: 32 | reduced_train_loss: 0.4537 | train_step_timing in s: 1.319 | consumed_samples: 528 | val_loss: 0.3666
Training epoch 0, iteration 33/998 | lr: 6.667e-05 | global_batch_size: 16 | global_step: 33 | reduced_train_loss: 0.4187 | train_step_timing in s: 1.261 | consumed_samples: 544 | val_loss: 0.3666
Training epoch 0, iteration 34/998 | lr: 6.863e-05 | global_batch_size: 16 | global_step: 34 | reduced_train_loss: 0.2957 | train_step_timing in s: 1.228 | consumed_samples: 560 | val_loss: 0.3666
Training epoch 0, iteration 35/998 | lr: 7.059e-05 | global_batch_size: 16 | global_step: 35 | reduced_train_loss: 0.2608 | train_step_timing in s: 1.259 | consumed_samples: 576 | val_loss: 0.3666
Training epoch 0, iteration 36/998 | lr: 7.255e-05 | global_batch_size: 16 | global_step: 36 | reduced_train_loss: 0.3148 | train_step_timing in s: 1.22 | consumed_samples: 592 | val_loss: 0.3666
Training epoch 0, iteration 37/998 | lr: 7.451e-05 | global_batch_size: 16 | global_step: 37 | reduced_train_loss: 0.3651 | train_step_timing in s: 1.522 | consumed_samples: 608 | val_loss: 0.3666
Training epoch 0, iteration 38/998 | lr: 7.647e-05 | global_batch_size: 16 | global_step: 38 | reduced_train_loss: 0.3244 | train_step_timing in s: 1.189 | consumed_samples: 624 | val_loss: 0.3666
Training epoch 0, iteration 39/998 | lr: 7.843e-05 | global_batch_size: 16 | global_step: 39 | reduced_train_loss: 0.1785 | train_step_timing in s: 1.191 | consumed_samples: 640 | val_loss: 0.3666
Training epoch 0, iteration 40/998 | lr: 8.039e-05 | global_batch_size: 16 | global_step: 40 | reduced_train_loss: 0.1604 | train_step_timing in s: 1.268 | consumed_samples: 656 | val_loss: 0.3666
Training epoch 0, iteration 41/998 | lr: 8.235e-05 | global_batch_size: 16 | global_step: 41 | reduced_train_loss: 0.3278 | train_step_timing in s: 1.718 | consumed_samples: 672 | val_loss: 0.3666
Training epoch 0, iteration 42/998 | lr: 8.431e-05 | global_batch_size: 16 | global_step: 42 | reduced_train_loss: 0.2614 | train_step_timing in s: 1.261 | consumed_samples: 688 | val_loss: 0.3666
Training epoch 0, iteration 43/998 | lr: 8.627e-05 | global_batch_size: 16 | global_step: 43 | reduced_train_loss: 0.2493 | train_step_timing in s: 1.271 | consumed_samples: 704 | val_loss: 0.3666
Training epoch 0, iteration 44/998 | lr: 8.824e-05 | global_batch_size: 16 | global_step: 44 | reduced_train_loss: 0.2473 | train_step_timing in s: 1.225 | consumed_samples: 720 | val_loss: 0.3666
Training epoch 0, iteration 45/998 | lr: 9.02e-05 | global_batch_size: 16 | global_step: 45 | reduced_train_loss: 0.2585 | train_step_timing in s: 1.319 | consumed_samples: 736 | val_loss: 0.3666
Training epoch 0, iteration 46/998 | lr: 9.216e-05 | global_batch_size: 16 | global_step: 46 | reduced_train_loss: 0.4604 | train_step_timing in s: 1.271 | consumed_samples: 752 | val_loss: 0.3666
Training epoch 0, iteration 47/998 | lr: 9.412e-05 | global_batch_size: 16 | global_step: 47 | reduced_train_loss: 0.2467 | train_step_timing in s: 1.386 | consumed_samples: 768 | val_loss: 0.3666
Training epoch 0, iteration 48/998 | lr: 9.608e-05 | global_batch_size: 16 | global_step: 48 | reduced_train_loss: 0.1621 | train_step_timing in s: 1.197 | consumed_samples: 784 | val_loss: 0.3666
Training epoch 0, iteration 49/998 | lr: 9.804e-05 | global_batch_size: 16 | global_step: 49 | reduced_train_loss: 0.4527 | train_step_timing in s: 1.333 | consumed_samples: 800 | val_loss: 0.3666
Epoch 0, global step 49: 'val_loss' reached 0.36660 (best 0.36660), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=49-consumed_samples=800.0.ckpt' as top 2
[NeMo I 2025-10-06 23:03:39 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 49 : Start time: 1759791819.412s : Save duration: 0.567s
[NeMo I 2025-10-06 23:03:43 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=49-consumed_samples=800.0.ckpt
[NeMo I 2025-10-06 23:03:43 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 50/998 | lr: 0.0001 | global_batch_size: 16 | global_step: 50 | reduced_train_loss: 0.208 | train_step_timing in s: 2.526 | consumed_samples: 816 | val_loss: 0.3666
[NeMo I 2025-10-06 23:03:48 nemo_logging:393] Successfully saved checkpoint from iteration      49 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=49-consumed_samples=800.0.ckpt
[NeMo I 2025-10-06 23:03:48 nemo_logging:393] Async checkpoint save for step 50 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.37-step=49-consumed_samples=800.0.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:03:52 nemo_logging:393] Async finalization time took 4.590 s
Training epoch 0, iteration 51/998 | lr: 0.0001 | global_batch_size: 16 | global_step: 51 | reduced_train_loss: 0.1315 | train_step_timing in s: 1.271 | consumed_samples: 832 | val_loss: 0.3666
Training epoch 0, iteration 52/998 | lr: 0.0001 | global_batch_size: 16 | global_step: 52 | reduced_train_loss: 0.1708 | train_step_timing in s: 1.259 | consumed_samples: 848 | val_loss: 0.3666
Training epoch 0, iteration 53/998 | lr: 0.0001 | global_batch_size: 16 | global_step: 53 | reduced_train_loss: 0.3926 | train_step_timing in s: 1.253 | consumed_samples: 864 | val_loss: 0.3666
Training epoch 0, iteration 54/998 | lr: 0.0001 | global_batch_size: 16 | global_step: 54 | reduced_train_loss: 0.3078 | train_step_timing in s: 1.574 | consumed_samples: 880 | val_loss: 0.3666
Training epoch 0, iteration 55/998 | lr: 9.999e-05 | global_batch_size: 16 | global_step: 55 | reduced_train_loss: 0.2875 | train_step_timing in s: 1.345 | consumed_samples: 896 | val_loss: 0.3666
Training epoch 0, iteration 56/998 | lr: 9.999e-05 | global_batch_size: 16 | global_step: 56 | reduced_train_loss: 0.5068 | train_step_timing in s: 1.281 | consumed_samples: 912 | val_loss: 0.3666
Training epoch 0, iteration 57/998 | lr: 9.999e-05 | global_batch_size: 16 | global_step: 57 | reduced_train_loss: 0.2484 | train_step_timing in s: 1.995 | consumed_samples: 928 | val_loss: 0.3666
Training epoch 0, iteration 58/998 | lr: 9.998e-05 | global_batch_size: 16 | global_step: 58 | reduced_train_loss: 0.4283 | train_step_timing in s: 1.2 | consumed_samples: 944 | val_loss: 0.3666
Training epoch 0, iteration 59/998 | lr: 9.998e-05 | global_batch_size: 16 | global_step: 59 | reduced_train_loss: 0.2378 | train_step_timing in s: 1.16 | consumed_samples: 960 | val_loss: 0.3666
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:08:46 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 60 : Start time: 1759792125.916s : Save duration: 1.070s
[NeMo I 2025-10-06 23:08:50 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.30-step=59-consumed_samples=960.0-last.ckpt
Training epoch 0, iteration 60/998 | lr: 9.997e-05 | global_batch_size: 16 | global_step: 60 | reduced_train_loss: 0.1763 | train_step_timing in s: 1.257 | consumed_samples: 976 | val_loss: 0.2973
[NeMo I 2025-10-06 23:08:51 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 61/998 | lr: 9.997e-05 | global_batch_size: 16 | global_step: 61 | reduced_train_loss: 0.1939 | train_step_timing in s: 1.521 | consumed_samples: 992 | val_loss: 0.2973
[NeMo I 2025-10-06 23:08:55 nemo_logging:393] Successfully saved checkpoint from iteration      60 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.30-step=59-consumed_samples=960.0-last.ckpt
[NeMo I 2025-10-06 23:08:56 nemo_logging:393] Async checkpoint save for step 60 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.30-step=59-consumed_samples=960.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:08:56 nemo_logging:393] Async finalization time took 1.451 s
Training epoch 0, iteration 62/998 | lr: 9.996e-05 | global_batch_size: 16 | global_step: 62 | reduced_train_loss: 0.2435 | train_step_timing in s: 1.215 | consumed_samples: 1008 | val_loss: 0.2973
Training epoch 0, iteration 63/998 | lr: 9.995e-05 | global_batch_size: 16 | global_step: 63 | reduced_train_loss: 0.2818 | train_step_timing in s: 1.382 | consumed_samples: 1024 | val_loss: 0.2973
Training epoch 0, iteration 64/998 | lr: 9.995e-05 | global_batch_size: 16 | global_step: 64 | reduced_train_loss: 0.405 | train_step_timing in s: 1.73 | consumed_samples: 1040 | val_loss: 0.2973
Training epoch 0, iteration 65/998 | lr: 9.994e-05 | global_batch_size: 16 | global_step: 65 | reduced_train_loss: 0.2016 | train_step_timing in s: 1.319 | consumed_samples: 1056 | val_loss: 0.2973
Training epoch 0, iteration 66/998 | lr: 9.993e-05 | global_batch_size: 16 | global_step: 66 | reduced_train_loss: 0.1588 | train_step_timing in s: 1.658 | consumed_samples: 1072 | val_loss: 0.2973
Training epoch 0, iteration 67/998 | lr: 9.992e-05 | global_batch_size: 16 | global_step: 67 | reduced_train_loss: 0.2378 | train_step_timing in s: 1.297 | consumed_samples: 1088 | val_loss: 0.2973
Training epoch 0, iteration 68/998 | lr: 9.991e-05 | global_batch_size: 16 | global_step: 68 | reduced_train_loss: 0.3213 | train_step_timing in s: 1.697 | consumed_samples: 1104 | val_loss: 0.2973
Training epoch 0, iteration 69/998 | lr: 9.99e-05 | global_batch_size: 16 | global_step: 69 | reduced_train_loss: 0.2959 | train_step_timing in s: 1.519 | consumed_samples: 1120 | val_loss: 0.2973
Training epoch 0, iteration 70/998 | lr: 9.989e-05 | global_batch_size: 16 | global_step: 70 | reduced_train_loss: 0.1955 | train_step_timing in s: 2.146 | consumed_samples: 1136 | val_loss: 0.2973
Training epoch 0, iteration 71/998 | lr: 9.988e-05 | global_batch_size: 16 | global_step: 71 | reduced_train_loss: 0.2179 | train_step_timing in s: 1.138 | consumed_samples: 1152 | val_loss: 0.2973
Training epoch 0, iteration 72/998 | lr: 9.987e-05 | global_batch_size: 16 | global_step: 72 | reduced_train_loss: 0.28 | train_step_timing in s: 1.322 | consumed_samples: 1168 | val_loss: 0.2973
Training epoch 0, iteration 73/998 | lr: 9.986e-05 | global_batch_size: 16 | global_step: 73 | reduced_train_loss: 0.1693 | train_step_timing in s: 1.274 | consumed_samples: 1184 | val_loss: 0.2973
Training epoch 0, iteration 74/998 | lr: 9.984e-05 | global_batch_size: 16 | global_step: 74 | reduced_train_loss: 0.1174 | train_step_timing in s: 1.242 | consumed_samples: 1200 | val_loss: 0.2973
Training epoch 0, iteration 75/998 | lr: 9.983e-05 | global_batch_size: 16 | global_step: 75 | reduced_train_loss: 0.3269 | train_step_timing in s: 1.282 | consumed_samples: 1216 | val_loss: 0.2973
Training epoch 0, iteration 76/998 | lr: 9.981e-05 | global_batch_size: 16 | global_step: 76 | reduced_train_loss: 0.3492 | train_step_timing in s: 1.282 | consumed_samples: 1232 | val_loss: 0.2973
Training epoch 0, iteration 77/998 | lr: 9.98e-05 | global_batch_size: 16 | global_step: 77 | reduced_train_loss: 0.2237 | train_step_timing in s: 1.201 | consumed_samples: 1248 | val_loss: 0.2973
Training epoch 0, iteration 78/998 | lr: 9.979e-05 | global_batch_size: 16 | global_step: 78 | reduced_train_loss: 0.5816 | train_step_timing in s: 1.198 | consumed_samples: 1264 | val_loss: 0.2973
Training epoch 0, iteration 79/998 | lr: 9.977e-05 | global_batch_size: 16 | global_step: 79 | reduced_train_loss: 0.3072 | train_step_timing in s: 1.252 | consumed_samples: 1280 | val_loss: 0.2973
Training epoch 0, iteration 80/998 | lr: 9.975e-05 | global_batch_size: 16 | global_step: 80 | reduced_train_loss: 0.1446 | train_step_timing in s: 1.669 | consumed_samples: 1296 | val_loss: 0.2973
Training epoch 0, iteration 81/998 | lr: 9.974e-05 | global_batch_size: 16 | global_step: 81 | reduced_train_loss: 0.4045 | train_step_timing in s: 1.322 | consumed_samples: 1312 | val_loss: 0.2973
Training epoch 0, iteration 82/998 | lr: 9.972e-05 | global_batch_size: 16 | global_step: 82 | reduced_train_loss: 0.3864 | train_step_timing in s: 1.337 | consumed_samples: 1328 | val_loss: 0.2973
Training epoch 0, iteration 83/998 | lr: 9.97e-05 | global_batch_size: 16 | global_step: 83 | reduced_train_loss: 0.2422 | train_step_timing in s: 1.264 | consumed_samples: 1344 | val_loss: 0.2973
Training epoch 0, iteration 84/998 | lr: 9.968e-05 | global_batch_size: 16 | global_step: 84 | reduced_train_loss: 0.1348 | train_step_timing in s: 1.207 | consumed_samples: 1360 | val_loss: 0.2973
Training epoch 0, iteration 85/998 | lr: 9.966e-05 | global_batch_size: 16 | global_step: 85 | reduced_train_loss: 0.2106 | train_step_timing in s: 1.369 | consumed_samples: 1376 | val_loss: 0.2973
Training epoch 0, iteration 86/998 | lr: 9.965e-05 | global_batch_size: 16 | global_step: 86 | reduced_train_loss: 0.3251 | train_step_timing in s: 1.233 | consumed_samples: 1392 | val_loss: 0.2973
Training epoch 0, iteration 87/998 | lr: 9.963e-05 | global_batch_size: 16 | global_step: 87 | reduced_train_loss: 0.2921 | train_step_timing in s: 1.339 | consumed_samples: 1408 | val_loss: 0.2973
Training epoch 0, iteration 88/998 | lr: 9.96e-05 | global_batch_size: 16 | global_step: 88 | reduced_train_loss: 0.2287 | train_step_timing in s: 1.387 | consumed_samples: 1424 | val_loss: 0.2973
Training epoch 0, iteration 89/998 | lr: 9.958e-05 | global_batch_size: 16 | global_step: 89 | reduced_train_loss: 0.1331 | train_step_timing in s: 1.247 | consumed_samples: 1440 | val_loss: 0.2973
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:15:08 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 90 : Start time: 1759792507.437s : Save duration: 1.064s
[NeMo I 2025-10-06 23:15:11 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=89-consumed_samples=1440.0-last.ckpt
Training epoch 0, iteration 90/998 | lr: 9.956e-05 | global_batch_size: 16 | global_step: 90 | reduced_train_loss: 0.2109 | train_step_timing in s: 1.229 | consumed_samples: 1456 | val_loss: 0.2741
[NeMo I 2025-10-06 23:15:13 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 91/998 | lr: 9.954e-05 | global_batch_size: 16 | global_step: 91 | reduced_train_loss: 0.2026 | train_step_timing in s: 1.52 | consumed_samples: 1472 | val_loss: 0.2741
[NeMo I 2025-10-06 23:15:17 nemo_logging:393] Successfully saved checkpoint from iteration      90 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=89-consumed_samples=1440.0-last.ckpt
[NeMo I 2025-10-06 23:15:17 nemo_logging:393] Async checkpoint save for step 90 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=89-consumed_samples=1440.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:15:20 nemo_logging:393] Async finalization time took 4.293 s
Training epoch 0, iteration 92/998 | lr: 9.952e-05 | global_batch_size: 16 | global_step: 92 | reduced_train_loss: 0.286 | train_step_timing in s: 1.519 | consumed_samples: 1488 | val_loss: 0.2741
Training epoch 0, iteration 93/998 | lr: 9.949e-05 | global_batch_size: 16 | global_step: 93 | reduced_train_loss: 0.2604 | train_step_timing in s: 1.314 | consumed_samples: 1504 | val_loss: 0.2741
Training epoch 0, iteration 94/998 | lr: 9.947e-05 | global_batch_size: 16 | global_step: 94 | reduced_train_loss: 0.3163 | train_step_timing in s: 1.523 | consumed_samples: 1520 | val_loss: 0.2741
Training epoch 0, iteration 95/998 | lr: 9.945e-05 | global_batch_size: 16 | global_step: 95 | reduced_train_loss: 0.269 | train_step_timing in s: 1.188 | consumed_samples: 1536 | val_loss: 0.2741
Training epoch 0, iteration 96/998 | lr: 9.942e-05 | global_batch_size: 16 | global_step: 96 | reduced_train_loss: 0.3371 | train_step_timing in s: 1.327 | consumed_samples: 1552 | val_loss: 0.2741
Training epoch 0, iteration 97/998 | lr: 9.94e-05 | global_batch_size: 16 | global_step: 97 | reduced_train_loss: 0.5857 | train_step_timing in s: 1.326 | consumed_samples: 1568 | val_loss: 0.2741
Training epoch 0, iteration 98/998 | lr: 9.937e-05 | global_batch_size: 16 | global_step: 98 | reduced_train_loss: 0.09352 | train_step_timing in s: 1.228 | consumed_samples: 1584 | val_loss: 0.2741
Training epoch 0, iteration 99/998 | lr: 9.934e-05 | global_batch_size: 16 | global_step: 99 | reduced_train_loss: 0.1028 | train_step_timing in s: 1.243 | consumed_samples: 1600 | val_loss: 0.2741
Epoch 0, global step 99: 'val_loss' reached 0.27407 (best 0.27407), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=99-consumed_samples=1600.0.ckpt' as top 2
[NeMo I 2025-10-06 23:15:56 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 99 : Start time: 1759792555.921s : Save duration: 0.563s
[NeMo I 2025-10-06 23:15:59 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=99-consumed_samples=1600.0.ckpt
[NeMo I 2025-10-06 23:15:59 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 100/998 | lr: 9.932e-05 | global_batch_size: 16 | global_step: 100 | reduced_train_loss: 0.3353 | train_step_timing in s: 1.207 | consumed_samples: 1616 | val_loss: 0.2741
[NeMo I 2025-10-06 23:16:03 nemo_logging:393] Successfully saved checkpoint from iteration      99 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=99-consumed_samples=1600.0.ckpt
[NeMo I 2025-10-06 23:16:03 nemo_logging:393] Async checkpoint save for step 100 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.27-step=99-consumed_samples=1600.0.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:16:07 nemo_logging:393] Async finalization time took 4.468 s
Training epoch 0, iteration 101/998 | lr: 9.929e-05 | global_batch_size: 16 | global_step: 101 | reduced_train_loss: 0.3756 | train_step_timing in s: 1.707 | consumed_samples: 1632 | val_loss: 0.2741
Training epoch 0, iteration 102/998 | lr: 9.926e-05 | global_batch_size: 16 | global_step: 102 | reduced_train_loss: 0.329 | train_step_timing in s: 1.262 | consumed_samples: 1648 | val_loss: 0.2741
Training epoch 0, iteration 103/998 | lr: 9.923e-05 | global_batch_size: 16 | global_step: 103 | reduced_train_loss: 0.2982 | train_step_timing in s: 1.746 | consumed_samples: 1664 | val_loss: 0.2741
Training epoch 0, iteration 104/998 | lr: 9.92e-05 | global_batch_size: 16 | global_step: 104 | reduced_train_loss: 0.2855 | train_step_timing in s: 1.221 | consumed_samples: 1680 | val_loss: 0.2741
Training epoch 0, iteration 105/998 | lr: 9.917e-05 | global_batch_size: 16 | global_step: 105 | reduced_train_loss: 0.3824 | train_step_timing in s: 1.274 | consumed_samples: 1696 | val_loss: 0.2741
Training epoch 0, iteration 106/998 | lr: 9.914e-05 | global_batch_size: 16 | global_step: 106 | reduced_train_loss: 0.2668 | train_step_timing in s: 1.292 | consumed_samples: 1712 | val_loss: 0.2741
Training epoch 0, iteration 107/998 | lr: 9.911e-05 | global_batch_size: 16 | global_step: 107 | reduced_train_loss: 0.2614 | train_step_timing in s: 2.43 | consumed_samples: 1728 | val_loss: 0.2741
Training epoch 0, iteration 108/998 | lr: 9.908e-05 | global_batch_size: 16 | global_step: 108 | reduced_train_loss: 0.3449 | train_step_timing in s: 1.258 | consumed_samples: 1744 | val_loss: 0.2741
Training epoch 0, iteration 109/998 | lr: 9.905e-05 | global_batch_size: 16 | global_step: 109 | reduced_train_loss: 0.2313 | train_step_timing in s: 1.324 | consumed_samples: 1760 | val_loss: 0.2741
Training epoch 0, iteration 110/998 | lr: 9.902e-05 | global_batch_size: 16 | global_step: 110 | reduced_train_loss: 0.1275 | train_step_timing in s: 1.259 | consumed_samples: 1776 | val_loss: 0.2741
Training epoch 0, iteration 111/998 | lr: 9.898e-05 | global_batch_size: 16 | global_step: 111 | reduced_train_loss: 0.3255 | train_step_timing in s: 1.305 | consumed_samples: 1792 | val_loss: 0.2741
Training epoch 0, iteration 112/998 | lr: 9.895e-05 | global_batch_size: 16 | global_step: 112 | reduced_train_loss: 0.2525 | train_step_timing in s: 1.229 | consumed_samples: 1808 | val_loss: 0.2741
Training epoch 0, iteration 113/998 | lr: 9.892e-05 | global_batch_size: 16 | global_step: 113 | reduced_train_loss: 0.1948 | train_step_timing in s: 1.244 | consumed_samples: 1824 | val_loss: 0.2741
Training epoch 0, iteration 114/998 | lr: 9.888e-05 | global_batch_size: 16 | global_step: 114 | reduced_train_loss: 0.3376 | train_step_timing in s: 1.245 | consumed_samples: 1840 | val_loss: 0.2741
Training epoch 0, iteration 115/998 | lr: 9.885e-05 | global_batch_size: 16 | global_step: 115 | reduced_train_loss: 0.1061 | train_step_timing in s: 1.388 | consumed_samples: 1856 | val_loss: 0.2741
Training epoch 0, iteration 116/998 | lr: 9.881e-05 | global_batch_size: 16 | global_step: 116 | reduced_train_loss: 0.2041 | train_step_timing in s: 1.674 | consumed_samples: 1872 | val_loss: 0.2741
Training epoch 0, iteration 117/998 | lr: 9.878e-05 | global_batch_size: 16 | global_step: 117 | reduced_train_loss: 0.2381 | train_step_timing in s: 1.207 | consumed_samples: 1888 | val_loss: 0.2741
Training epoch 0, iteration 118/998 | lr: 9.874e-05 | global_batch_size: 16 | global_step: 118 | reduced_train_loss: 0.3338 | train_step_timing in s: 1.362 | consumed_samples: 1904 | val_loss: 0.2741
Training epoch 0, iteration 119/998 | lr: 9.87e-05 | global_batch_size: 16 | global_step: 119 | reduced_train_loss: 0.3779 | train_step_timing in s: 1.707 | consumed_samples: 1920 | val_loss: 0.2741
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:21:50 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 120 : Start time: 1759792910.295s : Save duration: 0.561s
[NeMo I 2025-10-06 23:21:54 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=119-consumed_samples=1920.0-last.ckpt
Training epoch 0, iteration 120/998 | lr: 9.866e-05 | global_batch_size: 16 | global_step: 120 | reduced_train_loss: 0.08073 | train_step_timing in s: 1.256 | consumed_samples: 1936 | val_loss: 0.2574
[NeMo I 2025-10-06 23:21:55 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 121/998 | lr: 9.863e-05 | global_batch_size: 16 | global_step: 121 | reduced_train_loss: 0.157 | train_step_timing in s: 1.352 | consumed_samples: 1952 | val_loss: 0.2574
[NeMo I 2025-10-06 23:21:59 nemo_logging:393] Successfully saved checkpoint from iteration     120 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=119-consumed_samples=1920.0-last.ckpt
[NeMo I 2025-10-06 23:21:59 nemo_logging:393] Async checkpoint save for step 120 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=119-consumed_samples=1920.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:22:00 nemo_logging:393] Async finalization time took 1.521 s
Training epoch 0, iteration 122/998 | lr: 9.859e-05 | global_batch_size: 16 | global_step: 122 | reduced_train_loss: 0.2347 | train_step_timing in s: 1.217 | consumed_samples: 1968 | val_loss: 0.2574
Training epoch 0, iteration 123/998 | lr: 9.855e-05 | global_batch_size: 16 | global_step: 123 | reduced_train_loss: 0.1136 | train_step_timing in s: 1.52 | consumed_samples: 1984 | val_loss: 0.2574
Training epoch 0, iteration 124/998 | lr: 9.851e-05 | global_batch_size: 16 | global_step: 124 | reduced_train_loss: 0.1674 | train_step_timing in s: 1.658 | consumed_samples: 2000 | val_loss: 0.2574
Training epoch 0, iteration 125/998 | lr: 9.847e-05 | global_batch_size: 16 | global_step: 125 | reduced_train_loss: 0.2597 | train_step_timing in s: 1.321 | consumed_samples: 2016 | val_loss: 0.2574
Training epoch 0, iteration 126/998 | lr: 9.843e-05 | global_batch_size: 16 | global_step: 126 | reduced_train_loss: 0.3582 | train_step_timing in s: 1.681 | consumed_samples: 2032 | val_loss: 0.2574
Training epoch 0, iteration 127/998 | lr: 9.838e-05 | global_batch_size: 16 | global_step: 127 | reduced_train_loss: 0.3088 | train_step_timing in s: 1.247 | consumed_samples: 2048 | val_loss: 0.2574
Training epoch 0, iteration 128/998 | lr: 9.834e-05 | global_batch_size: 16 | global_step: 128 | reduced_train_loss: 0.1516 | train_step_timing in s: 1.316 | consumed_samples: 2064 | val_loss: 0.2574
Training epoch 0, iteration 129/998 | lr: 9.83e-05 | global_batch_size: 16 | global_step: 129 | reduced_train_loss: 0.2633 | train_step_timing in s: 1.229 | consumed_samples: 2080 | val_loss: 0.2574
Training epoch 0, iteration 130/998 | lr: 9.826e-05 | global_batch_size: 16 | global_step: 130 | reduced_train_loss: 0.3539 | train_step_timing in s: 1.195 | consumed_samples: 2096 | val_loss: 0.2574
Training epoch 0, iteration 131/998 | lr: 9.821e-05 | global_batch_size: 16 | global_step: 131 | reduced_train_loss: 0.3982 | train_step_timing in s: 1.265 | consumed_samples: 2112 | val_loss: 0.2574
Training epoch 0, iteration 132/998 | lr: 9.817e-05 | global_batch_size: 16 | global_step: 132 | reduced_train_loss: 0.2681 | train_step_timing in s: 1.241 | consumed_samples: 2128 | val_loss: 0.2574
Training epoch 0, iteration 133/998 | lr: 9.812e-05 | global_batch_size: 16 | global_step: 133 | reduced_train_loss: 0.3868 | train_step_timing in s: 1.203 | consumed_samples: 2144 | val_loss: 0.2574
Training epoch 0, iteration 134/998 | lr: 9.808e-05 | global_batch_size: 16 | global_step: 134 | reduced_train_loss: 0.2772 | train_step_timing in s: 1.326 | consumed_samples: 2160 | val_loss: 0.2574
Training epoch 0, iteration 135/998 | lr: 9.803e-05 | global_batch_size: 16 | global_step: 135 | reduced_train_loss: 0.3681 | train_step_timing in s: 1.259 | consumed_samples: 2176 | val_loss: 0.2574
Training epoch 0, iteration 136/998 | lr: 9.799e-05 | global_batch_size: 16 | global_step: 136 | reduced_train_loss: 0.08104 | train_step_timing in s: 1.261 | consumed_samples: 2192 | val_loss: 0.2574
Training epoch 0, iteration 137/998 | lr: 9.794e-05 | global_batch_size: 16 | global_step: 137 | reduced_train_loss: 0.3919 | train_step_timing in s: 1.64 | consumed_samples: 2208 | val_loss: 0.2574
Training epoch 0, iteration 138/998 | lr: 9.789e-05 | global_batch_size: 16 | global_step: 138 | reduced_train_loss: 0.3181 | train_step_timing in s: 1.243 | consumed_samples: 2224 | val_loss: 0.2574
Training epoch 0, iteration 139/998 | lr: 9.785e-05 | global_batch_size: 16 | global_step: 139 | reduced_train_loss: 0.1093 | train_step_timing in s: 1.198 | consumed_samples: 2240 | val_loss: 0.2574
Training epoch 0, iteration 140/998 | lr: 9.78e-05 | global_batch_size: 16 | global_step: 140 | reduced_train_loss: 0.2197 | train_step_timing in s: 1.252 | consumed_samples: 2256 | val_loss: 0.2574
Training epoch 0, iteration 141/998 | lr: 9.775e-05 | global_batch_size: 16 | global_step: 141 | reduced_train_loss: 0.1995 | train_step_timing in s: 1.389 | consumed_samples: 2272 | val_loss: 0.2574
Training epoch 0, iteration 142/998 | lr: 9.77e-05 | global_batch_size: 16 | global_step: 142 | reduced_train_loss: 0.2754 | train_step_timing in s: 1.31 | consumed_samples: 2288 | val_loss: 0.2574
Training epoch 0, iteration 143/998 | lr: 9.765e-05 | global_batch_size: 16 | global_step: 143 | reduced_train_loss: 0.2051 | train_step_timing in s: 1.157 | consumed_samples: 2304 | val_loss: 0.2574
Training epoch 0, iteration 144/998 | lr: 9.76e-05 | global_batch_size: 16 | global_step: 144 | reduced_train_loss: 0.2661 | train_step_timing in s: 1.401 | consumed_samples: 2320 | val_loss: 0.2574
Training epoch 0, iteration 145/998 | lr: 9.755e-05 | global_batch_size: 16 | global_step: 145 | reduced_train_loss: 0.3561 | train_step_timing in s: 1.333 | consumed_samples: 2336 | val_loss: 0.2574
Training epoch 0, iteration 146/998 | lr: 9.75e-05 | global_batch_size: 16 | global_step: 146 | reduced_train_loss: 0.1892 | train_step_timing in s: 1.327 | consumed_samples: 2352 | val_loss: 0.2574
Training epoch 0, iteration 147/998 | lr: 9.744e-05 | global_batch_size: 16 | global_step: 147 | reduced_train_loss: 0.1934 | train_step_timing in s: 1.192 | consumed_samples: 2368 | val_loss: 0.2574
Training epoch 0, iteration 148/998 | lr: 9.739e-05 | global_batch_size: 16 | global_step: 148 | reduced_train_loss: 0.3302 | train_step_timing in s: 1.54 | consumed_samples: 2384 | val_loss: 0.2574
Training epoch 0, iteration 149/998 | lr: 9.734e-05 | global_batch_size: 16 | global_step: 149 | reduced_train_loss: 0.1868 | train_step_timing in s: 2.014 | consumed_samples: 2400 | val_loss: 0.2574
Epoch 0, global step 149: 'val_loss' reached 0.25741 (best 0.25741), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=149-consumed_samples=2400.0.ckpt' as top 2
[NeMo I 2025-10-06 23:24:07 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 149 : Start time: 1759793046.741s : Save duration: 0.549s
[NeMo I 2025-10-06 23:24:11 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=149-consumed_samples=2400.0.ckpt
[NeMo I 2025-10-06 23:24:11 nemo_logging:393] Async finalization time took 0.000 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 150/998 | lr: 9.729e-05 | global_batch_size: 16 | global_step: 150 | reduced_train_loss: 0.2211 | train_step_timing in s: 1.203 | consumed_samples: 2416 | val_loss: 0.2515
[NeMo I 2025-10-06 23:28:30 nemo_logging:393] Successfully saved checkpoint from iteration     149 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=149-consumed_samples=2400.0.ckpt
[NeMo I 2025-10-06 23:28:30 nemo_logging:393] Async checkpoint save for step 150 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=149-consumed_samples=2400.0.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:28:37 nemo_logging:393] Async finalization time took 7.838 s
Training epoch 0, iteration 151/998 | lr: 9.723e-05 | global_batch_size: 16 | global_step: 151 | reduced_train_loss: 0.2927 | train_step_timing in s: 1.259 | consumed_samples: 2432 | val_loss: 0.2515
Training epoch 0, iteration 152/998 | lr: 9.718e-05 | global_batch_size: 16 | global_step: 152 | reduced_train_loss: 0.1271 | train_step_timing in s: 1.23 | consumed_samples: 2448 | val_loss: 0.2515
Training epoch 0, iteration 153/998 | lr: 9.712e-05 | global_batch_size: 16 | global_step: 153 | reduced_train_loss: 0.465 | train_step_timing in s: 1.554 | consumed_samples: 2464 | val_loss: 0.2515
Training epoch 0, iteration 154/998 | lr: 9.707e-05 | global_batch_size: 16 | global_step: 154 | reduced_train_loss: 0.2043 | train_step_timing in s: 1.197 | consumed_samples: 2480 | val_loss: 0.2515
Training epoch 0, iteration 155/998 | lr: 9.701e-05 | global_batch_size: 16 | global_step: 155 | reduced_train_loss: 0.2281 | train_step_timing in s: 1.13 | consumed_samples: 2496 | val_loss: 0.2515
Training epoch 0, iteration 156/998 | lr: 9.695e-05 | global_batch_size: 16 | global_step: 156 | reduced_train_loss: 0.2644 | train_step_timing in s: 1.733 | consumed_samples: 2512 | val_loss: 0.2515
Training epoch 0, iteration 157/998 | lr: 9.69e-05 | global_batch_size: 16 | global_step: 157 | reduced_train_loss: 0.3217 | train_step_timing in s: 1.663 | consumed_samples: 2528 | val_loss: 0.2515
Training epoch 0, iteration 158/998 | lr: 9.684e-05 | global_batch_size: 16 | global_step: 158 | reduced_train_loss: 0.2651 | train_step_timing in s: 1.337 | consumed_samples: 2544 | val_loss: 0.2515
Training epoch 0, iteration 159/998 | lr: 9.678e-05 | global_batch_size: 16 | global_step: 159 | reduced_train_loss: 0.2733 | train_step_timing in s: 1.361 | consumed_samples: 2560 | val_loss: 0.2515
Training epoch 0, iteration 160/998 | lr: 9.672e-05 | global_batch_size: 16 | global_step: 160 | reduced_train_loss: 0.3094 | train_step_timing in s: 1.525 | consumed_samples: 2576 | val_loss: 0.2515
Training epoch 0, iteration 161/998 | lr: 9.666e-05 | global_batch_size: 16 | global_step: 161 | reduced_train_loss: 0.1709 | train_step_timing in s: 1.527 | consumed_samples: 2592 | val_loss: 0.2515
Training epoch 0, iteration 162/998 | lr: 9.66e-05 | global_batch_size: 16 | global_step: 162 | reduced_train_loss: 0.421 | train_step_timing in s: 1.159 | consumed_samples: 2608 | val_loss: 0.2515
Training epoch 0, iteration 163/998 | lr: 9.654e-05 | global_batch_size: 16 | global_step: 163 | reduced_train_loss: 0.177 | train_step_timing in s: 1.328 | consumed_samples: 2624 | val_loss: 0.2515
Training epoch 0, iteration 164/998 | lr: 9.648e-05 | global_batch_size: 16 | global_step: 164 | reduced_train_loss: 0.1547 | train_step_timing in s: 1.527 | consumed_samples: 2640 | val_loss: 0.2515
Training epoch 0, iteration 165/998 | lr: 9.642e-05 | global_batch_size: 16 | global_step: 165 | reduced_train_loss: 0.3576 | train_step_timing in s: 1.805 | consumed_samples: 2656 | val_loss: 0.2515
Training epoch 0, iteration 166/998 | lr: 9.636e-05 | global_batch_size: 16 | global_step: 166 | reduced_train_loss: 0.3597 | train_step_timing in s: 1.383 | consumed_samples: 2672 | val_loss: 0.2515
Training epoch 0, iteration 167/998 | lr: 9.63e-05 | global_batch_size: 16 | global_step: 167 | reduced_train_loss: 0.2214 | train_step_timing in s: 1.169 | consumed_samples: 2688 | val_loss: 0.2515
Training epoch 0, iteration 168/998 | lr: 9.623e-05 | global_batch_size: 16 | global_step: 168 | reduced_train_loss: 0.2002 | train_step_timing in s: 1.773 | consumed_samples: 2704 | val_loss: 0.2515
Training epoch 0, iteration 169/998 | lr: 9.617e-05 | global_batch_size: 16 | global_step: 169 | reduced_train_loss: 0.2213 | train_step_timing in s: 1.993 | consumed_samples: 2720 | val_loss: 0.2515
Training epoch 0, iteration 170/998 | lr: 9.611e-05 | global_batch_size: 16 | global_step: 170 | reduced_train_loss: 0.2199 | train_step_timing in s: 1.143 | consumed_samples: 2736 | val_loss: 0.2515
Training epoch 0, iteration 171/998 | lr: 9.604e-05 | global_batch_size: 16 | global_step: 171 | reduced_train_loss: 0.1565 | train_step_timing in s: 1.244 | consumed_samples: 2752 | val_loss: 0.2515
Training epoch 0, iteration 172/998 | lr: 9.598e-05 | global_batch_size: 16 | global_step: 172 | reduced_train_loss: 0.1148 | train_step_timing in s: 1.235 | consumed_samples: 2768 | val_loss: 0.2515
Training epoch 0, iteration 173/998 | lr: 9.591e-05 | global_batch_size: 16 | global_step: 173 | reduced_train_loss: 0.2534 | train_step_timing in s: 1.564 | consumed_samples: 2784 | val_loss: 0.2515
Training epoch 0, iteration 174/998 | lr: 9.585e-05 | global_batch_size: 16 | global_step: 174 | reduced_train_loss: 0.2422 | train_step_timing in s: 1.674 | consumed_samples: 2800 | val_loss: 0.2515
Training epoch 0, iteration 175/998 | lr: 9.578e-05 | global_batch_size: 16 | global_step: 175 | reduced_train_loss: 0.2133 | train_step_timing in s: 1.304 | consumed_samples: 2816 | val_loss: 0.2515
Training epoch 0, iteration 176/998 | lr: 9.571e-05 | global_batch_size: 16 | global_step: 176 | reduced_train_loss: 0.2091 | train_step_timing in s: 1.326 | consumed_samples: 2832 | val_loss: 0.2515
Training epoch 0, iteration 177/998 | lr: 9.565e-05 | global_batch_size: 16 | global_step: 177 | reduced_train_loss: 0.09055 | train_step_timing in s: 1.221 | consumed_samples: 2848 | val_loss: 0.2515
Training epoch 0, iteration 178/998 | lr: 9.558e-05 | global_batch_size: 16 | global_step: 178 | reduced_train_loss: 0.1463 | train_step_timing in s: 1.697 | consumed_samples: 2864 | val_loss: 0.2515
Training epoch 0, iteration 179/998 | lr: 9.551e-05 | global_batch_size: 16 | global_step: 179 | reduced_train_loss: 0.2671 | train_step_timing in s: 1.302 | consumed_samples: 2880 | val_loss: 0.2515
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:35:14 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 180 : Start time: 1759793713.017s : Save duration: 1.005s
[NeMo I 2025-10-06 23:35:17 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=179-consumed_samples=2880.0-last.ckpt
Training epoch 0, iteration 180/998 | lr: 9.544e-05 | global_batch_size: 16 | global_step: 180 | reduced_train_loss: 0.2474 | train_step_timing in s: 1.295 | consumed_samples: 2896 | val_loss: 0.2464
[NeMo I 2025-10-06 23:35:18 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 181/998 | lr: 9.537e-05 | global_batch_size: 16 | global_step: 181 | reduced_train_loss: 0.107 | train_step_timing in s: 1.273 | consumed_samples: 2912 | val_loss: 0.2464
[NeMo I 2025-10-06 23:35:22 nemo_logging:393] Successfully saved checkpoint from iteration     180 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=179-consumed_samples=2880.0-last.ckpt
[NeMo I 2025-10-06 23:35:22 nemo_logging:393] Async checkpoint save for step 180 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=179-consumed_samples=2880.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:35:23 nemo_logging:393] Async finalization time took 1.468 s
Training epoch 0, iteration 182/998 | lr: 9.53e-05 | global_batch_size: 16 | global_step: 182 | reduced_train_loss: 0.1724 | train_step_timing in s: 1.831 | consumed_samples: 2928 | val_loss: 0.2464
Training epoch 0, iteration 183/998 | lr: 9.523e-05 | global_batch_size: 16 | global_step: 183 | reduced_train_loss: 0.08905 | train_step_timing in s: 1.28 | consumed_samples: 2944 | val_loss: 0.2464
Training epoch 0, iteration 184/998 | lr: 9.516e-05 | global_batch_size: 16 | global_step: 184 | reduced_train_loss: 0.1854 | train_step_timing in s: 1.281 | consumed_samples: 2960 | val_loss: 0.2464
Training epoch 0, iteration 185/998 | lr: 9.509e-05 | global_batch_size: 16 | global_step: 185 | reduced_train_loss: 0.3206 | train_step_timing in s: 2.147 | consumed_samples: 2976 | val_loss: 0.2464
Training epoch 0, iteration 186/998 | lr: 9.502e-05 | global_batch_size: 16 | global_step: 186 | reduced_train_loss: 0.1993 | train_step_timing in s: 1.268 | consumed_samples: 2992 | val_loss: 0.2464
Training epoch 0, iteration 187/998 | lr: 9.495e-05 | global_batch_size: 16 | global_step: 187 | reduced_train_loss: 0.109 | train_step_timing in s: 1.803 | consumed_samples: 3008 | val_loss: 0.2464
Training epoch 0, iteration 188/998 | lr: 9.487e-05 | global_batch_size: 16 | global_step: 188 | reduced_train_loss: 0.2821 | train_step_timing in s: 1.252 | consumed_samples: 3024 | val_loss: 0.2464
Training epoch 0, iteration 189/998 | lr: 9.48e-05 | global_batch_size: 16 | global_step: 189 | reduced_train_loss: 0.2707 | train_step_timing in s: 1.801 | consumed_samples: 3040 | val_loss: 0.2464
Training epoch 0, iteration 190/998 | lr: 9.473e-05 | global_batch_size: 16 | global_step: 190 | reduced_train_loss: 0.1574 | train_step_timing in s: 1.163 | consumed_samples: 3056 | val_loss: 0.2464
Training epoch 0, iteration 191/998 | lr: 9.465e-05 | global_batch_size: 16 | global_step: 191 | reduced_train_loss: 0.1278 | train_step_timing in s: 1.169 | consumed_samples: 3072 | val_loss: 0.2464
Training epoch 0, iteration 192/998 | lr: 9.458e-05 | global_batch_size: 16 | global_step: 192 | reduced_train_loss: 0.4175 | train_step_timing in s: 1.187 | consumed_samples: 3088 | val_loss: 0.2464
Training epoch 0, iteration 193/998 | lr: 9.45e-05 | global_batch_size: 16 | global_step: 193 | reduced_train_loss: 0.3499 | train_step_timing in s: 1.37 | consumed_samples: 3104 | val_loss: 0.2464
Training epoch 0, iteration 194/998 | lr: 9.443e-05 | global_batch_size: 16 | global_step: 194 | reduced_train_loss: 0.4296 | train_step_timing in s: 1.366 | consumed_samples: 3120 | val_loss: 0.2464
Training epoch 0, iteration 195/998 | lr: 9.435e-05 | global_batch_size: 16 | global_step: 195 | reduced_train_loss: 0.4308 | train_step_timing in s: 1.21 | consumed_samples: 3136 | val_loss: 0.2464
Training epoch 0, iteration 196/998 | lr: 9.427e-05 | global_batch_size: 16 | global_step: 196 | reduced_train_loss: 0.12 | train_step_timing in s: 1.344 | consumed_samples: 3152 | val_loss: 0.2464
Training epoch 0, iteration 197/998 | lr: 9.42e-05 | global_batch_size: 16 | global_step: 197 | reduced_train_loss: 0.5275 | train_step_timing in s: 1.287 | consumed_samples: 3168 | val_loss: 0.2464
Training epoch 0, iteration 198/998 | lr: 9.412e-05 | global_batch_size: 16 | global_step: 198 | reduced_train_loss: 0.1806 | train_step_timing in s: 1.331 | consumed_samples: 3184 | val_loss: 0.2464
Training epoch 0, iteration 199/998 | lr: 9.404e-05 | global_batch_size: 16 | global_step: 199 | reduced_train_loss: 0.157 | train_step_timing in s: 2.175 | consumed_samples: 3200 | val_loss: 0.2464
Epoch 0, global step 199: 'val_loss' reached 0.24645 (best 0.24645), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=199-consumed_samples=3200.0.ckpt' as top 2
[NeMo I 2025-10-06 23:36:45 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 199 : Start time: 1759793804.810s : Save duration: 0.554s
[NeMo I 2025-10-06 23:36:48 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=199-consumed_samples=3200.0.ckpt
[NeMo I 2025-10-06 23:36:48 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 200/998 | lr: 9.396e-05 | global_batch_size: 16 | global_step: 200 | reduced_train_loss: 0.2909 | train_step_timing in s: 1.375 | consumed_samples: 3216 | val_loss: 0.2464
[NeMo I 2025-10-06 23:36:52 nemo_logging:393] Successfully saved checkpoint from iteration     199 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=199-consumed_samples=3200.0.ckpt
[NeMo I 2025-10-06 23:36:53 nemo_logging:393] Async checkpoint save for step 200 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.25-step=199-consumed_samples=3200.0.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:36:59 nemo_logging:393] Async finalization time took 7.719 s
Training epoch 0, iteration 201/998 | lr: 9.388e-05 | global_batch_size: 16 | global_step: 201 | reduced_train_loss: 0.1963 | train_step_timing in s: 1.26 | consumed_samples: 3232 | val_loss: 0.2464
Training epoch 0, iteration 202/998 | lr: 9.38e-05 | global_batch_size: 16 | global_step: 202 | reduced_train_loss: 0.1354 | train_step_timing in s: 1.254 | consumed_samples: 3248 | val_loss: 0.2464
Training epoch 0, iteration 203/998 | lr: 9.372e-05 | global_batch_size: 16 | global_step: 203 | reduced_train_loss: 0.278 | train_step_timing in s: 1.318 | consumed_samples: 3264 | val_loss: 0.2464
Training epoch 0, iteration 204/998 | lr: 9.364e-05 | global_batch_size: 16 | global_step: 204 | reduced_train_loss: 0.1438 | train_step_timing in s: 1.217 | consumed_samples: 3280 | val_loss: 0.2464
Training epoch 0, iteration 205/998 | lr: 9.356e-05 | global_batch_size: 16 | global_step: 205 | reduced_train_loss: 0.3842 | train_step_timing in s: 1.704 | consumed_samples: 3296 | val_loss: 0.2464
Training epoch 0, iteration 206/998 | lr: 9.348e-05 | global_batch_size: 16 | global_step: 206 | reduced_train_loss: 0.2148 | train_step_timing in s: 2.124 | consumed_samples: 3312 | val_loss: 0.2464
Training epoch 0, iteration 207/998 | lr: 9.34e-05 | global_batch_size: 16 | global_step: 207 | reduced_train_loss: 0.2034 | train_step_timing in s: 1.216 | consumed_samples: 3328 | val_loss: 0.2464
Training epoch 0, iteration 208/998 | lr: 9.332e-05 | global_batch_size: 16 | global_step: 208 | reduced_train_loss: 0.165 | train_step_timing in s: 1.335 | consumed_samples: 3344 | val_loss: 0.2464
Training epoch 0, iteration 209/998 | lr: 9.323e-05 | global_batch_size: 16 | global_step: 209 | reduced_train_loss: 0.421 | train_step_timing in s: 1.152 | consumed_samples: 3360 | val_loss: 0.2464
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:41:55 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 210 : Start time: 1759794114.725s : Save duration: 0.583s
[NeMo I 2025-10-06 23:41:58 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=209-consumed_samples=3360.0-last.ckpt
Training epoch 0, iteration 210/998 | lr: 9.315e-05 | global_batch_size: 16 | global_step: 210 | reduced_train_loss: 0.2094 | train_step_timing in s: 1.774 | consumed_samples: 3376 | val_loss: 0.2642
[NeMo I 2025-10-06 23:42:00 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 211/998 | lr: 9.306e-05 | global_batch_size: 16 | global_step: 211 | reduced_train_loss: 0.2518 | train_step_timing in s: 1.524 | consumed_samples: 3392 | val_loss: 0.2642
[NeMo I 2025-10-06 23:42:04 nemo_logging:393] Successfully saved checkpoint from iteration     210 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=209-consumed_samples=3360.0-last.ckpt
[NeMo I 2025-10-06 23:42:04 nemo_logging:393] Async checkpoint save for step 210 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.26-step=209-consumed_samples=3360.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:42:05 nemo_logging:393] Async finalization time took 1.445 s
Training epoch 0, iteration 212/998 | lr: 9.298e-05 | global_batch_size: 16 | global_step: 212 | reduced_train_loss: 0.2732 | train_step_timing in s: 1.525 | consumed_samples: 3408 | val_loss: 0.2642
Training epoch 0, iteration 213/998 | lr: 9.29e-05 | global_batch_size: 16 | global_step: 213 | reduced_train_loss: 0.3352 | train_step_timing in s: 1.264 | consumed_samples: 3424 | val_loss: 0.2642
Training epoch 0, iteration 214/998 | lr: 9.281e-05 | global_batch_size: 16 | global_step: 214 | reduced_train_loss: 0.221 | train_step_timing in s: 1.834 | consumed_samples: 3440 | val_loss: 0.2642
Training epoch 0, iteration 215/998 | lr: 9.272e-05 | global_batch_size: 16 | global_step: 215 | reduced_train_loss: 0.2047 | train_step_timing in s: 1.252 | consumed_samples: 3456 | val_loss: 0.2642
Training epoch 0, iteration 216/998 | lr: 9.264e-05 | global_batch_size: 16 | global_step: 216 | reduced_train_loss: 0.3766 | train_step_timing in s: 1.218 | consumed_samples: 3472 | val_loss: 0.2642
Training epoch 0, iteration 217/998 | lr: 9.255e-05 | global_batch_size: 16 | global_step: 217 | reduced_train_loss: 0.2168 | train_step_timing in s: 1.382 | consumed_samples: 3488 | val_loss: 0.2642
Training epoch 0, iteration 218/998 | lr: 9.246e-05 | global_batch_size: 16 | global_step: 218 | reduced_train_loss: 0.2838 | train_step_timing in s: 1.379 | consumed_samples: 3504 | val_loss: 0.2642
Training epoch 0, iteration 219/998 | lr: 9.238e-05 | global_batch_size: 16 | global_step: 219 | reduced_train_loss: 0.3172 | train_step_timing in s: 1.256 | consumed_samples: 3520 | val_loss: 0.2642
Training epoch 0, iteration 220/998 | lr: 9.229e-05 | global_batch_size: 16 | global_step: 220 | reduced_train_loss: 0.2503 | train_step_timing in s: 1.238 | consumed_samples: 3536 | val_loss: 0.2642
Training epoch 0, iteration 221/998 | lr: 9.22e-05 | global_batch_size: 16 | global_step: 221 | reduced_train_loss: 0.2542 | train_step_timing in s: 1.561 | consumed_samples: 3552 | val_loss: 0.2642
Training epoch 0, iteration 222/998 | lr: 9.211e-05 | global_batch_size: 16 | global_step: 222 | reduced_train_loss: 0.2257 | train_step_timing in s: 1.362 | consumed_samples: 3568 | val_loss: 0.2642
Training epoch 0, iteration 223/998 | lr: 9.202e-05 | global_batch_size: 16 | global_step: 223 | reduced_train_loss: 0.2026 | train_step_timing in s: 1.263 | consumed_samples: 3584 | val_loss: 0.2642
Training epoch 0, iteration 224/998 | lr: 9.193e-05 | global_batch_size: 16 | global_step: 224 | reduced_train_loss: 0.1578 | train_step_timing in s: 1.673 | consumed_samples: 3600 | val_loss: 0.2642
Training epoch 0, iteration 225/998 | lr: 9.184e-05 | global_batch_size: 16 | global_step: 225 | reduced_train_loss: 0.3231 | train_step_timing in s: 2.111 | consumed_samples: 3616 | val_loss: 0.2642
Training epoch 0, iteration 226/998 | lr: 9.175e-05 | global_batch_size: 16 | global_step: 226 | reduced_train_loss: 0.264 | train_step_timing in s: 1.273 | consumed_samples: 3632 | val_loss: 0.2642
Training epoch 0, iteration 227/998 | lr: 9.166e-05 | global_batch_size: 16 | global_step: 227 | reduced_train_loss: 0.1102 | train_step_timing in s: 1.279 | consumed_samples: 3648 | val_loss: 0.2642
Training epoch 0, iteration 228/998 | lr: 9.157e-05 | global_batch_size: 16 | global_step: 228 | reduced_train_loss: 0.1855 | train_step_timing in s: 1.247 | consumed_samples: 3664 | val_loss: 0.2642
Training epoch 0, iteration 229/998 | lr: 9.148e-05 | global_batch_size: 16 | global_step: 229 | reduced_train_loss: 0.2329 | train_step_timing in s: 1.338 | consumed_samples: 3680 | val_loss: 0.2642
Training epoch 0, iteration 230/998 | lr: 9.138e-05 | global_batch_size: 16 | global_step: 230 | reduced_train_loss: 0.09878 | train_step_timing in s: 1.254 | consumed_samples: 3696 | val_loss: 0.2642
Training epoch 0, iteration 231/998 | lr: 9.129e-05 | global_batch_size: 16 | global_step: 231 | reduced_train_loss: 0.1593 | train_step_timing in s: 1.324 | consumed_samples: 3712 | val_loss: 0.2642
Training epoch 0, iteration 232/998 | lr: 9.12e-05 | global_batch_size: 16 | global_step: 232 | reduced_train_loss: 0.4072 | train_step_timing in s: 1.36 | consumed_samples: 3728 | val_loss: 0.2642
Training epoch 0, iteration 233/998 | lr: 9.11e-05 | global_batch_size: 16 | global_step: 233 | reduced_train_loss: 0.2661 | train_step_timing in s: 1.2 | consumed_samples: 3744 | val_loss: 0.2642
Training epoch 0, iteration 234/998 | lr: 9.101e-05 | global_batch_size: 16 | global_step: 234 | reduced_train_loss: 0.3048 | train_step_timing in s: 1.273 | consumed_samples: 3760 | val_loss: 0.2642
Training epoch 0, iteration 235/998 | lr: 9.091e-05 | global_batch_size: 16 | global_step: 235 | reduced_train_loss: 0.2371 | train_step_timing in s: 1.235 | consumed_samples: 3776 | val_loss: 0.2642
Training epoch 0, iteration 236/998 | lr: 9.082e-05 | global_batch_size: 16 | global_step: 236 | reduced_train_loss: 0.2017 | train_step_timing in s: 1.24 | consumed_samples: 3792 | val_loss: 0.2642
Training epoch 0, iteration 237/998 | lr: 9.072e-05 | global_batch_size: 16 | global_step: 237 | reduced_train_loss: 0.1132 | train_step_timing in s: 1.987 | consumed_samples: 3808 | val_loss: 0.2642
Training epoch 0, iteration 238/998 | lr: 9.063e-05 | global_batch_size: 16 | global_step: 238 | reduced_train_loss: 0.3282 | train_step_timing in s: 1.533 | consumed_samples: 3824 | val_loss: 0.2642
Training epoch 0, iteration 239/998 | lr: 9.053e-05 | global_batch_size: 16 | global_step: 239 | reduced_train_loss: 0.2356 | train_step_timing in s: 1.385 | consumed_samples: 3840 | val_loss: 0.2642
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:48:23 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 240 : Start time: 1759794502.667s : Save duration: 0.588s
[NeMo I 2025-10-06 23:48:26 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=239-consumed_samples=3840.0-last.ckpt
Training epoch 0, iteration 240/998 | lr: 9.043e-05 | global_batch_size: 16 | global_step: 240 | reduced_train_loss: 0.3039 | train_step_timing in s: 1.321 | consumed_samples: 3856 | val_loss: 0.2399
[NeMo I 2025-10-06 23:48:28 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 241/998 | lr: 9.033e-05 | global_batch_size: 16 | global_step: 241 | reduced_train_loss: 0.2319 | train_step_timing in s: 1.269 | consumed_samples: 3872 | val_loss: 0.2399
[NeMo I 2025-10-06 23:48:33 nemo_logging:393] Successfully saved checkpoint from iteration     240 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=239-consumed_samples=3840.0-last.ckpt
[NeMo I 2025-10-06 23:48:33 nemo_logging:393] Async checkpoint save for step 240 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=239-consumed_samples=3840.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:48:36 nemo_logging:393] Async finalization time took 4.572 s
Training epoch 0, iteration 242/998 | lr: 9.024e-05 | global_batch_size: 16 | global_step: 242 | reduced_train_loss: 0.2908 | train_step_timing in s: 1.265 | consumed_samples: 3888 | val_loss: 0.2399
Training epoch 0, iteration 243/998 | lr: 9.014e-05 | global_batch_size: 16 | global_step: 243 | reduced_train_loss: 0.2289 | train_step_timing in s: 1.662 | consumed_samples: 3904 | val_loss: 0.2399
Training epoch 0, iteration 244/998 | lr: 9.004e-05 | global_batch_size: 16 | global_step: 244 | reduced_train_loss: 0.2059 | train_step_timing in s: 1.245 | consumed_samples: 3920 | val_loss: 0.2399
Training epoch 0, iteration 245/998 | lr: 8.994e-05 | global_batch_size: 16 | global_step: 245 | reduced_train_loss: 0.4384 | train_step_timing in s: 1.734 | consumed_samples: 3936 | val_loss: 0.2399
Training epoch 0, iteration 246/998 | lr: 8.984e-05 | global_batch_size: 16 | global_step: 246 | reduced_train_loss: 0.2401 | train_step_timing in s: 1.668 | consumed_samples: 3952 | val_loss: 0.2399
Training epoch 0, iteration 247/998 | lr: 8.974e-05 | global_batch_size: 16 | global_step: 247 | reduced_train_loss: 0.1297 | train_step_timing in s: 1.282 | consumed_samples: 3968 | val_loss: 0.2399
Training epoch 0, iteration 248/998 | lr: 8.964e-05 | global_batch_size: 16 | global_step: 248 | reduced_train_loss: 0.1912 | train_step_timing in s: 1.214 | consumed_samples: 3984 | val_loss: 0.2399
Training epoch 0, iteration 249/998 | lr: 8.954e-05 | global_batch_size: 16 | global_step: 249 | reduced_train_loss: 0.2272 | train_step_timing in s: 1.281 | consumed_samples: 4000 | val_loss: 0.2399
Epoch 0, global step 249: 'val_loss' reached 0.23988 (best 0.23988), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=249-consumed_samples=4000.0.ckpt' as top 2
[NeMo I 2025-10-06 23:49:17 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 249 : Start time: 1759794557.371s : Save duration: 0.563s
[NeMo I 2025-10-06 23:49:21 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=249-consumed_samples=4000.0.ckpt
[NeMo I 2025-10-06 23:49:21 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 250/998 | lr: 8.944e-05 | global_batch_size: 16 | global_step: 250 | reduced_train_loss: 0.2301 | train_step_timing in s: 1.305 | consumed_samples: 4016 | val_loss: 0.2399
[NeMo I 2025-10-06 23:49:25 nemo_logging:393] Successfully saved checkpoint from iteration     249 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=249-consumed_samples=4000.0.ckpt
[NeMo I 2025-10-06 23:49:25 nemo_logging:393] Async checkpoint save for step 250 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=249-consumed_samples=4000.0.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:49:32 nemo_logging:393] Async finalization time took 7.750 s
Training epoch 0, iteration 251/998 | lr: 8.933e-05 | global_batch_size: 16 | global_step: 251 | reduced_train_loss: 0.2842 | train_step_timing in s: 1.387 | consumed_samples: 4032 | val_loss: 0.2399
Training epoch 0, iteration 252/998 | lr: 8.923e-05 | global_batch_size: 16 | global_step: 252 | reduced_train_loss: 0.1747 | train_step_timing in s: 1.277 | consumed_samples: 4048 | val_loss: 0.2399
Training epoch 0, iteration 253/998 | lr: 8.913e-05 | global_batch_size: 16 | global_step: 253 | reduced_train_loss: 0.2289 | train_step_timing in s: 1.867 | consumed_samples: 4064 | val_loss: 0.2399
Training epoch 0, iteration 254/998 | lr: 8.903e-05 | global_batch_size: 16 | global_step: 254 | reduced_train_loss: 0.1746 | train_step_timing in s: 1.342 | consumed_samples: 4080 | val_loss: 0.2399
Training epoch 0, iteration 255/998 | lr: 8.892e-05 | global_batch_size: 16 | global_step: 255 | reduced_train_loss: 0.1604 | train_step_timing in s: 1.246 | consumed_samples: 4096 | val_loss: 0.2399
Training epoch 0, iteration 256/998 | lr: 8.882e-05 | global_batch_size: 16 | global_step: 256 | reduced_train_loss: 0.425 | train_step_timing in s: 1.202 | consumed_samples: 4112 | val_loss: 0.2399
Training epoch 0, iteration 257/998 | lr: 8.871e-05 | global_batch_size: 16 | global_step: 257 | reduced_train_loss: 0.1911 | train_step_timing in s: 1.54 | consumed_samples: 4128 | val_loss: 0.2399
Training epoch 0, iteration 258/998 | lr: 8.861e-05 | global_batch_size: 16 | global_step: 258 | reduced_train_loss: 0.3981 | train_step_timing in s: 1.254 | consumed_samples: 4144 | val_loss: 0.2399
Training epoch 0, iteration 259/998 | lr: 8.85e-05 | global_batch_size: 16 | global_step: 259 | reduced_train_loss: 0.3293 | train_step_timing in s: 1.204 | consumed_samples: 4160 | val_loss: 0.2399
Training epoch 0, iteration 260/998 | lr: 8.84e-05 | global_batch_size: 16 | global_step: 260 | reduced_train_loss: 0.07385 | train_step_timing in s: 1.267 | consumed_samples: 4176 | val_loss: 0.2399
Training epoch 0, iteration 261/998 | lr: 8.829e-05 | global_batch_size: 16 | global_step: 261 | reduced_train_loss: 0.2473 | train_step_timing in s: 1.244 | consumed_samples: 4192 | val_loss: 0.2399
Training epoch 0, iteration 262/998 | lr: 8.818e-05 | global_batch_size: 16 | global_step: 262 | reduced_train_loss: 0.24 | train_step_timing in s: 1.293 | consumed_samples: 4208 | val_loss: 0.2399
Training epoch 0, iteration 263/998 | lr: 8.808e-05 | global_batch_size: 16 | global_step: 263 | reduced_train_loss: 0.2042 | train_step_timing in s: 1.723 | consumed_samples: 4224 | val_loss: 0.2399
Training epoch 0, iteration 264/998 | lr: 8.797e-05 | global_batch_size: 16 | global_step: 264 | reduced_train_loss: 0.1772 | train_step_timing in s: 2.139 | consumed_samples: 4240 | val_loss: 0.2399
Training epoch 0, iteration 265/998 | lr: 8.786e-05 | global_batch_size: 16 | global_step: 265 | reduced_train_loss: 0.1012 | train_step_timing in s: 1.533 | consumed_samples: 4256 | val_loss: 0.2399
Training epoch 0, iteration 266/998 | lr: 8.775e-05 | global_batch_size: 16 | global_step: 266 | reduced_train_loss: 0.09107 | train_step_timing in s: 1.811 | consumed_samples: 4272 | val_loss: 0.2399
Training epoch 0, iteration 267/998 | lr: 8.764e-05 | global_batch_size: 16 | global_step: 267 | reduced_train_loss: 0.1719 | train_step_timing in s: 1.284 | consumed_samples: 4288 | val_loss: 0.2399
Training epoch 0, iteration 268/998 | lr: 8.754e-05 | global_batch_size: 16 | global_step: 268 | reduced_train_loss: 0.3274 | train_step_timing in s: 1.371 | consumed_samples: 4304 | val_loss: 0.2399
Training epoch 0, iteration 269/998 | lr: 8.743e-05 | global_batch_size: 16 | global_step: 269 | reduced_train_loss: 0.08243 | train_step_timing in s: 1.276 | consumed_samples: 4320 | val_loss: 0.2399
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-06 23:55:15 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 270 : Start time: 1759794914.815s : Save duration: 0.563s
[NeMo I 2025-10-06 23:55:18 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=269-consumed_samples=4320.0-last.ckpt
Training epoch 0, iteration 270/998 | lr: 8.732e-05 | global_batch_size: 16 | global_step: 270 | reduced_train_loss: 0.3182 | train_step_timing in s: 1.365 | consumed_samples: 4336 | val_loss: 0.2338
[NeMo I 2025-10-06 23:55:20 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 271/998 | lr: 8.721e-05 | global_batch_size: 16 | global_step: 271 | reduced_train_loss: 0.1764 | train_step_timing in s: 1.299 | consumed_samples: 4352 | val_loss: 0.2338
[NeMo I 2025-10-06 23:55:24 nemo_logging:393] Successfully saved checkpoint from iteration     270 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=269-consumed_samples=4320.0-last.ckpt
[NeMo I 2025-10-06 23:55:24 nemo_logging:393] Async checkpoint save for step 270 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=269-consumed_samples=4320.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-06 23:55:24 nemo_logging:393] Async finalization time took 1.485 s
Training epoch 0, iteration 272/998 | lr: 8.709e-05 | global_batch_size: 16 | global_step: 272 | reduced_train_loss: 0.3198 | train_step_timing in s: 1.623 | consumed_samples: 4368 | val_loss: 0.2338
Training epoch 0, iteration 273/998 | lr: 8.698e-05 | global_batch_size: 16 | global_step: 273 | reduced_train_loss: 0.343 | train_step_timing in s: 1.356 | consumed_samples: 4384 | val_loss: 0.2338
Training epoch 0, iteration 274/998 | lr: 8.687e-05 | global_batch_size: 16 | global_step: 274 | reduced_train_loss: 0.2432 | train_step_timing in s: 1.381 | consumed_samples: 4400 | val_loss: 0.2338
Training epoch 0, iteration 275/998 | lr: 8.676e-05 | global_batch_size: 16 | global_step: 275 | reduced_train_loss: 0.2929 | train_step_timing in s: 1.254 | consumed_samples: 4416 | val_loss: 0.2338
Training epoch 0, iteration 276/998 | lr: 8.665e-05 | global_batch_size: 16 | global_step: 276 | reduced_train_loss: 0.1065 | train_step_timing in s: 1.686 | consumed_samples: 4432 | val_loss: 0.2338
Training epoch 0, iteration 277/998 | lr: 8.653e-05 | global_batch_size: 16 | global_step: 277 | reduced_train_loss: 0.3836 | train_step_timing in s: 1.261 | consumed_samples: 4448 | val_loss: 0.2338
Training epoch 0, iteration 278/998 | lr: 8.642e-05 | global_batch_size: 16 | global_step: 278 | reduced_train_loss: 0.1411 | train_step_timing in s: 1.734 | consumed_samples: 4464 | val_loss: 0.2338
Training epoch 0, iteration 279/998 | lr: 8.631e-05 | global_batch_size: 16 | global_step: 279 | reduced_train_loss: 0.225 | train_step_timing in s: 2.784 | consumed_samples: 4480 | val_loss: 0.2338
Training epoch 0, iteration 280/998 | lr: 8.619e-05 | global_batch_size: 16 | global_step: 280 | reduced_train_loss: 0.07503 | train_step_timing in s: 1.324 | consumed_samples: 4496 | val_loss: 0.2338
Training epoch 0, iteration 281/998 | lr: 8.608e-05 | global_batch_size: 16 | global_step: 281 | reduced_train_loss: 0.2237 | train_step_timing in s: 1.381 | consumed_samples: 4512 | val_loss: 0.2338
Training epoch 0, iteration 282/998 | lr: 8.596e-05 | global_batch_size: 16 | global_step: 282 | reduced_train_loss: 0.2554 | train_step_timing in s: 1.664 | consumed_samples: 4528 | val_loss: 0.2338
Training epoch 0, iteration 283/998 | lr: 8.585e-05 | global_batch_size: 16 | global_step: 283 | reduced_train_loss: 0.2092 | train_step_timing in s: 1.355 | consumed_samples: 4544 | val_loss: 0.2338
Training epoch 0, iteration 284/998 | lr: 8.573e-05 | global_batch_size: 16 | global_step: 284 | reduced_train_loss: 0.1919 | train_step_timing in s: 1.665 | consumed_samples: 4560 | val_loss: 0.2338
Training epoch 0, iteration 285/998 | lr: 8.562e-05 | global_batch_size: 16 | global_step: 285 | reduced_train_loss: 0.4051 | train_step_timing in s: 1.381 | consumed_samples: 4576 | val_loss: 0.2338
Training epoch 0, iteration 286/998 | lr: 8.55e-05 | global_batch_size: 16 | global_step: 286 | reduced_train_loss: 0.3157 | train_step_timing in s: 1.363 | consumed_samples: 4592 | val_loss: 0.2338
Training epoch 0, iteration 287/998 | lr: 8.538e-05 | global_batch_size: 16 | global_step: 287 | reduced_train_loss: 0.1913 | train_step_timing in s: 1.327 | consumed_samples: 4608 | val_loss: 0.2338
Training epoch 0, iteration 288/998 | lr: 8.527e-05 | global_batch_size: 16 | global_step: 288 | reduced_train_loss: 0.3939 | train_step_timing in s: 1.679 | consumed_samples: 4624 | val_loss: 0.2338
Training epoch 0, iteration 289/998 | lr: 8.515e-05 | global_batch_size: 16 | global_step: 289 | reduced_train_loss: 0.2169 | train_step_timing in s: 1.363 | consumed_samples: 4640 | val_loss: 0.2338
Training epoch 0, iteration 290/998 | lr: 8.503e-05 | global_batch_size: 16 | global_step: 290 | reduced_train_loss: 0.2974 | train_step_timing in s: 1.188 | consumed_samples: 4656 | val_loss: 0.2338
Training epoch 0, iteration 291/998 | lr: 8.491e-05 | global_batch_size: 16 | global_step: 291 | reduced_train_loss: 0.1327 | train_step_timing in s: 1.385 | consumed_samples: 4672 | val_loss: 0.2338
Training epoch 0, iteration 292/998 | lr: 8.48e-05 | global_batch_size: 16 | global_step: 292 | reduced_train_loss: 0.3122 | train_step_timing in s: 1.388 | consumed_samples: 4688 | val_loss: 0.2338
Training epoch 0, iteration 293/998 | lr: 8.468e-05 | global_batch_size: 16 | global_step: 293 | reduced_train_loss: 0.1545 | train_step_timing in s: 1.199 | consumed_samples: 4704 | val_loss: 0.2338
Training epoch 0, iteration 294/998 | lr: 8.456e-05 | global_batch_size: 16 | global_step: 294 | reduced_train_loss: 0.3638 | train_step_timing in s: 1.258 | consumed_samples: 4720 | val_loss: 0.2338
Training epoch 0, iteration 295/998 | lr: 8.444e-05 | global_batch_size: 16 | global_step: 295 | reduced_train_loss: 0.1852 | train_step_timing in s: 1.716 | consumed_samples: 4736 | val_loss: 0.2338
Training epoch 0, iteration 296/998 | lr: 8.432e-05 | global_batch_size: 16 | global_step: 296 | reduced_train_loss: 0.1156 | train_step_timing in s: 1.279 | consumed_samples: 4752 | val_loss: 0.2338
Training epoch 0, iteration 297/998 | lr: 8.42e-05 | global_batch_size: 16 | global_step: 297 | reduced_train_loss: 0.3975 | train_step_timing in s: 1.811 | consumed_samples: 4768 | val_loss: 0.2338
Training epoch 0, iteration 298/998 | lr: 8.408e-05 | global_batch_size: 16 | global_step: 298 | reduced_train_loss: 0.6447 | train_step_timing in s: 1.331 | consumed_samples: 4784 | val_loss: 0.2338
Training epoch 0, iteration 299/998 | lr: 8.395e-05 | global_batch_size: 16 | global_step: 299 | reduced_train_loss: 0.2167 | train_step_timing in s: 1.773 | consumed_samples: 4800 | val_loss: 0.2338
Epoch 0, global step 299: 'val_loss' reached 0.23379 (best 0.23379), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=299-consumed_samples=4800.0.ckpt' as top 2
[NeMo I 2025-10-06 23:57:30 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 299 : Start time: 1759795049.988s : Save duration: 0.543s
[NeMo I 2025-10-06 23:57:33 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=299-consumed_samples=4800.0.ckpt
[NeMo I 2025-10-06 23:57:33 nemo_logging:393] Async finalization time took 0.000 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 300/998 | lr: 8.383e-05 | global_batch_size: 16 | global_step: 300 | reduced_train_loss: 0.1654 | train_step_timing in s: 1.386 | consumed_samples: 4816 | val_loss: 0.2325
[NeMo I 2025-10-07 00:01:50 nemo_logging:393] Successfully saved checkpoint from iteration     299 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=299-consumed_samples=4800.0.ckpt
[NeMo I 2025-10-07 00:01:50 nemo_logging:393] Async checkpoint save for step 300 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=299-consumed_samples=4800.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:01:56 nemo_logging:393] Async finalization time took 7.786 s
Training epoch 0, iteration 301/998 | lr: 8.371e-05 | global_batch_size: 16 | global_step: 301 | reduced_train_loss: 0.1927 | train_step_timing in s: 1.527 | consumed_samples: 4832 | val_loss: 0.2325
Training epoch 0, iteration 302/998 | lr: 8.359e-05 | global_batch_size: 16 | global_step: 302 | reduced_train_loss: 0.3298 | train_step_timing in s: 1.223 | consumed_samples: 4848 | val_loss: 0.2325
Training epoch 0, iteration 303/998 | lr: 8.346e-05 | global_batch_size: 16 | global_step: 303 | reduced_train_loss: 0.1438 | train_step_timing in s: 1.825 | consumed_samples: 4864 | val_loss: 0.2325
Training epoch 0, iteration 304/998 | lr: 8.334e-05 | global_batch_size: 16 | global_step: 304 | reduced_train_loss: 0.2862 | train_step_timing in s: 1.38 | consumed_samples: 4880 | val_loss: 0.2325
Training epoch 0, iteration 305/998 | lr: 8.322e-05 | global_batch_size: 16 | global_step: 305 | reduced_train_loss: 0.1558 | train_step_timing in s: 1.338 | consumed_samples: 4896 | val_loss: 0.2325
Training epoch 0, iteration 306/998 | lr: 8.309e-05 | global_batch_size: 16 | global_step: 306 | reduced_train_loss: 0.2233 | train_step_timing in s: 1.771 | consumed_samples: 4912 | val_loss: 0.2325
Training epoch 0, iteration 307/998 | lr: 8.297e-05 | global_batch_size: 16 | global_step: 307 | reduced_train_loss: 0.2256 | train_step_timing in s: 1.251 | consumed_samples: 4928 | val_loss: 0.2325
Training epoch 0, iteration 308/998 | lr: 8.285e-05 | global_batch_size: 16 | global_step: 308 | reduced_train_loss: 0.214 | train_step_timing in s: 1.562 | consumed_samples: 4944 | val_loss: 0.2325
Training epoch 0, iteration 309/998 | lr: 8.272e-05 | global_batch_size: 16 | global_step: 309 | reduced_train_loss: 0.1895 | train_step_timing in s: 1.206 | consumed_samples: 4960 | val_loss: 0.2325
Training epoch 0, iteration 310/998 | lr: 8.259e-05 | global_batch_size: 16 | global_step: 310 | reduced_train_loss: 0.2391 | train_step_timing in s: 1.345 | consumed_samples: 4976 | val_loss: 0.2325
Training epoch 0, iteration 311/998 | lr: 8.247e-05 | global_batch_size: 16 | global_step: 311 | reduced_train_loss: 0.1874 | train_step_timing in s: 1.805 | consumed_samples: 4992 | val_loss: 0.2325
Training epoch 0, iteration 312/998 | lr: 8.234e-05 | global_batch_size: 16 | global_step: 312 | reduced_train_loss: 0.164 | train_step_timing in s: 1.252 | consumed_samples: 5008 | val_loss: 0.2325
Training epoch 0, iteration 313/998 | lr: 8.222e-05 | global_batch_size: 16 | global_step: 313 | reduced_train_loss: 0.122 | train_step_timing in s: 1.269 | consumed_samples: 5024 | val_loss: 0.2325
Training epoch 0, iteration 314/998 | lr: 8.209e-05 | global_batch_size: 16 | global_step: 314 | reduced_train_loss: 0.324 | train_step_timing in s: 1.671 | consumed_samples: 5040 | val_loss: 0.2325
Training epoch 0, iteration 315/998 | lr: 8.196e-05 | global_batch_size: 16 | global_step: 315 | reduced_train_loss: 0.2816 | train_step_timing in s: 1.663 | consumed_samples: 5056 | val_loss: 0.2325
Training epoch 0, iteration 316/998 | lr: 8.184e-05 | global_batch_size: 16 | global_step: 316 | reduced_train_loss: 0.4697 | train_step_timing in s: 1.323 | consumed_samples: 5072 | val_loss: 0.2325
Training epoch 0, iteration 317/998 | lr: 8.171e-05 | global_batch_size: 16 | global_step: 317 | reduced_train_loss: 0.2323 | train_step_timing in s: 1.27 | consumed_samples: 5088 | val_loss: 0.2325
Training epoch 0, iteration 318/998 | lr: 8.158e-05 | global_batch_size: 16 | global_step: 318 | reduced_train_loss: 0.1885 | train_step_timing in s: 1.327 | consumed_samples: 5104 | val_loss: 0.2325
Training epoch 0, iteration 319/998 | lr: 8.145e-05 | global_batch_size: 16 | global_step: 319 | reduced_train_loss: 0.1939 | train_step_timing in s: 1.274 | consumed_samples: 5120 | val_loss: 0.2325
Training epoch 0, iteration 320/998 | lr: 8.132e-05 | global_batch_size: 16 | global_step: 320 | reduced_train_loss: 0.2083 | train_step_timing in s: 1.315 | consumed_samples: 5136 | val_loss: 0.2325
Training epoch 0, iteration 321/998 | lr: 8.119e-05 | global_batch_size: 16 | global_step: 321 | reduced_train_loss: 0.06521 | train_step_timing in s: 1.703 | consumed_samples: 5152 | val_loss: 0.2325
Training epoch 0, iteration 322/998 | lr: 8.106e-05 | global_batch_size: 16 | global_step: 322 | reduced_train_loss: 0.195 | train_step_timing in s: 1.309 | consumed_samples: 5168 | val_loss: 0.2325
Training epoch 0, iteration 323/998 | lr: 8.093e-05 | global_batch_size: 16 | global_step: 323 | reduced_train_loss: 0.1594 | train_step_timing in s: 1.383 | consumed_samples: 5184 | val_loss: 0.2325
Training epoch 0, iteration 324/998 | lr: 8.08e-05 | global_batch_size: 16 | global_step: 324 | reduced_train_loss: 0.1942 | train_step_timing in s: 1.276 | consumed_samples: 5200 | val_loss: 0.2325
Training epoch 0, iteration 325/998 | lr: 8.067e-05 | global_batch_size: 16 | global_step: 325 | reduced_train_loss: 0.4647 | train_step_timing in s: 1.691 | consumed_samples: 5216 | val_loss: 0.2325
Training epoch 0, iteration 326/998 | lr: 8.054e-05 | global_batch_size: 16 | global_step: 326 | reduced_train_loss: 0.344 | train_step_timing in s: 1.755 | consumed_samples: 5232 | val_loss: 0.2325
Training epoch 0, iteration 327/998 | lr: 8.041e-05 | global_batch_size: 16 | global_step: 327 | reduced_train_loss: 0.2411 | train_step_timing in s: 1.206 | consumed_samples: 5248 | val_loss: 0.2325
Training epoch 0, iteration 328/998 | lr: 8.028e-05 | global_batch_size: 16 | global_step: 328 | reduced_train_loss: 0.3473 | train_step_timing in s: 1.252 | consumed_samples: 5264 | val_loss: 0.2325
Training epoch 0, iteration 329/998 | lr: 8.015e-05 | global_batch_size: 16 | global_step: 329 | reduced_train_loss: 0.231 | train_step_timing in s: 1.844 | consumed_samples: 5280 | val_loss: 0.2325
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:08:32 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 330 : Start time: 1759795712.424s : Save duration: 0.554s
[NeMo I 2025-10-07 00:08:36 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=329-consumed_samples=5280.0-last.ckpt
Training epoch 0, iteration 330/998 | lr: 8.002e-05 | global_batch_size: 16 | global_step: 330 | reduced_train_loss: 0.2895 | train_step_timing in s: 1.371 | consumed_samples: 5296 | val_loss: 0.2387
[NeMo I 2025-10-07 00:08:37 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 331/998 | lr: 7.988e-05 | global_batch_size: 16 | global_step: 331 | reduced_train_loss: 0.1908 | train_step_timing in s: 1.194 | consumed_samples: 5312 | val_loss: 0.2387
[NeMo I 2025-10-07 00:08:41 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 332/998 | lr: 7.975e-05 | global_batch_size: 16 | global_step: 332 | reduced_train_loss: 0.3361 | train_step_timing in s: 1.38 | consumed_samples: 5328 | val_loss: 0.2387
[NeMo I 2025-10-07 00:08:45 nemo_logging:393] Successfully saved checkpoint from iteration     330 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=329-consumed_samples=5280.0-last.ckpt
[NeMo I 2025-10-07 00:08:45 nemo_logging:393] Async checkpoint save for step 330 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=329-consumed_samples=5280.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:08:46 nemo_logging:393] Async finalization time took 1.393 s
Training epoch 0, iteration 333/998 | lr: 7.962e-05 | global_batch_size: 16 | global_step: 333 | reduced_train_loss: 0.1868 | train_step_timing in s: 1.264 | consumed_samples: 5344 | val_loss: 0.2387
Training epoch 0, iteration 334/998 | lr: 7.948e-05 | global_batch_size: 16 | global_step: 334 | reduced_train_loss: 0.3244 | train_step_timing in s: 1.769 | consumed_samples: 5360 | val_loss: 0.2387
Training epoch 0, iteration 335/998 | lr: 7.935e-05 | global_batch_size: 16 | global_step: 335 | reduced_train_loss: 0.09679 | train_step_timing in s: 1.366 | consumed_samples: 5376 | val_loss: 0.2387
Training epoch 0, iteration 336/998 | lr: 7.921e-05 | global_batch_size: 16 | global_step: 336 | reduced_train_loss: 0.3617 | train_step_timing in s: 1.36 | consumed_samples: 5392 | val_loss: 0.2387
Training epoch 0, iteration 337/998 | lr: 7.908e-05 | global_batch_size: 16 | global_step: 337 | reduced_train_loss: 0.1765 | train_step_timing in s: 1.319 | consumed_samples: 5408 | val_loss: 0.2387
Training epoch 0, iteration 338/998 | lr: 7.895e-05 | global_batch_size: 16 | global_step: 338 | reduced_train_loss: 0.2281 | train_step_timing in s: 1.228 | consumed_samples: 5424 | val_loss: 0.2387
Training epoch 0, iteration 339/998 | lr: 7.881e-05 | global_batch_size: 16 | global_step: 339 | reduced_train_loss: 0.1808 | train_step_timing in s: 1.321 | consumed_samples: 5440 | val_loss: 0.2387
Training epoch 0, iteration 340/998 | lr: 7.868e-05 | global_batch_size: 16 | global_step: 340 | reduced_train_loss: 0.3273 | train_step_timing in s: 1.228 | consumed_samples: 5456 | val_loss: 0.2387
Training epoch 0, iteration 341/998 | lr: 7.854e-05 | global_batch_size: 16 | global_step: 341 | reduced_train_loss: 0.2963 | train_step_timing in s: 1.385 | consumed_samples: 5472 | val_loss: 0.2387
Training epoch 0, iteration 342/998 | lr: 7.84e-05 | global_batch_size: 16 | global_step: 342 | reduced_train_loss: 0.3192 | train_step_timing in s: 1.24 | consumed_samples: 5488 | val_loss: 0.2387
Training epoch 0, iteration 343/998 | lr: 7.827e-05 | global_batch_size: 16 | global_step: 343 | reduced_train_loss: 0.2227 | train_step_timing in s: 1.24 | consumed_samples: 5504 | val_loss: 0.2387
Training epoch 0, iteration 344/998 | lr: 7.813e-05 | global_batch_size: 16 | global_step: 344 | reduced_train_loss: 0.2218 | train_step_timing in s: 1.339 | consumed_samples: 5520 | val_loss: 0.2387
Training epoch 0, iteration 345/998 | lr: 7.799e-05 | global_batch_size: 16 | global_step: 345 | reduced_train_loss: 0.2709 | train_step_timing in s: 1.704 | consumed_samples: 5536 | val_loss: 0.2387
Training epoch 0, iteration 346/998 | lr: 7.786e-05 | global_batch_size: 16 | global_step: 346 | reduced_train_loss: 0.3989 | train_step_timing in s: 1.569 | consumed_samples: 5552 | val_loss: 0.2387
Training epoch 0, iteration 347/998 | lr: 7.772e-05 | global_batch_size: 16 | global_step: 347 | reduced_train_loss: 0.1584 | train_step_timing in s: 1.674 | consumed_samples: 5568 | val_loss: 0.2387
Training epoch 0, iteration 348/998 | lr: 7.758e-05 | global_batch_size: 16 | global_step: 348 | reduced_train_loss: 0.1637 | train_step_timing in s: 1.236 | consumed_samples: 5584 | val_loss: 0.2387
Training epoch 0, iteration 349/998 | lr: 7.744e-05 | global_batch_size: 16 | global_step: 349 | reduced_train_loss: 0.266 | train_step_timing in s: 1.232 | consumed_samples: 5600 | val_loss: 0.2387
Epoch 0, global step 349: 'val_loss' reached 0.23874 (best 0.23379), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=349-consumed_samples=5600.0.ckpt' as top 2
[NeMo I 2025-10-07 00:10:05 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 349 : Start time: 1759795804.982s : Save duration: 0.549s
[NeMo I 2025-10-07 00:10:08 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=349-consumed_samples=5600.0.ckpt
[NeMo I 2025-10-07 00:10:08 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 350/998 | lr: 7.73e-05 | global_batch_size: 16 | global_step: 350 | reduced_train_loss: 0.2557 | train_step_timing in s: 1.342 | consumed_samples: 5616 | val_loss: 0.2387
[NeMo I 2025-10-07 00:10:13 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 351/998 | lr: 7.716e-05 | global_batch_size: 16 | global_step: 351 | reduced_train_loss: 0.1376 | train_step_timing in s: 1.541 | consumed_samples: 5632 | val_loss: 0.2387
[NeMo I 2025-10-07 00:10:17 nemo_logging:393] Successfully saved checkpoint from iteration     349 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=349-consumed_samples=5600.0.ckpt
[NeMo I 2025-10-07 00:10:18 nemo_logging:393] Async checkpoint save for step 350 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.24-step=349-consumed_samples=5600.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:10:25 nemo_logging:393] Async finalization time took 8.529 s
Training epoch 0, iteration 352/998 | lr: 7.703e-05 | global_batch_size: 16 | global_step: 352 | reduced_train_loss: 0.2246 | train_step_timing in s: 1.277 | consumed_samples: 5648 | val_loss: 0.2387
Training epoch 0, iteration 353/998 | lr: 7.689e-05 | global_batch_size: 16 | global_step: 353 | reduced_train_loss: 0.3737 | train_step_timing in s: 1.673 | consumed_samples: 5664 | val_loss: 0.2387
Training epoch 0, iteration 354/998 | lr: 7.675e-05 | global_batch_size: 16 | global_step: 354 | reduced_train_loss: 0.1479 | train_step_timing in s: 1.318 | consumed_samples: 5680 | val_loss: 0.2387
Training epoch 0, iteration 355/998 | lr: 7.661e-05 | global_batch_size: 16 | global_step: 355 | reduced_train_loss: 0.3599 | train_step_timing in s: 1.391 | consumed_samples: 5696 | val_loss: 0.2387
Training epoch 0, iteration 356/998 | lr: 7.647e-05 | global_batch_size: 16 | global_step: 356 | reduced_train_loss: 0.2756 | train_step_timing in s: 1.26 | consumed_samples: 5712 | val_loss: 0.2387
Training epoch 0, iteration 357/998 | lr: 7.633e-05 | global_batch_size: 16 | global_step: 357 | reduced_train_loss: 0.2025 | train_step_timing in s: 1.247 | consumed_samples: 5728 | val_loss: 0.2387
Training epoch 0, iteration 358/998 | lr: 7.618e-05 | global_batch_size: 16 | global_step: 358 | reduced_train_loss: 0.3081 | train_step_timing in s: 1.286 | consumed_samples: 5744 | val_loss: 0.2387
Training epoch 0, iteration 359/998 | lr: 7.604e-05 | global_batch_size: 16 | global_step: 359 | reduced_train_loss: 0.3316 | train_step_timing in s: 1.758 | consumed_samples: 5760 | val_loss: 0.2387
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:15:23 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 360 : Start time: 1759796122.557s : Save duration: 0.573s
[NeMo I 2025-10-07 00:15:26 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=359-consumed_samples=5760.0-last.ckpt
Training epoch 0, iteration 360/998 | lr: 7.59e-05 | global_batch_size: 16 | global_step: 360 | reduced_train_loss: 0.2202 | train_step_timing in s: 1.274 | consumed_samples: 5776 | val_loss: 0.2298
[NeMo I 2025-10-07 00:15:27 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 361/998 | lr: 7.576e-05 | global_batch_size: 16 | global_step: 361 | reduced_train_loss: 0.2444 | train_step_timing in s: 1.749 | consumed_samples: 5792 | val_loss: 0.2298
[NeMo I 2025-10-07 00:15:32 nemo_logging:393] Successfully saved checkpoint from iteration     360 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=359-consumed_samples=5760.0-last.ckpt
[NeMo I 2025-10-07 00:15:32 nemo_logging:393] Async checkpoint save for step 360 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=359-consumed_samples=5760.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:15:32 nemo_logging:393] Async finalization time took 1.407 s
Training epoch 0, iteration 362/998 | lr: 7.562e-05 | global_batch_size: 16 | global_step: 362 | reduced_train_loss: 0.1904 | train_step_timing in s: 1.682 | consumed_samples: 5808 | val_loss: 0.2298
Training epoch 0, iteration 363/998 | lr: 7.548e-05 | global_batch_size: 16 | global_step: 363 | reduced_train_loss: 0.1552 | train_step_timing in s: 1.27 | consumed_samples: 5824 | val_loss: 0.2298
Training epoch 0, iteration 364/998 | lr: 7.533e-05 | global_batch_size: 16 | global_step: 364 | reduced_train_loss: 0.2996 | train_step_timing in s: 1.665 | consumed_samples: 5840 | val_loss: 0.2298
Training epoch 0, iteration 365/998 | lr: 7.519e-05 | global_batch_size: 16 | global_step: 365 | reduced_train_loss: 0.2222 | train_step_timing in s: 1.207 | consumed_samples: 5856 | val_loss: 0.2298
Training epoch 0, iteration 366/998 | lr: 7.505e-05 | global_batch_size: 16 | global_step: 366 | reduced_train_loss: 0.1925 | train_step_timing in s: 1.235 | consumed_samples: 5872 | val_loss: 0.2298
Training epoch 0, iteration 367/998 | lr: 7.49e-05 | global_batch_size: 16 | global_step: 367 | reduced_train_loss: 0.1887 | train_step_timing in s: 1.213 | consumed_samples: 5888 | val_loss: 0.2298
Training epoch 0, iteration 368/998 | lr: 7.476e-05 | global_batch_size: 16 | global_step: 368 | reduced_train_loss: 0.2298 | train_step_timing in s: 1.317 | consumed_samples: 5904 | val_loss: 0.2298
Training epoch 0, iteration 369/998 | lr: 7.462e-05 | global_batch_size: 16 | global_step: 369 | reduced_train_loss: 0.3545 | train_step_timing in s: 1.33 | consumed_samples: 5920 | val_loss: 0.2298
Training epoch 0, iteration 370/998 | lr: 7.447e-05 | global_batch_size: 16 | global_step: 370 | reduced_train_loss: 0.1945 | train_step_timing in s: 1.368 | consumed_samples: 5936 | val_loss: 0.2298
Training epoch 0, iteration 371/998 | lr: 7.433e-05 | global_batch_size: 16 | global_step: 371 | reduced_train_loss: 0.2823 | train_step_timing in s: 1.671 | consumed_samples: 5952 | val_loss: 0.2298
Training epoch 0, iteration 372/998 | lr: 7.418e-05 | global_batch_size: 16 | global_step: 372 | reduced_train_loss: 0.3178 | train_step_timing in s: 1.664 | consumed_samples: 5968 | val_loss: 0.2298
Training epoch 0, iteration 373/998 | lr: 7.404e-05 | global_batch_size: 16 | global_step: 373 | reduced_train_loss: 0.3152 | train_step_timing in s: 1.665 | consumed_samples: 5984 | val_loss: 0.2298
Training epoch 0, iteration 374/998 | lr: 7.389e-05 | global_batch_size: 16 | global_step: 374 | reduced_train_loss: 0.1169 | train_step_timing in s: 1.326 | consumed_samples: 6000 | val_loss: 0.2298
Training epoch 0, iteration 375/998 | lr: 7.375e-05 | global_batch_size: 16 | global_step: 375 | reduced_train_loss: 0.1975 | train_step_timing in s: 1.957 | consumed_samples: 6016 | val_loss: 0.2298
Training epoch 0, iteration 376/998 | lr: 7.36e-05 | global_batch_size: 16 | global_step: 376 | reduced_train_loss: 0.2689 | train_step_timing in s: 1.246 | consumed_samples: 6032 | val_loss: 0.2298
Training epoch 0, iteration 377/998 | lr: 7.346e-05 | global_batch_size: 16 | global_step: 377 | reduced_train_loss: 0.1804 | train_step_timing in s: 1.346 | consumed_samples: 6048 | val_loss: 0.2298
Training epoch 0, iteration 378/998 | lr: 7.331e-05 | global_batch_size: 16 | global_step: 378 | reduced_train_loss: 0.2463 | train_step_timing in s: 1.387 | consumed_samples: 6064 | val_loss: 0.2298
Training epoch 0, iteration 379/998 | lr: 7.316e-05 | global_batch_size: 16 | global_step: 379 | reduced_train_loss: 0.2109 | train_step_timing in s: 1.243 | consumed_samples: 6080 | val_loss: 0.2298
Training epoch 0, iteration 380/998 | lr: 7.302e-05 | global_batch_size: 16 | global_step: 380 | reduced_train_loss: 0.211 | train_step_timing in s: 1.334 | consumed_samples: 6096 | val_loss: 0.2298
Training epoch 0, iteration 381/998 | lr: 7.287e-05 | global_batch_size: 16 | global_step: 381 | reduced_train_loss: 0.1011 | train_step_timing in s: 1.345 | consumed_samples: 6112 | val_loss: 0.2298
Training epoch 0, iteration 382/998 | lr: 7.272e-05 | global_batch_size: 16 | global_step: 382 | reduced_train_loss: 0.2325 | train_step_timing in s: 1.274 | consumed_samples: 6128 | val_loss: 0.2298
Training epoch 0, iteration 383/998 | lr: 7.257e-05 | global_batch_size: 16 | global_step: 383 | reduced_train_loss: 0.2062 | train_step_timing in s: 1.211 | consumed_samples: 6144 | val_loss: 0.2298
Training epoch 0, iteration 384/998 | lr: 7.243e-05 | global_batch_size: 16 | global_step: 384 | reduced_train_loss: 0.1449 | train_step_timing in s: 1.714 | consumed_samples: 6160 | val_loss: 0.2298
Training epoch 0, iteration 385/998 | lr: 7.228e-05 | global_batch_size: 16 | global_step: 385 | reduced_train_loss: 0.1606 | train_step_timing in s: 1.289 | consumed_samples: 6176 | val_loss: 0.2298
Training epoch 0, iteration 386/998 | lr: 7.213e-05 | global_batch_size: 16 | global_step: 386 | reduced_train_loss: 0.2265 | train_step_timing in s: 1.281 | consumed_samples: 6192 | val_loss: 0.2298
Training epoch 0, iteration 387/998 | lr: 7.198e-05 | global_batch_size: 16 | global_step: 387 | reduced_train_loss: 0.1856 | train_step_timing in s: 1.27 | consumed_samples: 6208 | val_loss: 0.2298
Training epoch 0, iteration 388/998 | lr: 7.183e-05 | global_batch_size: 16 | global_step: 388 | reduced_train_loss: 0.2313 | train_step_timing in s: 1.31 | consumed_samples: 6224 | val_loss: 0.2298
Training epoch 0, iteration 389/998 | lr: 7.168e-05 | global_batch_size: 16 | global_step: 389 | reduced_train_loss: 0.1688 | train_step_timing in s: 1.178 | consumed_samples: 6240 | val_loss: 0.2298
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:21:53 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 390 : Start time: 1759796512.540s : Save duration: 0.543s
[NeMo I 2025-10-07 00:21:56 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=389-consumed_samples=6240.0-last.ckpt
Training epoch 0, iteration 390/998 | lr: 7.153e-05 | global_batch_size: 16 | global_step: 390 | reduced_train_loss: 0.3099 | train_step_timing in s: 1.368 | consumed_samples: 6256 | val_loss: 0.2304
[NeMo I 2025-10-07 00:21:57 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 391/998 | lr: 7.138e-05 | global_batch_size: 16 | global_step: 391 | reduced_train_loss: 0.4459 | train_step_timing in s: 1.354 | consumed_samples: 6272 | val_loss: 0.2304
[NeMo I 2025-10-07 00:22:02 nemo_logging:393] Successfully saved checkpoint from iteration     390 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=389-consumed_samples=6240.0-last.ckpt
[NeMo I 2025-10-07 00:22:02 nemo_logging:393] Async checkpoint save for step 390 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=389-consumed_samples=6240.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:22:05 nemo_logging:393] Async finalization time took 4.683 s
Training epoch 0, iteration 392/998 | lr: 7.124e-05 | global_batch_size: 16 | global_step: 392 | reduced_train_loss: 0.09775 | train_step_timing in s: 1.157 | consumed_samples: 6288 | val_loss: 0.2304
Training epoch 0, iteration 393/998 | lr: 7.109e-05 | global_batch_size: 16 | global_step: 393 | reduced_train_loss: 0.2384 | train_step_timing in s: 1.258 | consumed_samples: 6304 | val_loss: 0.2304
Training epoch 0, iteration 394/998 | lr: 7.093e-05 | global_batch_size: 16 | global_step: 394 | reduced_train_loss: 0.1314 | train_step_timing in s: 1.304 | consumed_samples: 6320 | val_loss: 0.2304
Training epoch 0, iteration 395/998 | lr: 7.078e-05 | global_batch_size: 16 | global_step: 395 | reduced_train_loss: 0.139 | train_step_timing in s: 1.383 | consumed_samples: 6336 | val_loss: 0.2304
Training epoch 0, iteration 396/998 | lr: 7.063e-05 | global_batch_size: 16 | global_step: 396 | reduced_train_loss: 0.3021 | train_step_timing in s: 1.156 | consumed_samples: 6352 | val_loss: 0.2304
Training epoch 0, iteration 397/998 | lr: 7.048e-05 | global_batch_size: 16 | global_step: 397 | reduced_train_loss: 0.5321 | train_step_timing in s: 1.238 | consumed_samples: 6368 | val_loss: 0.2304
Training epoch 0, iteration 398/998 | lr: 7.033e-05 | global_batch_size: 16 | global_step: 398 | reduced_train_loss: 0.1102 | train_step_timing in s: 1.227 | consumed_samples: 6384 | val_loss: 0.2304
Training epoch 0, iteration 399/998 | lr: 7.018e-05 | global_batch_size: 16 | global_step: 399 | reduced_train_loss: 0.151 | train_step_timing in s: 1.957 | consumed_samples: 6400 | val_loss: 0.2304
Epoch 0, global step 399: 'val_loss' reached 0.23042 (best 0.23042), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=399-consumed_samples=6400.0.ckpt' as top 2
[NeMo I 2025-10-07 00:22:41 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 399 : Start time: 1759796561.394s : Save duration: 0.590s
[NeMo I 2025-10-07 00:22:45 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=399-consumed_samples=6400.0.ckpt
[NeMo I 2025-10-07 00:22:45 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 400/998 | lr: 7.003e-05 | global_batch_size: 16 | global_step: 400 | reduced_train_loss: 0.1476 | train_step_timing in s: 1.74 | consumed_samples: 6416 | val_loss: 0.2304
[NeMo I 2025-10-07 00:22:49 nemo_logging:393] Successfully saved checkpoint from iteration     399 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=399-consumed_samples=6400.0.ckpt
[NeMo I 2025-10-07 00:22:49 nemo_logging:393] Async checkpoint save for step 400 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.23-step=399-consumed_samples=6400.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:22:57 nemo_logging:393] Async finalization time took 8.832 s
Training epoch 0, iteration 401/998 | lr: 6.988e-05 | global_batch_size: 16 | global_step: 401 | reduced_train_loss: 0.1167 | train_step_timing in s: 1.265 | consumed_samples: 6432 | val_loss: 0.2304
Training epoch 0, iteration 402/998 | lr: 6.973e-05 | global_batch_size: 16 | global_step: 402 | reduced_train_loss: 0.2462 | train_step_timing in s: 1.349 | consumed_samples: 6448 | val_loss: 0.2304
Training epoch 0, iteration 403/998 | lr: 6.957e-05 | global_batch_size: 16 | global_step: 403 | reduced_train_loss: 0.101 | train_step_timing in s: 1.222 | consumed_samples: 6464 | val_loss: 0.2304
Training epoch 0, iteration 404/998 | lr: 6.942e-05 | global_batch_size: 16 | global_step: 404 | reduced_train_loss: 0.1436 | train_step_timing in s: 1.254 | consumed_samples: 6480 | val_loss: 0.2304
Training epoch 0, iteration 405/998 | lr: 6.927e-05 | global_batch_size: 16 | global_step: 405 | reduced_train_loss: 0.2965 | train_step_timing in s: 1.283 | consumed_samples: 6496 | val_loss: 0.2304
Training epoch 0, iteration 406/998 | lr: 6.912e-05 | global_batch_size: 16 | global_step: 406 | reduced_train_loss: 0.2036 | train_step_timing in s: 1.276 | consumed_samples: 6512 | val_loss: 0.2304
Training epoch 0, iteration 407/998 | lr: 6.896e-05 | global_batch_size: 16 | global_step: 407 | reduced_train_loss: 0.3015 | train_step_timing in s: 1.274 | consumed_samples: 6528 | val_loss: 0.2304
Training epoch 0, iteration 408/998 | lr: 6.881e-05 | global_batch_size: 16 | global_step: 408 | reduced_train_loss: 0.1446 | train_step_timing in s: 1.259 | consumed_samples: 6544 | val_loss: 0.2304
Training epoch 0, iteration 409/998 | lr: 6.866e-05 | global_batch_size: 16 | global_step: 409 | reduced_train_loss: 0.1179 | train_step_timing in s: 1.222 | consumed_samples: 6560 | val_loss: 0.2304
Training epoch 0, iteration 410/998 | lr: 6.85e-05 | global_batch_size: 16 | global_step: 410 | reduced_train_loss: 0.1722 | train_step_timing in s: 1.202 | consumed_samples: 6576 | val_loss: 0.2304
Training epoch 0, iteration 411/998 | lr: 6.835e-05 | global_batch_size: 16 | global_step: 411 | reduced_train_loss: 0.07121 | train_step_timing in s: 2.272 | consumed_samples: 6592 | val_loss: 0.2304
Training epoch 0, iteration 412/998 | lr: 6.819e-05 | global_batch_size: 16 | global_step: 412 | reduced_train_loss: 0.2334 | train_step_timing in s: 1.347 | consumed_samples: 6608 | val_loss: 0.2304
Training epoch 0, iteration 413/998 | lr: 6.804e-05 | global_batch_size: 16 | global_step: 413 | reduced_train_loss: 0.3114 | train_step_timing in s: 1.534 | consumed_samples: 6624 | val_loss: 0.2304
Training epoch 0, iteration 414/998 | lr: 6.788e-05 | global_batch_size: 16 | global_step: 414 | reduced_train_loss: 0.3428 | train_step_timing in s: 1.775 | consumed_samples: 6640 | val_loss: 0.2304
Training epoch 0, iteration 415/998 | lr: 6.773e-05 | global_batch_size: 16 | global_step: 415 | reduced_train_loss: 0.09691 | train_step_timing in s: 1.237 | consumed_samples: 6656 | val_loss: 0.2304
Training epoch 0, iteration 416/998 | lr: 6.758e-05 | global_batch_size: 16 | global_step: 416 | reduced_train_loss: 0.3869 | train_step_timing in s: 1.963 | consumed_samples: 6672 | val_loss: 0.2304
Training epoch 0, iteration 417/998 | lr: 6.742e-05 | global_batch_size: 16 | global_step: 417 | reduced_train_loss: 0.2122 | train_step_timing in s: 1.241 | consumed_samples: 6688 | val_loss: 0.2304
Training epoch 0, iteration 418/998 | lr: 6.727e-05 | global_batch_size: 16 | global_step: 418 | reduced_train_loss: 0.2752 | train_step_timing in s: 1.275 | consumed_samples: 6704 | val_loss: 0.2304
Training epoch 0, iteration 419/998 | lr: 6.711e-05 | global_batch_size: 16 | global_step: 419 | reduced_train_loss: 0.305 | train_step_timing in s: 1.566 | consumed_samples: 6720 | val_loss: 0.2304
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:28:44 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 420 : Start time: 1759796924.289s : Save duration: 0.540s
[NeMo I 2025-10-07 00:28:48 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=419-consumed_samples=6720.0-last.ckpt
Training epoch 0, iteration 420/998 | lr: 6.695e-05 | global_batch_size: 16 | global_step: 420 | reduced_train_loss: 0.1664 | train_step_timing in s: 1.792 | consumed_samples: 6736 | val_loss: 0.2239
[NeMo I 2025-10-07 00:28:49 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 421/998 | lr: 6.68e-05 | global_batch_size: 16 | global_step: 421 | reduced_train_loss: 0.153 | train_step_timing in s: 1.244 | consumed_samples: 6752 | val_loss: 0.2239
[NeMo I 2025-10-07 00:28:53 nemo_logging:393] Successfully saved checkpoint from iteration     420 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=419-consumed_samples=6720.0-last.ckpt
[NeMo I 2025-10-07 00:28:54 nemo_logging:393] Async checkpoint save for step 420 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=419-consumed_samples=6720.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:28:54 nemo_logging:393] Async finalization time took 1.514 s
Training epoch 0, iteration 422/998 | lr: 6.664e-05 | global_batch_size: 16 | global_step: 422 | reduced_train_loss: 0.16 | train_step_timing in s: 1.214 | consumed_samples: 6768 | val_loss: 0.2239
Training epoch 0, iteration 423/998 | lr: 6.649e-05 | global_batch_size: 16 | global_step: 423 | reduced_train_loss: 0.1413 | train_step_timing in s: 1.3 | consumed_samples: 6784 | val_loss: 0.2239
Training epoch 0, iteration 424/998 | lr: 6.633e-05 | global_batch_size: 16 | global_step: 424 | reduced_train_loss: 0.1791 | train_step_timing in s: 1.255 | consumed_samples: 6800 | val_loss: 0.2239
Training epoch 0, iteration 425/998 | lr: 6.617e-05 | global_batch_size: 16 | global_step: 425 | reduced_train_loss: 0.09958 | train_step_timing in s: 1.307 | consumed_samples: 6816 | val_loss: 0.2239
Training epoch 0, iteration 426/998 | lr: 6.602e-05 | global_batch_size: 16 | global_step: 426 | reduced_train_loss: 0.1771 | train_step_timing in s: 1.233 | consumed_samples: 6832 | val_loss: 0.2239
Training epoch 0, iteration 427/998 | lr: 6.586e-05 | global_batch_size: 16 | global_step: 427 | reduced_train_loss: 0.1292 | train_step_timing in s: 1.733 | consumed_samples: 6848 | val_loss: 0.2239
Training epoch 0, iteration 428/998 | lr: 6.57e-05 | global_batch_size: 16 | global_step: 428 | reduced_train_loss: 0.2927 | train_step_timing in s: 1.272 | consumed_samples: 6864 | val_loss: 0.2239
Training epoch 0, iteration 429/998 | lr: 6.555e-05 | global_batch_size: 16 | global_step: 429 | reduced_train_loss: 0.2951 | train_step_timing in s: 1.836 | consumed_samples: 6880 | val_loss: 0.2239
Training epoch 0, iteration 430/998 | lr: 6.539e-05 | global_batch_size: 16 | global_step: 430 | reduced_train_loss: 0.1811 | train_step_timing in s: 1.331 | consumed_samples: 6896 | val_loss: 0.2239
Training epoch 0, iteration 431/998 | lr: 6.523e-05 | global_batch_size: 16 | global_step: 431 | reduced_train_loss: 0.181 | train_step_timing in s: 3.329 | consumed_samples: 6912 | val_loss: 0.2239
Training epoch 0, iteration 432/998 | lr: 6.507e-05 | global_batch_size: 16 | global_step: 432 | reduced_train_loss: 0.1426 | train_step_timing in s: 1.383 | consumed_samples: 6928 | val_loss: 0.2239
Training epoch 0, iteration 433/998 | lr: 6.491e-05 | global_batch_size: 16 | global_step: 433 | reduced_train_loss: 0.2656 | train_step_timing in s: 1.282 | consumed_samples: 6944 | val_loss: 0.2239
Training epoch 0, iteration 434/998 | lr: 6.476e-05 | global_batch_size: 16 | global_step: 434 | reduced_train_loss: 0.07561 | train_step_timing in s: 1.532 | consumed_samples: 6960 | val_loss: 0.2239
Training epoch 0, iteration 435/998 | lr: 6.46e-05 | global_batch_size: 16 | global_step: 435 | reduced_train_loss: 0.1905 | train_step_timing in s: 1.385 | consumed_samples: 6976 | val_loss: 0.2239
Training epoch 0, iteration 436/998 | lr: 6.444e-05 | global_batch_size: 16 | global_step: 436 | reduced_train_loss: 0.2037 | train_step_timing in s: 1.272 | consumed_samples: 6992 | val_loss: 0.2239
Training epoch 0, iteration 437/998 | lr: 6.428e-05 | global_batch_size: 16 | global_step: 437 | reduced_train_loss: 0.5992 | train_step_timing in s: 1.269 | consumed_samples: 7008 | val_loss: 0.2239
Training epoch 0, iteration 438/998 | lr: 6.412e-05 | global_batch_size: 16 | global_step: 438 | reduced_train_loss: 0.1707 | train_step_timing in s: 1.38 | consumed_samples: 7024 | val_loss: 0.2239
Training epoch 0, iteration 439/998 | lr: 6.396e-05 | global_batch_size: 16 | global_step: 439 | reduced_train_loss: 0.3904 | train_step_timing in s: 1.307 | consumed_samples: 7040 | val_loss: 0.2239
Training epoch 0, iteration 440/998 | lr: 6.38e-05 | global_batch_size: 16 | global_step: 440 | reduced_train_loss: 0.1964 | train_step_timing in s: 1.527 | consumed_samples: 7056 | val_loss: 0.2239
Training epoch 0, iteration 441/998 | lr: 6.365e-05 | global_batch_size: 16 | global_step: 441 | reduced_train_loss: 0.3057 | train_step_timing in s: 1.328 | consumed_samples: 7072 | val_loss: 0.2239
Training epoch 0, iteration 442/998 | lr: 6.349e-05 | global_batch_size: 16 | global_step: 442 | reduced_train_loss: 0.1736 | train_step_timing in s: 1.697 | consumed_samples: 7088 | val_loss: 0.2239
Training epoch 0, iteration 443/998 | lr: 6.333e-05 | global_batch_size: 16 | global_step: 443 | reduced_train_loss: 0.2217 | train_step_timing in s: 1.801 | consumed_samples: 7104 | val_loss: 0.2239
Training epoch 0, iteration 444/998 | lr: 6.317e-05 | global_batch_size: 16 | global_step: 444 | reduced_train_loss: 0.2984 | train_step_timing in s: 1.152 | consumed_samples: 7120 | val_loss: 0.2239
Training epoch 0, iteration 445/998 | lr: 6.301e-05 | global_batch_size: 16 | global_step: 445 | reduced_train_loss: 0.206 | train_step_timing in s: 1.371 | consumed_samples: 7136 | val_loss: 0.2239
Training epoch 0, iteration 446/998 | lr: 6.285e-05 | global_batch_size: 16 | global_step: 446 | reduced_train_loss: 0.2246 | train_step_timing in s: 1.219 | consumed_samples: 7152 | val_loss: 0.2239
Training epoch 0, iteration 447/998 | lr: 6.269e-05 | global_batch_size: 16 | global_step: 447 | reduced_train_loss: 0.2712 | train_step_timing in s: 1.271 | consumed_samples: 7168 | val_loss: 0.2239
Training epoch 0, iteration 448/998 | lr: 6.253e-05 | global_batch_size: 16 | global_step: 448 | reduced_train_loss: 0.1423 | train_step_timing in s: 1.772 | consumed_samples: 7184 | val_loss: 0.2239
Training epoch 0, iteration 449/998 | lr: 6.237e-05 | global_batch_size: 16 | global_step: 449 | reduced_train_loss: 0.08171 | train_step_timing in s: 1.217 | consumed_samples: 7200 | val_loss: 0.2239
Epoch 0, global step 449: 'val_loss' reached 0.22391 (best 0.22391), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=449-consumed_samples=7200.0.ckpt' as top 2
[NeMo I 2025-10-07 00:30:57 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 449 : Start time: 1759797057.254s : Save duration: 0.555s
[NeMo I 2025-10-07 00:31:01 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=449-consumed_samples=7200.0.ckpt
[NeMo I 2025-10-07 00:31:01 nemo_logging:393] Async finalization time took 0.000 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 450/998 | lr: 6.221e-05 | global_batch_size: 16 | global_step: 450 | reduced_train_loss: 0.1156 | train_step_timing in s: 1.302 | consumed_samples: 7216 | val_loss: 0.2159
[NeMo I 2025-10-07 00:35:21 nemo_logging:393] Successfully saved checkpoint from iteration     449 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=449-consumed_samples=7200.0.ckpt
[NeMo I 2025-10-07 00:35:21 nemo_logging:393] Async checkpoint save for step 450 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=449-consumed_samples=7200.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:35:29 nemo_logging:393] Async finalization time took 9.209 s
Training epoch 0, iteration 451/998 | lr: 6.205e-05 | global_batch_size: 16 | global_step: 451 | reduced_train_loss: 0.3491 | train_step_timing in s: 1.679 | consumed_samples: 7232 | val_loss: 0.2159
Training epoch 0, iteration 452/998 | lr: 6.189e-05 | global_batch_size: 16 | global_step: 452 | reduced_train_loss: 0.1719 | train_step_timing in s: 1.198 | consumed_samples: 7248 | val_loss: 0.2159
Training epoch 0, iteration 453/998 | lr: 6.172e-05 | global_batch_size: 16 | global_step: 453 | reduced_train_loss: 0.2241 | train_step_timing in s: 1.557 | consumed_samples: 7264 | val_loss: 0.2159
Training epoch 0, iteration 454/998 | lr: 6.156e-05 | global_batch_size: 16 | global_step: 454 | reduced_train_loss: 0.1805 | train_step_timing in s: 1.225 | consumed_samples: 7280 | val_loss: 0.2159
Training epoch 0, iteration 455/998 | lr: 6.14e-05 | global_batch_size: 16 | global_step: 455 | reduced_train_loss: 0.05879 | train_step_timing in s: 1.155 | consumed_samples: 7296 | val_loss: 0.2159
Training epoch 0, iteration 456/998 | lr: 6.124e-05 | global_batch_size: 16 | global_step: 456 | reduced_train_loss: 0.1878 | train_step_timing in s: 1.329 | consumed_samples: 7312 | val_loss: 0.2159
Training epoch 0, iteration 457/998 | lr: 6.108e-05 | global_batch_size: 16 | global_step: 457 | reduced_train_loss: 0.1135 | train_step_timing in s: 1.211 | consumed_samples: 7328 | val_loss: 0.2159
Training epoch 0, iteration 458/998 | lr: 6.092e-05 | global_batch_size: 16 | global_step: 458 | reduced_train_loss: 0.0873 | train_step_timing in s: 1.152 | consumed_samples: 7344 | val_loss: 0.2159
Training epoch 0, iteration 459/998 | lr: 6.076e-05 | global_batch_size: 16 | global_step: 459 | reduced_train_loss: 0.1825 | train_step_timing in s: 1.529 | consumed_samples: 7360 | val_loss: 0.2159
Training epoch 0, iteration 460/998 | lr: 6.06e-05 | global_batch_size: 16 | global_step: 460 | reduced_train_loss: 0.134 | train_step_timing in s: 1.225 | consumed_samples: 7376 | val_loss: 0.2159
Training epoch 0, iteration 461/998 | lr: 6.043e-05 | global_batch_size: 16 | global_step: 461 | reduced_train_loss: 0.2411 | train_step_timing in s: 1.241 | consumed_samples: 7392 | val_loss: 0.2159
Training epoch 0, iteration 462/998 | lr: 6.027e-05 | global_batch_size: 16 | global_step: 462 | reduced_train_loss: 0.2765 | train_step_timing in s: 1.6 | consumed_samples: 7408 | val_loss: 0.2159
Training epoch 0, iteration 463/998 | lr: 6.011e-05 | global_batch_size: 16 | global_step: 463 | reduced_train_loss: 0.1533 | train_step_timing in s: 1.528 | consumed_samples: 7424 | val_loss: 0.2159
Training epoch 0, iteration 464/998 | lr: 5.995e-05 | global_batch_size: 16 | global_step: 464 | reduced_train_loss: 0.1865 | train_step_timing in s: 2.656 | consumed_samples: 7440 | val_loss: 0.2159
Training epoch 0, iteration 465/998 | lr: 5.978e-05 | global_batch_size: 16 | global_step: 465 | reduced_train_loss: 0.4512 | train_step_timing in s: 1.149 | consumed_samples: 7456 | val_loss: 0.2159
Training epoch 0, iteration 466/998 | lr: 5.962e-05 | global_batch_size: 16 | global_step: 466 | reduced_train_loss: 0.2311 | train_step_timing in s: 1.358 | consumed_samples: 7472 | val_loss: 0.2159
Training epoch 0, iteration 467/998 | lr: 5.946e-05 | global_batch_size: 16 | global_step: 467 | reduced_train_loss: 0.1713 | train_step_timing in s: 1.3 | consumed_samples: 7488 | val_loss: 0.2159
Training epoch 0, iteration 468/998 | lr: 5.93e-05 | global_batch_size: 16 | global_step: 468 | reduced_train_loss: 0.1458 | train_step_timing in s: 1.304 | consumed_samples: 7504 | val_loss: 0.2159
Training epoch 0, iteration 469/998 | lr: 5.913e-05 | global_batch_size: 16 | global_step: 469 | reduced_train_loss: 0.07141 | train_step_timing in s: 1.207 | consumed_samples: 7520 | val_loss: 0.2159
Training epoch 0, iteration 470/998 | lr: 5.897e-05 | global_batch_size: 16 | global_step: 470 | reduced_train_loss: 0.1655 | train_step_timing in s: 1.216 | consumed_samples: 7536 | val_loss: 0.2159
Training epoch 0, iteration 471/998 | lr: 5.881e-05 | global_batch_size: 16 | global_step: 471 | reduced_train_loss: 0.1243 | train_step_timing in s: 1.24 | consumed_samples: 7552 | val_loss: 0.2159
Training epoch 0, iteration 472/998 | lr: 5.865e-05 | global_batch_size: 16 | global_step: 472 | reduced_train_loss: 0.134 | train_step_timing in s: 1.162 | consumed_samples: 7568 | val_loss: 0.2159
Training epoch 0, iteration 473/998 | lr: 5.848e-05 | global_batch_size: 16 | global_step: 473 | reduced_train_loss: 0.4309 | train_step_timing in s: 1.77 | consumed_samples: 7584 | val_loss: 0.2159
Training epoch 0, iteration 474/998 | lr: 5.832e-05 | global_batch_size: 16 | global_step: 474 | reduced_train_loss: 0.03638 | train_step_timing in s: 2.563 | consumed_samples: 7600 | val_loss: 0.2159
Training epoch 0, iteration 475/998 | lr: 5.816e-05 | global_batch_size: 16 | global_step: 475 | reduced_train_loss: 0.2045 | train_step_timing in s: 1.524 | consumed_samples: 7616 | val_loss: 0.2159
Training epoch 0, iteration 476/998 | lr: 5.799e-05 | global_batch_size: 16 | global_step: 476 | reduced_train_loss: 0.2529 | train_step_timing in s: 1.222 | consumed_samples: 7632 | val_loss: 0.2159
Training epoch 0, iteration 477/998 | lr: 5.783e-05 | global_batch_size: 16 | global_step: 477 | reduced_train_loss: 0.09153 | train_step_timing in s: 1.217 | consumed_samples: 7648 | val_loss: 0.2159
Training epoch 0, iteration 478/998 | lr: 5.767e-05 | global_batch_size: 16 | global_step: 478 | reduced_train_loss: 0.3243 | train_step_timing in s: 1.709 | consumed_samples: 7664 | val_loss: 0.2159
Training epoch 0, iteration 479/998 | lr: 5.75e-05 | global_batch_size: 16 | global_step: 479 | reduced_train_loss: 0.2187 | train_step_timing in s: 1.23 | consumed_samples: 7680 | val_loss: 0.2159
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:42:02 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 480 : Start time: 1759797722.075s : Save duration: 0.587s
[NeMo I 2025-10-07 00:42:06 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=479-consumed_samples=7680.0-last.ckpt
Training epoch 0, iteration 480/998 | lr: 5.734e-05 | global_batch_size: 16 | global_step: 480 | reduced_train_loss: 0.1459 | train_step_timing in s: 1.356 | consumed_samples: 7696 | val_loss: 0.2218
[NeMo I 2025-10-07 00:42:07 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 481/998 | lr: 5.718e-05 | global_batch_size: 16 | global_step: 481 | reduced_train_loss: 0.2673 | train_step_timing in s: 1.214 | consumed_samples: 7712 | val_loss: 0.2218
[NeMo I 2025-10-07 00:42:11 nemo_logging:393] Successfully saved checkpoint from iteration     480 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=479-consumed_samples=7680.0-last.ckpt
[NeMo I 2025-10-07 00:42:12 nemo_logging:393] Async checkpoint save for step 480 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=479-consumed_samples=7680.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:42:12 nemo_logging:393] Async finalization time took 1.982 s
Training epoch 0, iteration 482/998 | lr: 5.701e-05 | global_batch_size: 16 | global_step: 482 | reduced_train_loss: 0.1813 | train_step_timing in s: 1.279 | consumed_samples: 7728 | val_loss: 0.2218
Training epoch 0, iteration 483/998 | lr: 5.685e-05 | global_batch_size: 16 | global_step: 483 | reduced_train_loss: 0.204 | train_step_timing in s: 1.216 | consumed_samples: 7744 | val_loss: 0.2218
Training epoch 0, iteration 484/998 | lr: 5.668e-05 | global_batch_size: 16 | global_step: 484 | reduced_train_loss: 0.2078 | train_step_timing in s: 1.232 | consumed_samples: 7760 | val_loss: 0.2218
Training epoch 0, iteration 485/998 | lr: 5.652e-05 | global_batch_size: 16 | global_step: 485 | reduced_train_loss: 0.2556 | train_step_timing in s: 1.286 | consumed_samples: 7776 | val_loss: 0.2218
Training epoch 0, iteration 486/998 | lr: 5.636e-05 | global_batch_size: 16 | global_step: 486 | reduced_train_loss: 0.3195 | train_step_timing in s: 2.148 | consumed_samples: 7792 | val_loss: 0.2218
Training epoch 0, iteration 487/998 | lr: 5.619e-05 | global_batch_size: 16 | global_step: 487 | reduced_train_loss: 0.2237 | train_step_timing in s: 1.321 | consumed_samples: 7808 | val_loss: 0.2218
Training epoch 0, iteration 488/998 | lr: 5.603e-05 | global_batch_size: 16 | global_step: 488 | reduced_train_loss: 0.3592 | train_step_timing in s: 1.525 | consumed_samples: 7824 | val_loss: 0.2218
Training epoch 0, iteration 489/998 | lr: 5.586e-05 | global_batch_size: 16 | global_step: 489 | reduced_train_loss: 0.3626 | train_step_timing in s: 1.206 | consumed_samples: 7840 | val_loss: 0.2218
Training epoch 0, iteration 490/998 | lr: 5.57e-05 | global_batch_size: 16 | global_step: 490 | reduced_train_loss: 0.1603 | train_step_timing in s: 1.333 | consumed_samples: 7856 | val_loss: 0.2218
Training epoch 0, iteration 491/998 | lr: 5.553e-05 | global_batch_size: 16 | global_step: 491 | reduced_train_loss: 0.3123 | train_step_timing in s: 1.253 | consumed_samples: 7872 | val_loss: 0.2218
Training epoch 0, iteration 492/998 | lr: 5.537e-05 | global_batch_size: 16 | global_step: 492 | reduced_train_loss: 0.1196 | train_step_timing in s: 1.251 | consumed_samples: 7888 | val_loss: 0.2218
Training epoch 0, iteration 493/998 | lr: 5.52e-05 | global_batch_size: 16 | global_step: 493 | reduced_train_loss: 0.2066 | train_step_timing in s: 1.604 | consumed_samples: 7904 | val_loss: 0.2218
Training epoch 0, iteration 494/998 | lr: 5.504e-05 | global_batch_size: 16 | global_step: 494 | reduced_train_loss: 0.1958 | train_step_timing in s: 1.26 | consumed_samples: 7920 | val_loss: 0.2218
Training epoch 0, iteration 495/998 | lr: 5.488e-05 | global_batch_size: 16 | global_step: 495 | reduced_train_loss: 0.1313 | train_step_timing in s: 1.332 | consumed_samples: 7936 | val_loss: 0.2218
Training epoch 0, iteration 496/998 | lr: 5.471e-05 | global_batch_size: 16 | global_step: 496 | reduced_train_loss: 0.2162 | train_step_timing in s: 1.258 | consumed_samples: 7952 | val_loss: 0.2218
Training epoch 0, iteration 497/998 | lr: 5.455e-05 | global_batch_size: 16 | global_step: 497 | reduced_train_loss: 0.1698 | train_step_timing in s: 2.404 | consumed_samples: 7968 | val_loss: 0.2218
Training epoch 0, iteration 498/998 | lr: 5.438e-05 | global_batch_size: 16 | global_step: 498 | reduced_train_loss: 0.2927 | train_step_timing in s: 1.335 | consumed_samples: 7984 | val_loss: 0.2218
Training epoch 0, iteration 499/998 | lr: 5.422e-05 | global_batch_size: 16 | global_step: 499 | reduced_train_loss: 0.2339 | train_step_timing in s: 1.268 | consumed_samples: 8000 | val_loss: 0.2218
Epoch 0, global step 499: 'val_loss' reached 0.22177 (best 0.22177), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=499-consumed_samples=8000.0.ckpt' as top 2
[NeMo I 2025-10-07 00:43:34 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 499 : Start time: 1759797814.044s : Save duration: 0.571s
[NeMo I 2025-10-07 00:43:38 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=499-consumed_samples=8000.0.ckpt
[NeMo I 2025-10-07 00:43:38 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 500/998 | lr: 5.405e-05 | global_batch_size: 16 | global_step: 500 | reduced_train_loss: 0.3902 | train_step_timing in s: 1.3 | consumed_samples: 8016 | val_loss: 0.2218
[NeMo I 2025-10-07 00:43:43 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 501/998 | lr: 5.389e-05 | global_batch_size: 16 | global_step: 501 | reduced_train_loss: 0.2728 | train_step_timing in s: 1.533 | consumed_samples: 8032 | val_loss: 0.2218
[NeMo I 2025-10-07 00:43:48 nemo_logging:393] Successfully saved checkpoint from iteration     499 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=499-consumed_samples=8000.0.ckpt
[NeMo I 2025-10-07 00:43:48 nemo_logging:393] Async checkpoint save for step 500 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=499-consumed_samples=8000.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:43:56 nemo_logging:393] Async finalization time took 9.847 s
Training epoch 0, iteration 502/998 | lr: 5.372e-05 | global_batch_size: 16 | global_step: 502 | reduced_train_loss: 0.2045 | train_step_timing in s: 1.339 | consumed_samples: 8048 | val_loss: 0.2218
Training epoch 0, iteration 503/998 | lr: 5.356e-05 | global_batch_size: 16 | global_step: 503 | reduced_train_loss: 0.5883 | train_step_timing in s: 1.281 | consumed_samples: 8064 | val_loss: 0.2218
Training epoch 0, iteration 504/998 | lr: 5.339e-05 | global_batch_size: 16 | global_step: 504 | reduced_train_loss: 0.2476 | train_step_timing in s: 1.279 | consumed_samples: 8080 | val_loss: 0.2218
Training epoch 0, iteration 505/998 | lr: 5.323e-05 | global_batch_size: 16 | global_step: 505 | reduced_train_loss: 0.165 | train_step_timing in s: 1.548 | consumed_samples: 8096 | val_loss: 0.2218
Training epoch 0, iteration 506/998 | lr: 5.306e-05 | global_batch_size: 16 | global_step: 506 | reduced_train_loss: 0.252 | train_step_timing in s: 1.208 | consumed_samples: 8112 | val_loss: 0.2218
Training epoch 0, iteration 507/998 | lr: 5.29e-05 | global_batch_size: 16 | global_step: 507 | reduced_train_loss: 0.273 | train_step_timing in s: 1.334 | consumed_samples: 8128 | val_loss: 0.2218
Training epoch 0, iteration 508/998 | lr: 5.273e-05 | global_batch_size: 16 | global_step: 508 | reduced_train_loss: 0.1639 | train_step_timing in s: 1.275 | consumed_samples: 8144 | val_loss: 0.2218
Training epoch 0, iteration 509/998 | lr: 5.256e-05 | global_batch_size: 16 | global_step: 509 | reduced_train_loss: 0.1985 | train_step_timing in s: 1.279 | consumed_samples: 8160 | val_loss: 0.2218
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:48:51 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 510 : Start time: 1759798130.611s : Save duration: 0.571s
[NeMo I 2025-10-07 00:48:55 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=509-consumed_samples=8160.0-last.ckpt
Training epoch 0, iteration 510/998 | lr: 5.24e-05 | global_batch_size: 16 | global_step: 510 | reduced_train_loss: 0.1996 | train_step_timing in s: 1.712 | consumed_samples: 8176 | val_loss: 0.212
[NeMo I 2025-10-07 00:48:56 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 511/998 | lr: 5.223e-05 | global_batch_size: 16 | global_step: 511 | reduced_train_loss: 0.2123 | train_step_timing in s: 1.226 | consumed_samples: 8192 | val_loss: 0.212
[NeMo I 2025-10-07 00:49:00 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 512/998 | lr: 5.207e-05 | global_batch_size: 16 | global_step: 512 | reduced_train_loss: 0.2091 | train_step_timing in s: 1.201 | consumed_samples: 8208 | val_loss: 0.212
[NeMo I 2025-10-07 00:49:04 nemo_logging:393] Successfully saved checkpoint from iteration     510 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=509-consumed_samples=8160.0-last.ckpt
[NeMo I 2025-10-07 00:49:04 nemo_logging:393] Async checkpoint save for step 510 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=509-consumed_samples=8160.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:49:04 nemo_logging:393] Async finalization time took 1.443 s
Training epoch 0, iteration 513/998 | lr: 5.19e-05 | global_batch_size: 16 | global_step: 513 | reduced_train_loss: 0.1927 | train_step_timing in s: 1.158 | consumed_samples: 8224 | val_loss: 0.212
Training epoch 0, iteration 514/998 | lr: 5.174e-05 | global_batch_size: 16 | global_step: 514 | reduced_train_loss: 0.2398 | train_step_timing in s: 1.249 | consumed_samples: 8240 | val_loss: 0.212
Training epoch 0, iteration 515/998 | lr: 5.157e-05 | global_batch_size: 16 | global_step: 515 | reduced_train_loss: 0.3164 | train_step_timing in s: 1.248 | consumed_samples: 8256 | val_loss: 0.212
Training epoch 0, iteration 516/998 | lr: 5.141e-05 | global_batch_size: 16 | global_step: 516 | reduced_train_loss: 0.1066 | train_step_timing in s: 1.241 | consumed_samples: 8272 | val_loss: 0.212
Training epoch 0, iteration 517/998 | lr: 5.124e-05 | global_batch_size: 16 | global_step: 517 | reduced_train_loss: 0.09001 | train_step_timing in s: 1.328 | consumed_samples: 8288 | val_loss: 0.212
Training epoch 0, iteration 518/998 | lr: 5.108e-05 | global_batch_size: 16 | global_step: 518 | reduced_train_loss: 0.125 | train_step_timing in s: 1.228 | consumed_samples: 8304 | val_loss: 0.212
Training epoch 0, iteration 519/998 | lr: 5.091e-05 | global_batch_size: 16 | global_step: 519 | reduced_train_loss: 0.2515 | train_step_timing in s: 1.736 | consumed_samples: 8320 | val_loss: 0.212
Training epoch 0, iteration 520/998 | lr: 5.074e-05 | global_batch_size: 16 | global_step: 520 | reduced_train_loss: 0.212 | train_step_timing in s: 1.334 | consumed_samples: 8336 | val_loss: 0.212
Training epoch 0, iteration 521/998 | lr: 5.058e-05 | global_batch_size: 16 | global_step: 521 | reduced_train_loss: 0.07983 | train_step_timing in s: 1.655 | consumed_samples: 8352 | val_loss: 0.212
Training epoch 0, iteration 522/998 | lr: 5.041e-05 | global_batch_size: 16 | global_step: 522 | reduced_train_loss: 0.2557 | train_step_timing in s: 1.273 | consumed_samples: 8368 | val_loss: 0.212
Training epoch 0, iteration 523/998 | lr: 5.025e-05 | global_batch_size: 16 | global_step: 523 | reduced_train_loss: 0.1509 | train_step_timing in s: 1.372 | consumed_samples: 8384 | val_loss: 0.212
Training epoch 0, iteration 524/998 | lr: 5.008e-05 | global_batch_size: 16 | global_step: 524 | reduced_train_loss: 0.1796 | train_step_timing in s: 1.241 | consumed_samples: 8400 | val_loss: 0.212
Training epoch 0, iteration 525/998 | lr: 4.992e-05 | global_batch_size: 16 | global_step: 525 | reduced_train_loss: 0.3168 | train_step_timing in s: 1.22 | consumed_samples: 8416 | val_loss: 0.212
Training epoch 0, iteration 526/998 | lr: 4.975e-05 | global_batch_size: 16 | global_step: 526 | reduced_train_loss: 0.2471 | train_step_timing in s: 1.268 | consumed_samples: 8432 | val_loss: 0.212
Training epoch 0, iteration 527/998 | lr: 4.959e-05 | global_batch_size: 16 | global_step: 527 | reduced_train_loss: 0.2452 | train_step_timing in s: 1.275 | consumed_samples: 8448 | val_loss: 0.212
Training epoch 0, iteration 528/998 | lr: 4.942e-05 | global_batch_size: 16 | global_step: 528 | reduced_train_loss: 0.2699 | train_step_timing in s: 1.343 | consumed_samples: 8464 | val_loss: 0.212
Training epoch 0, iteration 529/998 | lr: 4.926e-05 | global_batch_size: 16 | global_step: 529 | reduced_train_loss: 0.1838 | train_step_timing in s: 1.253 | consumed_samples: 8480 | val_loss: 0.212
Training epoch 0, iteration 530/998 | lr: 4.909e-05 | global_batch_size: 16 | global_step: 530 | reduced_train_loss: 0.2005 | train_step_timing in s: 1.35 | consumed_samples: 8496 | val_loss: 0.212
Training epoch 0, iteration 531/998 | lr: 4.892e-05 | global_batch_size: 16 | global_step: 531 | reduced_train_loss: 0.1649 | train_step_timing in s: 1.209 | consumed_samples: 8512 | val_loss: 0.212
Training epoch 0, iteration 532/998 | lr: 4.876e-05 | global_batch_size: 16 | global_step: 532 | reduced_train_loss: 0.2535 | train_step_timing in s: 1.318 | consumed_samples: 8528 | val_loss: 0.212
Training epoch 0, iteration 533/998 | lr: 4.859e-05 | global_batch_size: 16 | global_step: 533 | reduced_train_loss: 0.0944 | train_step_timing in s: 1.669 | consumed_samples: 8544 | val_loss: 0.212
Training epoch 0, iteration 534/998 | lr: 4.843e-05 | global_batch_size: 16 | global_step: 534 | reduced_train_loss: 0.2207 | train_step_timing in s: 1.311 | consumed_samples: 8560 | val_loss: 0.212
Training epoch 0, iteration 535/998 | lr: 4.826e-05 | global_batch_size: 16 | global_step: 535 | reduced_train_loss: 0.2372 | train_step_timing in s: 1.586 | consumed_samples: 8576 | val_loss: 0.212
Training epoch 0, iteration 536/998 | lr: 4.81e-05 | global_batch_size: 16 | global_step: 536 | reduced_train_loss: 0.2903 | train_step_timing in s: 1.323 | consumed_samples: 8592 | val_loss: 0.212
Training epoch 0, iteration 537/998 | lr: 4.793e-05 | global_batch_size: 16 | global_step: 537 | reduced_train_loss: 0.2549 | train_step_timing in s: 1.257 | consumed_samples: 8608 | val_loss: 0.212
Training epoch 0, iteration 538/998 | lr: 4.777e-05 | global_batch_size: 16 | global_step: 538 | reduced_train_loss: 0.1146 | train_step_timing in s: 1.259 | consumed_samples: 8624 | val_loss: 0.212
Training epoch 0, iteration 539/998 | lr: 4.76e-05 | global_batch_size: 16 | global_step: 539 | reduced_train_loss: 0.08571 | train_step_timing in s: 1.309 | consumed_samples: 8640 | val_loss: 0.212
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 00:55:37 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 540 : Start time: 1759798536.627s : Save duration: 0.592s
[NeMo I 2025-10-07 00:55:41 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=539-consumed_samples=8640.0-last.ckpt
Training epoch 0, iteration 540/998 | lr: 4.744e-05 | global_batch_size: 16 | global_step: 540 | reduced_train_loss: 0.2084 | train_step_timing in s: 1.254 | consumed_samples: 8656 | val_loss: 0.22
[NeMo I 2025-10-07 00:55:43 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 541/998 | lr: 4.727e-05 | global_batch_size: 16 | global_step: 541 | reduced_train_loss: 0.2029 | train_step_timing in s: 1.826 | consumed_samples: 8672 | val_loss: 0.22
[NeMo I 2025-10-07 00:55:47 nemo_logging:393] Successfully saved checkpoint from iteration     540 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=539-consumed_samples=8640.0-last.ckpt
[NeMo I 2025-10-07 00:55:47 nemo_logging:393] Async checkpoint save for step 540 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=539-consumed_samples=8640.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:55:51 nemo_logging:393] Async finalization time took 4.225 s
Training epoch 0, iteration 542/998 | lr: 4.71e-05 | global_batch_size: 16 | global_step: 542 | reduced_train_loss: 0.2481 | train_step_timing in s: 1.383 | consumed_samples: 8688 | val_loss: 0.22
Training epoch 0, iteration 543/998 | lr: 4.694e-05 | global_batch_size: 16 | global_step: 543 | reduced_train_loss: 0.3071 | train_step_timing in s: 1.266 | consumed_samples: 8704 | val_loss: 0.22
Training epoch 0, iteration 544/998 | lr: 4.677e-05 | global_batch_size: 16 | global_step: 544 | reduced_train_loss: 0.6076 | train_step_timing in s: 1.329 | consumed_samples: 8720 | val_loss: 0.22
Training epoch 0, iteration 545/998 | lr: 4.661e-05 | global_batch_size: 16 | global_step: 545 | reduced_train_loss: 0.1562 | train_step_timing in s: 1.263 | consumed_samples: 8736 | val_loss: 0.22
Training epoch 0, iteration 546/998 | lr: 4.644e-05 | global_batch_size: 16 | global_step: 546 | reduced_train_loss: 0.2316 | train_step_timing in s: 1.527 | consumed_samples: 8752 | val_loss: 0.22
Training epoch 0, iteration 547/998 | lr: 4.628e-05 | global_batch_size: 16 | global_step: 547 | reduced_train_loss: 0.1167 | train_step_timing in s: 1.796 | consumed_samples: 8768 | val_loss: 0.22
Training epoch 0, iteration 548/998 | lr: 4.611e-05 | global_batch_size: 16 | global_step: 548 | reduced_train_loss: 0.1042 | train_step_timing in s: 1.242 | consumed_samples: 8784 | val_loss: 0.22
Training epoch 0, iteration 549/998 | lr: 4.595e-05 | global_batch_size: 16 | global_step: 549 | reduced_train_loss: 0.2448 | train_step_timing in s: 1.303 | consumed_samples: 8800 | val_loss: 0.22
Epoch 0, global step 549: 'val_loss' reached 0.21996 (best 0.21996), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=549-consumed_samples=8800.0.ckpt' as top 2
[NeMo I 2025-10-07 00:56:27 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 549 : Start time: 1759798586.613s : Save duration: 1.029s
[NeMo I 2025-10-07 00:56:30 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=549-consumed_samples=8800.0.ckpt
[NeMo I 2025-10-07 00:56:30 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 550/998 | lr: 4.578e-05 | global_batch_size: 16 | global_step: 550 | reduced_train_loss: 0.2755 | train_step_timing in s: 1.271 | consumed_samples: 8816 | val_loss: 0.22
[NeMo I 2025-10-07 00:56:33 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 551/998 | lr: 4.562e-05 | global_batch_size: 16 | global_step: 551 | reduced_train_loss: 0.2618 | train_step_timing in s: 1.169 | consumed_samples: 8832 | val_loss: 0.22
[NeMo I 2025-10-07 00:56:39 nemo_logging:393] Successfully saved checkpoint from iteration     549 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=549-consumed_samples=8800.0.ckpt
[NeMo I 2025-10-07 00:56:40 nemo_logging:393] Async checkpoint save for step 550 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=549-consumed_samples=8800.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 00:56:46 nemo_logging:393] Async finalization time took 7.774 s
Training epoch 0, iteration 552/998 | lr: 4.545e-05 | global_batch_size: 16 | global_step: 552 | reduced_train_loss: 0.2873 | train_step_timing in s: 1.294 | consumed_samples: 8848 | val_loss: 0.22
Training epoch 0, iteration 553/998 | lr: 4.529e-05 | global_batch_size: 16 | global_step: 553 | reduced_train_loss: 0.2009 | train_step_timing in s: 1.748 | consumed_samples: 8864 | val_loss: 0.22
Training epoch 0, iteration 554/998 | lr: 4.512e-05 | global_batch_size: 16 | global_step: 554 | reduced_train_loss: 0.2214 | train_step_timing in s: 1.375 | consumed_samples: 8880 | val_loss: 0.22
Training epoch 0, iteration 555/998 | lr: 4.496e-05 | global_batch_size: 16 | global_step: 555 | reduced_train_loss: 0.2686 | train_step_timing in s: 1.37 | consumed_samples: 8896 | val_loss: 0.22
Training epoch 0, iteration 556/998 | lr: 4.48e-05 | global_batch_size: 16 | global_step: 556 | reduced_train_loss: 0.1943 | train_step_timing in s: 1.368 | consumed_samples: 8912 | val_loss: 0.22
Training epoch 0, iteration 557/998 | lr: 4.463e-05 | global_batch_size: 16 | global_step: 557 | reduced_train_loss: 0.2517 | train_step_timing in s: 1.35 | consumed_samples: 8928 | val_loss: 0.22
Training epoch 0, iteration 558/998 | lr: 4.447e-05 | global_batch_size: 16 | global_step: 558 | reduced_train_loss: 0.1423 | train_step_timing in s: 1.774 | consumed_samples: 8944 | val_loss: 0.22
Training epoch 0, iteration 559/998 | lr: 4.43e-05 | global_batch_size: 16 | global_step: 559 | reduced_train_loss: 0.1458 | train_step_timing in s: 1.215 | consumed_samples: 8960 | val_loss: 0.22
Training epoch 0, iteration 560/998 | lr: 4.414e-05 | global_batch_size: 16 | global_step: 560 | reduced_train_loss: 0.3782 | train_step_timing in s: 2.158 | consumed_samples: 8976 | val_loss: 0.22
Training epoch 0, iteration 561/998 | lr: 4.397e-05 | global_batch_size: 16 | global_step: 561 | reduced_train_loss: 0.3957 | train_step_timing in s: 1.292 | consumed_samples: 8992 | val_loss: 0.22
Training epoch 0, iteration 562/998 | lr: 4.381e-05 | global_batch_size: 16 | global_step: 562 | reduced_train_loss: 0.3245 | train_step_timing in s: 1.279 | consumed_samples: 9008 | val_loss: 0.22
Training epoch 0, iteration 563/998 | lr: 4.364e-05 | global_batch_size: 16 | global_step: 563 | reduced_train_loss: 0.2971 | train_step_timing in s: 1.964 | consumed_samples: 9024 | val_loss: 0.22
Training epoch 0, iteration 564/998 | lr: 4.348e-05 | global_batch_size: 16 | global_step: 564 | reduced_train_loss: 0.1746 | train_step_timing in s: 1.268 | consumed_samples: 9040 | val_loss: 0.22
Training epoch 0, iteration 565/998 | lr: 4.332e-05 | global_batch_size: 16 | global_step: 565 | reduced_train_loss: 0.3269 | train_step_timing in s: 1.168 | consumed_samples: 9056 | val_loss: 0.22
Training epoch 0, iteration 566/998 | lr: 4.315e-05 | global_batch_size: 16 | global_step: 566 | reduced_train_loss: 0.2685 | train_step_timing in s: 1.361 | consumed_samples: 9072 | val_loss: 0.22
Training epoch 0, iteration 567/998 | lr: 4.299e-05 | global_batch_size: 16 | global_step: 567 | reduced_train_loss: 0.1026 | train_step_timing in s: 1.214 | consumed_samples: 9088 | val_loss: 0.22
Training epoch 0, iteration 568/998 | lr: 4.282e-05 | global_batch_size: 16 | global_step: 568 | reduced_train_loss: 0.3142 | train_step_timing in s: 1.248 | consumed_samples: 9104 | val_loss: 0.22
Training epoch 0, iteration 569/998 | lr: 4.266e-05 | global_batch_size: 16 | global_step: 569 | reduced_train_loss: 0.1096 | train_step_timing in s: 1.307 | consumed_samples: 9120 | val_loss: 0.22
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:02:23 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 570 : Start time: 1759798942.220s : Save duration: 1.045s
[NeMo I 2025-10-07 01:02:26 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=569-consumed_samples=9120.0-last.ckpt
Training epoch 0, iteration 570/998 | lr: 4.25e-05 | global_batch_size: 16 | global_step: 570 | reduced_train_loss: 0.4213 | train_step_timing in s: 1.274 | consumed_samples: 9136 | val_loss: 0.2223
[NeMo I 2025-10-07 01:02:27 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 571/998 | lr: 4.233e-05 | global_batch_size: 16 | global_step: 571 | reduced_train_loss: 0.1638 | train_step_timing in s: 1.238 | consumed_samples: 9152 | val_loss: 0.2223
[NeMo I 2025-10-07 01:02:31 nemo_logging:393] Successfully saved checkpoint from iteration     570 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=569-consumed_samples=9120.0-last.ckpt
[NeMo I 2025-10-07 01:02:32 nemo_logging:393] Async checkpoint save for step 570 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=569-consumed_samples=9120.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:02:32 nemo_logging:393] Async finalization time took 1.416 s
Training epoch 0, iteration 572/998 | lr: 4.217e-05 | global_batch_size: 16 | global_step: 572 | reduced_train_loss: 0.1833 | train_step_timing in s: 1.24 | consumed_samples: 9168 | val_loss: 0.2223
Training epoch 0, iteration 573/998 | lr: 4.201e-05 | global_batch_size: 16 | global_step: 573 | reduced_train_loss: 0.2733 | train_step_timing in s: 1.768 | consumed_samples: 9184 | val_loss: 0.2223
Training epoch 0, iteration 574/998 | lr: 4.184e-05 | global_batch_size: 16 | global_step: 574 | reduced_train_loss: 0.2204 | train_step_timing in s: 1.255 | consumed_samples: 9200 | val_loss: 0.2223
Training epoch 0, iteration 575/998 | lr: 4.168e-05 | global_batch_size: 16 | global_step: 575 | reduced_train_loss: 0.2643 | train_step_timing in s: 1.329 | consumed_samples: 9216 | val_loss: 0.2223
Training epoch 0, iteration 576/998 | lr: 4.152e-05 | global_batch_size: 16 | global_step: 576 | reduced_train_loss: 0.2743 | train_step_timing in s: 1.232 | consumed_samples: 9232 | val_loss: 0.2223
Training epoch 0, iteration 577/998 | lr: 4.135e-05 | global_batch_size: 16 | global_step: 577 | reduced_train_loss: 0.1271 | train_step_timing in s: 1.682 | consumed_samples: 9248 | val_loss: 0.2223
Training epoch 0, iteration 578/998 | lr: 4.119e-05 | global_batch_size: 16 | global_step: 578 | reduced_train_loss: 0.1646 | train_step_timing in s: 1.255 | consumed_samples: 9264 | val_loss: 0.2223
Training epoch 0, iteration 579/998 | lr: 4.103e-05 | global_batch_size: 16 | global_step: 579 | reduced_train_loss: 0.1074 | train_step_timing in s: 1.201 | consumed_samples: 9280 | val_loss: 0.2223
Training epoch 0, iteration 580/998 | lr: 4.087e-05 | global_batch_size: 16 | global_step: 580 | reduced_train_loss: 0.2489 | train_step_timing in s: 1.736 | consumed_samples: 9296 | val_loss: 0.2223
Training epoch 0, iteration 581/998 | lr: 4.07e-05 | global_batch_size: 16 | global_step: 581 | reduced_train_loss: 0.3563 | train_step_timing in s: 1.737 | consumed_samples: 9312 | val_loss: 0.2223
Training epoch 0, iteration 582/998 | lr: 4.054e-05 | global_batch_size: 16 | global_step: 582 | reduced_train_loss: 0.1209 | train_step_timing in s: 1.39 | consumed_samples: 9328 | val_loss: 0.2223
Training epoch 0, iteration 583/998 | lr: 4.038e-05 | global_batch_size: 16 | global_step: 583 | reduced_train_loss: 0.1528 | train_step_timing in s: 1.267 | consumed_samples: 9344 | val_loss: 0.2223
Training epoch 0, iteration 584/998 | lr: 4.022e-05 | global_batch_size: 16 | global_step: 584 | reduced_train_loss: 0.2221 | train_step_timing in s: 1.235 | consumed_samples: 9360 | val_loss: 0.2223
Training epoch 0, iteration 585/998 | lr: 4.005e-05 | global_batch_size: 16 | global_step: 585 | reduced_train_loss: 0.499 | train_step_timing in s: 1.531 | consumed_samples: 9376 | val_loss: 0.2223
Training epoch 0, iteration 586/998 | lr: 3.989e-05 | global_batch_size: 16 | global_step: 586 | reduced_train_loss: 0.1117 | train_step_timing in s: 1.248 | consumed_samples: 9392 | val_loss: 0.2223
Training epoch 0, iteration 587/998 | lr: 3.973e-05 | global_batch_size: 16 | global_step: 587 | reduced_train_loss: 0.3724 | train_step_timing in s: 1.564 | consumed_samples: 9408 | val_loss: 0.2223
Training epoch 0, iteration 588/998 | lr: 3.957e-05 | global_batch_size: 16 | global_step: 588 | reduced_train_loss: 0.3181 | train_step_timing in s: 1.267 | consumed_samples: 9424 | val_loss: 0.2223
Training epoch 0, iteration 589/998 | lr: 3.94e-05 | global_batch_size: 16 | global_step: 589 | reduced_train_loss: 0.2629 | train_step_timing in s: 1.667 | consumed_samples: 9440 | val_loss: 0.2223
Training epoch 0, iteration 590/998 | lr: 3.924e-05 | global_batch_size: 16 | global_step: 590 | reduced_train_loss: 0.1986 | train_step_timing in s: 1.801 | consumed_samples: 9456 | val_loss: 0.2223
Training epoch 0, iteration 591/998 | lr: 3.908e-05 | global_batch_size: 16 | global_step: 591 | reduced_train_loss: 0.1574 | train_step_timing in s: 1.713 | consumed_samples: 9472 | val_loss: 0.2223
Training epoch 0, iteration 592/998 | lr: 3.892e-05 | global_batch_size: 16 | global_step: 592 | reduced_train_loss: 0.279 | train_step_timing in s: 1.315 | consumed_samples: 9488 | val_loss: 0.2223
Training epoch 0, iteration 593/998 | lr: 3.876e-05 | global_batch_size: 16 | global_step: 593 | reduced_train_loss: 0.2464 | train_step_timing in s: 2.377 | consumed_samples: 9504 | val_loss: 0.2223
Training epoch 0, iteration 594/998 | lr: 3.86e-05 | global_batch_size: 16 | global_step: 594 | reduced_train_loss: 0.2398 | train_step_timing in s: 1.382 | consumed_samples: 9520 | val_loss: 0.2223
Training epoch 0, iteration 595/998 | lr: 3.844e-05 | global_batch_size: 16 | global_step: 595 | reduced_train_loss: 0.07997 | train_step_timing in s: 1.254 | consumed_samples: 9536 | val_loss: 0.2223
Training epoch 0, iteration 596/998 | lr: 3.828e-05 | global_batch_size: 16 | global_step: 596 | reduced_train_loss: 0.1047 | train_step_timing in s: 1.226 | consumed_samples: 9552 | val_loss: 0.2223
Training epoch 0, iteration 597/998 | lr: 3.811e-05 | global_batch_size: 16 | global_step: 597 | reduced_train_loss: 0.09213 | train_step_timing in s: 1.359 | consumed_samples: 9568 | val_loss: 0.2223
Training epoch 0, iteration 598/998 | lr: 3.795e-05 | global_batch_size: 16 | global_step: 598 | reduced_train_loss: 0.2048 | train_step_timing in s: 1.56 | consumed_samples: 9584 | val_loss: 0.2223
Training epoch 0, iteration 599/998 | lr: 3.779e-05 | global_batch_size: 16 | global_step: 599 | reduced_train_loss: 0.1895 | train_step_timing in s: 1.249 | consumed_samples: 9600 | val_loss: 0.2223
Epoch 0, global step 599: 'val_loss' was not in top 2
[NeMo I 2025-10-07 01:04:39 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 599 : Start time: 1759799078.484s : Save duration: 0.726s
[NeMo I 2025-10-07 01:04:42 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=599-consumed_samples=9600.0-last.ckpt
[NeMo I 2025-10-07 01:04:42 nemo_logging:393] Async finalization time took 0.003 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 600/998 | lr: 3.763e-05 | global_batch_size: 16 | global_step: 600 | reduced_train_loss: 0.1564 | train_step_timing in s: 1.821 | consumed_samples: 9616 | val_loss: 0.2101
[NeMo I 2025-10-07 01:09:07 nemo_logging:393] Successfully saved checkpoint from iteration     599 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=599-consumed_samples=9600.0-last.ckpt
[NeMo I 2025-10-07 01:09:08 nemo_logging:393] Async checkpoint save for step 600 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.22-step=599-consumed_samples=9600.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:09:12 nemo_logging:393] Async finalization time took 5.410 s
Training epoch 0, iteration 601/998 | lr: 3.747e-05 | global_batch_size: 16 | global_step: 601 | reduced_train_loss: 0.3668 | train_step_timing in s: 1.384 | consumed_samples: 9632 | val_loss: 0.2101
Training epoch 0, iteration 602/998 | lr: 3.731e-05 | global_batch_size: 16 | global_step: 602 | reduced_train_loss: 0.2109 | train_step_timing in s: 1.342 | consumed_samples: 9648 | val_loss: 0.2101
Training epoch 0, iteration 603/998 | lr: 3.715e-05 | global_batch_size: 16 | global_step: 603 | reduced_train_loss: 0.2402 | train_step_timing in s: 1.562 | consumed_samples: 9664 | val_loss: 0.2101
Training epoch 0, iteration 604/998 | lr: 3.699e-05 | global_batch_size: 16 | global_step: 604 | reduced_train_loss: 0.5227 | train_step_timing in s: 1.241 | consumed_samples: 9680 | val_loss: 0.2101
Training epoch 0, iteration 605/998 | lr: 3.683e-05 | global_batch_size: 16 | global_step: 605 | reduced_train_loss: 0.2195 | train_step_timing in s: 1.385 | consumed_samples: 9696 | val_loss: 0.2101
Training epoch 0, iteration 606/998 | lr: 3.667e-05 | global_batch_size: 16 | global_step: 606 | reduced_train_loss: 0.2106 | train_step_timing in s: 1.836 | consumed_samples: 9712 | val_loss: 0.2101
Training epoch 0, iteration 607/998 | lr: 3.651e-05 | global_batch_size: 16 | global_step: 607 | reduced_train_loss: 0.1348 | train_step_timing in s: 1.285 | consumed_samples: 9728 | val_loss: 0.2101
Training epoch 0, iteration 608/998 | lr: 3.635e-05 | global_batch_size: 16 | global_step: 608 | reduced_train_loss: 0.1624 | train_step_timing in s: 1.31 | consumed_samples: 9744 | val_loss: 0.2101
Training epoch 0, iteration 609/998 | lr: 3.62e-05 | global_batch_size: 16 | global_step: 609 | reduced_train_loss: 0.2238 | train_step_timing in s: 1.308 | consumed_samples: 9760 | val_loss: 0.2101
Training epoch 0, iteration 610/998 | lr: 3.604e-05 | global_batch_size: 16 | global_step: 610 | reduced_train_loss: 0.2219 | train_step_timing in s: 1.255 | consumed_samples: 9776 | val_loss: 0.2101
Training epoch 0, iteration 611/998 | lr: 3.588e-05 | global_batch_size: 16 | global_step: 611 | reduced_train_loss: 0.1565 | train_step_timing in s: 1.689 | consumed_samples: 9792 | val_loss: 0.2101
Training epoch 0, iteration 612/998 | lr: 3.572e-05 | global_batch_size: 16 | global_step: 612 | reduced_train_loss: 0.1863 | train_step_timing in s: 1.221 | consumed_samples: 9808 | val_loss: 0.2101
Training epoch 0, iteration 613/998 | lr: 3.556e-05 | global_batch_size: 16 | global_step: 613 | reduced_train_loss: 0.2602 | train_step_timing in s: 1.33 | consumed_samples: 9824 | val_loss: 0.2101
Training epoch 0, iteration 614/998 | lr: 3.54e-05 | global_batch_size: 16 | global_step: 614 | reduced_train_loss: 0.1378 | train_step_timing in s: 1.53 | consumed_samples: 9840 | val_loss: 0.2101
Training epoch 0, iteration 615/998 | lr: 3.524e-05 | global_batch_size: 16 | global_step: 615 | reduced_train_loss: 0.1432 | train_step_timing in s: 1.223 | consumed_samples: 9856 | val_loss: 0.2101
Training epoch 0, iteration 616/998 | lr: 3.509e-05 | global_batch_size: 16 | global_step: 616 | reduced_train_loss: 0.2336 | train_step_timing in s: 1.306 | consumed_samples: 9872 | val_loss: 0.2101
Training epoch 0, iteration 617/998 | lr: 3.493e-05 | global_batch_size: 16 | global_step: 617 | reduced_train_loss: 0.1372 | train_step_timing in s: 1.179 | consumed_samples: 9888 | val_loss: 0.2101
Training epoch 0, iteration 618/998 | lr: 3.477e-05 | global_batch_size: 16 | global_step: 618 | reduced_train_loss: 0.2114 | train_step_timing in s: 1.153 | consumed_samples: 9904 | val_loss: 0.2101
Training epoch 0, iteration 619/998 | lr: 3.461e-05 | global_batch_size: 16 | global_step: 619 | reduced_train_loss: 0.1685 | train_step_timing in s: 1.223 | consumed_samples: 9920 | val_loss: 0.2101
Training epoch 0, iteration 620/998 | lr: 3.445e-05 | global_batch_size: 16 | global_step: 620 | reduced_train_loss: 0.195 | train_step_timing in s: 1.331 | consumed_samples: 9936 | val_loss: 0.2101
Training epoch 0, iteration 621/998 | lr: 3.43e-05 | global_batch_size: 16 | global_step: 621 | reduced_train_loss: 0.1367 | train_step_timing in s: 1.684 | consumed_samples: 9952 | val_loss: 0.2101
Training epoch 0, iteration 622/998 | lr: 3.414e-05 | global_batch_size: 16 | global_step: 622 | reduced_train_loss: 0.1444 | train_step_timing in s: 1.23 | consumed_samples: 9968 | val_loss: 0.2101
Training epoch 0, iteration 623/998 | lr: 3.398e-05 | global_batch_size: 16 | global_step: 623 | reduced_train_loss: 0.1726 | train_step_timing in s: 1.793 | consumed_samples: 9984 | val_loss: 0.2101
Training epoch 0, iteration 624/998 | lr: 3.383e-05 | global_batch_size: 16 | global_step: 624 | reduced_train_loss: 0.08897 | train_step_timing in s: 1.311 | consumed_samples: 10000 | val_loss: 0.2101
Training epoch 0, iteration 625/998 | lr: 3.367e-05 | global_batch_size: 16 | global_step: 625 | reduced_train_loss: 0.1914 | train_step_timing in s: 1.238 | consumed_samples: 10016 | val_loss: 0.2101
Training epoch 0, iteration 626/998 | lr: 3.351e-05 | global_batch_size: 16 | global_step: 626 | reduced_train_loss: 0.2135 | train_step_timing in s: 1.217 | consumed_samples: 10032 | val_loss: 0.2101
Training epoch 0, iteration 627/998 | lr: 3.336e-05 | global_batch_size: 16 | global_step: 627 | reduced_train_loss: 0.2371 | train_step_timing in s: 1.327 | consumed_samples: 10048 | val_loss: 0.2101
Training epoch 0, iteration 628/998 | lr: 3.32e-05 | global_batch_size: 16 | global_step: 628 | reduced_train_loss: 0.169 | train_step_timing in s: 1.25 | consumed_samples: 10064 | val_loss: 0.2101
Training epoch 0, iteration 629/998 | lr: 3.305e-05 | global_batch_size: 16 | global_step: 629 | reduced_train_loss: 0.1394 | train_step_timing in s: 1.241 | consumed_samples: 10080 | val_loss: 0.2101
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:15:37 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 630 : Start time: 1759799736.957s : Save duration: 0.586s
[NeMo I 2025-10-07 01:15:41 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=629-consumed_samples=10080.0-last.ckpt
Training epoch 0, iteration 630/998 | lr: 3.289e-05 | global_batch_size: 16 | global_step: 630 | reduced_train_loss: 0.1847 | train_step_timing in s: 1.537 | consumed_samples: 10096 | val_loss: 0.2109
[NeMo I 2025-10-07 01:15:42 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 631/998 | lr: 3.273e-05 | global_batch_size: 16 | global_step: 631 | reduced_train_loss: 0.3791 | train_step_timing in s: 1.638 | consumed_samples: 10112 | val_loss: 0.2109
[NeMo I 2025-10-07 01:15:47 nemo_logging:393] Successfully saved checkpoint from iteration     630 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=629-consumed_samples=10080.0-last.ckpt
[NeMo I 2025-10-07 01:15:47 nemo_logging:393] Async checkpoint save for step 630 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=629-consumed_samples=10080.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:15:50 nemo_logging:393] Async finalization time took 4.295 s
Training epoch 0, iteration 632/998 | lr: 3.258e-05 | global_batch_size: 16 | global_step: 632 | reduced_train_loss: 0.2047 | train_step_timing in s: 1.203 | consumed_samples: 10128 | val_loss: 0.2109
Training epoch 0, iteration 633/998 | lr: 3.242e-05 | global_batch_size: 16 | global_step: 633 | reduced_train_loss: 0.4102 | train_step_timing in s: 1.522 | consumed_samples: 10144 | val_loss: 0.2109
Training epoch 0, iteration 634/998 | lr: 3.227e-05 | global_batch_size: 16 | global_step: 634 | reduced_train_loss: 0.3152 | train_step_timing in s: 1.132 | consumed_samples: 10160 | val_loss: 0.2109
Training epoch 0, iteration 635/998 | lr: 3.212e-05 | global_batch_size: 16 | global_step: 635 | reduced_train_loss: 0.1273 | train_step_timing in s: 1.121 | consumed_samples: 10176 | val_loss: 0.2109
Training epoch 0, iteration 636/998 | lr: 3.196e-05 | global_batch_size: 16 | global_step: 636 | reduced_train_loss: 0.1652 | train_step_timing in s: 1.301 | consumed_samples: 10192 | val_loss: 0.2109
Training epoch 0, iteration 637/998 | lr: 3.181e-05 | global_batch_size: 16 | global_step: 637 | reduced_train_loss: 0.2123 | train_step_timing in s: 1.24 | consumed_samples: 10208 | val_loss: 0.2109
Training epoch 0, iteration 638/998 | lr: 3.165e-05 | global_batch_size: 16 | global_step: 638 | reduced_train_loss: 0.1027 | train_step_timing in s: 1.735 | consumed_samples: 10224 | val_loss: 0.2109
Training epoch 0, iteration 639/998 | lr: 3.15e-05 | global_batch_size: 16 | global_step: 639 | reduced_train_loss: 0.07496 | train_step_timing in s: 1.357 | consumed_samples: 10240 | val_loss: 0.2109
Training epoch 0, iteration 640/998 | lr: 3.134e-05 | global_batch_size: 16 | global_step: 640 | reduced_train_loss: 0.1563 | train_step_timing in s: 1.211 | consumed_samples: 10256 | val_loss: 0.2109
Training epoch 0, iteration 641/998 | lr: 3.119e-05 | global_batch_size: 16 | global_step: 641 | reduced_train_loss: 0.208 | train_step_timing in s: 1.144 | consumed_samples: 10272 | val_loss: 0.2109
Training epoch 0, iteration 642/998 | lr: 3.104e-05 | global_batch_size: 16 | global_step: 642 | reduced_train_loss: 0.1895 | train_step_timing in s: 1.78 | consumed_samples: 10288 | val_loss: 0.2109
Training epoch 0, iteration 643/998 | lr: 3.088e-05 | global_batch_size: 16 | global_step: 643 | reduced_train_loss: 0.3917 | train_step_timing in s: 1.151 | consumed_samples: 10304 | val_loss: 0.2109
Training epoch 0, iteration 644/998 | lr: 3.073e-05 | global_batch_size: 16 | global_step: 644 | reduced_train_loss: 0.2504 | train_step_timing in s: 1.244 | consumed_samples: 10320 | val_loss: 0.2109
Training epoch 0, iteration 645/998 | lr: 3.058e-05 | global_batch_size: 16 | global_step: 645 | reduced_train_loss: 0.1014 | train_step_timing in s: 1.67 | consumed_samples: 10336 | val_loss: 0.2109
Training epoch 0, iteration 646/998 | lr: 3.043e-05 | global_batch_size: 16 | global_step: 646 | reduced_train_loss: 0.1405 | train_step_timing in s: 1.387 | consumed_samples: 10352 | val_loss: 0.2109
Training epoch 0, iteration 647/998 | lr: 3.027e-05 | global_batch_size: 16 | global_step: 647 | reduced_train_loss: 0.1519 | train_step_timing in s: 1.238 | consumed_samples: 10368 | val_loss: 0.2109
Training epoch 0, iteration 648/998 | lr: 3.012e-05 | global_batch_size: 16 | global_step: 648 | reduced_train_loss: 0.2672 | train_step_timing in s: 1.327 | consumed_samples: 10384 | val_loss: 0.2109
Training epoch 0, iteration 649/998 | lr: 2.997e-05 | global_batch_size: 16 | global_step: 649 | reduced_train_loss: 0.2475 | train_step_timing in s: 1.774 | consumed_samples: 10400 | val_loss: 0.2109
Epoch 0, global step 649: 'val_loss' reached 0.21093 (best 0.21093), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=649-consumed_samples=10400.0.ckpt' as top 2
[NeMo I 2025-10-07 01:17:12 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 649 : Start time: 1759799831.764s : Save duration: 0.577s
[NeMo I 2025-10-07 01:17:15 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=649-consumed_samples=10400.0.ckpt
[NeMo I 2025-10-07 01:17:15 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 650/998 | lr: 2.982e-05 | global_batch_size: 16 | global_step: 650 | reduced_train_loss: 0.1369 | train_step_timing in s: 1.226 | consumed_samples: 10416 | val_loss: 0.2109
[NeMo I 2025-10-07 01:17:18 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 651/998 | lr: 2.967e-05 | global_batch_size: 16 | global_step: 651 | reduced_train_loss: 0.2241 | train_step_timing in s: 1.756 | consumed_samples: 10432 | val_loss: 0.2109
[NeMo I 2025-10-07 01:17:24 nemo_logging:393] Successfully saved checkpoint from iteration     649 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=649-consumed_samples=10400.0.ckpt
[NeMo I 2025-10-07 01:17:24 nemo_logging:393] Async checkpoint save for step 650 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=649-consumed_samples=10400.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:17:31 nemo_logging:393] Async finalization time took 7.956 s
Training epoch 0, iteration 652/998 | lr: 2.952e-05 | global_batch_size: 16 | global_step: 652 | reduced_train_loss: 0.1419 | train_step_timing in s: 1.294 | consumed_samples: 10448 | val_loss: 0.2109
Training epoch 0, iteration 653/998 | lr: 2.937e-05 | global_batch_size: 16 | global_step: 653 | reduced_train_loss: 0.2394 | train_step_timing in s: 1.299 | consumed_samples: 10464 | val_loss: 0.2109
Training epoch 0, iteration 654/998 | lr: 2.922e-05 | global_batch_size: 16 | global_step: 654 | reduced_train_loss: 0.2969 | train_step_timing in s: 1.341 | consumed_samples: 10480 | val_loss: 0.2109
Training epoch 0, iteration 655/998 | lr: 2.907e-05 | global_batch_size: 16 | global_step: 655 | reduced_train_loss: 0.2746 | train_step_timing in s: 1.398 | consumed_samples: 10496 | val_loss: 0.2109
Training epoch 0, iteration 656/998 | lr: 2.891e-05 | global_batch_size: 16 | global_step: 656 | reduced_train_loss: 0.4049 | train_step_timing in s: 1.281 | consumed_samples: 10512 | val_loss: 0.2109
Training epoch 0, iteration 657/998 | lr: 2.876e-05 | global_batch_size: 16 | global_step: 657 | reduced_train_loss: 0.2436 | train_step_timing in s: 1.341 | consumed_samples: 10528 | val_loss: 0.2109
Training epoch 0, iteration 658/998 | lr: 2.862e-05 | global_batch_size: 16 | global_step: 658 | reduced_train_loss: 0.1932 | train_step_timing in s: 1.776 | consumed_samples: 10544 | val_loss: 0.2109
Training epoch 0, iteration 659/998 | lr: 2.847e-05 | global_batch_size: 16 | global_step: 659 | reduced_train_loss: 0.181 | train_step_timing in s: 1.275 | consumed_samples: 10560 | val_loss: 0.2109
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:22:26 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 660 : Start time: 1759800145.215s : Save duration: 1.073s
[NeMo I 2025-10-07 01:22:29 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=659-consumed_samples=10560.0-last.ckpt
Training epoch 0, iteration 660/998 | lr: 2.832e-05 | global_batch_size: 16 | global_step: 660 | reduced_train_loss: 0.2582 | train_step_timing in s: 1.317 | consumed_samples: 10576 | val_loss: 0.2085
[NeMo I 2025-10-07 01:22:30 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 661/998 | lr: 2.817e-05 | global_batch_size: 16 | global_step: 661 | reduced_train_loss: 0.1891 | train_step_timing in s: 1.233 | consumed_samples: 10592 | val_loss: 0.2085
[NeMo I 2025-10-07 01:22:35 nemo_logging:393] Successfully saved checkpoint from iteration     660 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=659-consumed_samples=10560.0-last.ckpt
[NeMo I 2025-10-07 01:22:35 nemo_logging:393] Async checkpoint save for step 660 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=659-consumed_samples=10560.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:22:35 nemo_logging:393] Async finalization time took 1.455 s
Training epoch 0, iteration 662/998 | lr: 2.802e-05 | global_batch_size: 16 | global_step: 662 | reduced_train_loss: 0.1847 | train_step_timing in s: 1.325 | consumed_samples: 10608 | val_loss: 0.2085
Training epoch 0, iteration 663/998 | lr: 2.787e-05 | global_batch_size: 16 | global_step: 663 | reduced_train_loss: 0.1723 | train_step_timing in s: 1.26 | consumed_samples: 10624 | val_loss: 0.2085
Training epoch 0, iteration 664/998 | lr: 2.772e-05 | global_batch_size: 16 | global_step: 664 | reduced_train_loss: 0.1612 | train_step_timing in s: 2.148 | consumed_samples: 10640 | val_loss: 0.2085
Training epoch 0, iteration 665/998 | lr: 2.757e-05 | global_batch_size: 16 | global_step: 665 | reduced_train_loss: 0.1677 | train_step_timing in s: 1.342 | consumed_samples: 10656 | val_loss: 0.2085
Training epoch 0, iteration 666/998 | lr: 2.743e-05 | global_batch_size: 16 | global_step: 666 | reduced_train_loss: 0.2488 | train_step_timing in s: 1.282 | consumed_samples: 10672 | val_loss: 0.2085
Training epoch 0, iteration 667/998 | lr: 2.728e-05 | global_batch_size: 16 | global_step: 667 | reduced_train_loss: 0.2586 | train_step_timing in s: 1.265 | consumed_samples: 10688 | val_loss: 0.2085
Training epoch 0, iteration 668/998 | lr: 2.713e-05 | global_batch_size: 16 | global_step: 668 | reduced_train_loss: 0.1318 | train_step_timing in s: 1.662 | consumed_samples: 10704 | val_loss: 0.2085
Training epoch 0, iteration 669/998 | lr: 2.698e-05 | global_batch_size: 16 | global_step: 669 | reduced_train_loss: 0.1681 | train_step_timing in s: 1.667 | consumed_samples: 10720 | val_loss: 0.2085
Training epoch 0, iteration 670/998 | lr: 2.684e-05 | global_batch_size: 16 | global_step: 670 | reduced_train_loss: 0.316 | train_step_timing in s: 1.687 | consumed_samples: 10736 | val_loss: 0.2085
Training epoch 0, iteration 671/998 | lr: 2.669e-05 | global_batch_size: 16 | global_step: 671 | reduced_train_loss: 0.2278 | train_step_timing in s: 1.249 | consumed_samples: 10752 | val_loss: 0.2085
Training epoch 0, iteration 672/998 | lr: 2.654e-05 | global_batch_size: 16 | global_step: 672 | reduced_train_loss: 0.171 | train_step_timing in s: 1.156 | consumed_samples: 10768 | val_loss: 0.2085
Training epoch 0, iteration 673/998 | lr: 2.64e-05 | global_batch_size: 16 | global_step: 673 | reduced_train_loss: 0.2591 | train_step_timing in s: 1.692 | consumed_samples: 10784 | val_loss: 0.2085
Training epoch 0, iteration 674/998 | lr: 2.625e-05 | global_batch_size: 16 | global_step: 674 | reduced_train_loss: 0.2321 | train_step_timing in s: 1.239 | consumed_samples: 10800 | val_loss: 0.2085
Training epoch 0, iteration 675/998 | lr: 2.611e-05 | global_batch_size: 16 | global_step: 675 | reduced_train_loss: 0.1272 | train_step_timing in s: 1.386 | consumed_samples: 10816 | val_loss: 0.2085
Training epoch 0, iteration 676/998 | lr: 2.596e-05 | global_batch_size: 16 | global_step: 676 | reduced_train_loss: 0.1273 | train_step_timing in s: 1.364 | consumed_samples: 10832 | val_loss: 0.2085
Training epoch 0, iteration 677/998 | lr: 2.582e-05 | global_batch_size: 16 | global_step: 677 | reduced_train_loss: 0.2274 | train_step_timing in s: 1.248 | consumed_samples: 10848 | val_loss: 0.2085
Training epoch 0, iteration 678/998 | lr: 2.567e-05 | global_batch_size: 16 | global_step: 678 | reduced_train_loss: 0.1518 | train_step_timing in s: 1.25 | consumed_samples: 10864 | val_loss: 0.2085
Training epoch 0, iteration 679/998 | lr: 2.553e-05 | global_batch_size: 16 | global_step: 679 | reduced_train_loss: 0.1854 | train_step_timing in s: 1.25 | consumed_samples: 10880 | val_loss: 0.2085
Training epoch 0, iteration 680/998 | lr: 2.538e-05 | global_batch_size: 16 | global_step: 680 | reduced_train_loss: 0.2551 | train_step_timing in s: 1.31 | consumed_samples: 10896 | val_loss: 0.2085
Training epoch 0, iteration 681/998 | lr: 2.524e-05 | global_batch_size: 16 | global_step: 681 | reduced_train_loss: 0.1225 | train_step_timing in s: 1.713 | consumed_samples: 10912 | val_loss: 0.2085
Training epoch 0, iteration 682/998 | lr: 2.51e-05 | global_batch_size: 16 | global_step: 682 | reduced_train_loss: 0.2964 | train_step_timing in s: 1.257 | consumed_samples: 10928 | val_loss: 0.2085
Training epoch 0, iteration 683/998 | lr: 2.495e-05 | global_batch_size: 16 | global_step: 683 | reduced_train_loss: 0.3305 | train_step_timing in s: 1.266 | consumed_samples: 10944 | val_loss: 0.2085
Training epoch 0, iteration 684/998 | lr: 2.481e-05 | global_batch_size: 16 | global_step: 684 | reduced_train_loss: 0.2329 | train_step_timing in s: 1.236 | consumed_samples: 10960 | val_loss: 0.2085
Training epoch 0, iteration 685/998 | lr: 2.467e-05 | global_batch_size: 16 | global_step: 685 | reduced_train_loss: 0.3089 | train_step_timing in s: 1.675 | consumed_samples: 10976 | val_loss: 0.2085
Training epoch 0, iteration 686/998 | lr: 2.452e-05 | global_batch_size: 16 | global_step: 686 | reduced_train_loss: 0.2162 | train_step_timing in s: 1.855 | consumed_samples: 10992 | val_loss: 0.2085
Training epoch 0, iteration 687/998 | lr: 2.438e-05 | global_batch_size: 16 | global_step: 687 | reduced_train_loss: 0.1738 | train_step_timing in s: 1.402 | consumed_samples: 11008 | val_loss: 0.2085
Training epoch 0, iteration 688/998 | lr: 2.424e-05 | global_batch_size: 16 | global_step: 688 | reduced_train_loss: 0.1526 | train_step_timing in s: 1.692 | consumed_samples: 11024 | val_loss: 0.2085
Training epoch 0, iteration 689/998 | lr: 2.41e-05 | global_batch_size: 16 | global_step: 689 | reduced_train_loss: 0.1922 | train_step_timing in s: 1.177 | consumed_samples: 11040 | val_loss: 0.2085
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:29:17 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 690 : Start time: 1759800557.179s : Save duration: 0.566s
[NeMo I 2025-10-07 01:29:21 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=689-consumed_samples=11040.0-last.ckpt
Training epoch 0, iteration 690/998 | lr: 2.396e-05 | global_batch_size: 16 | global_step: 690 | reduced_train_loss: 0.4034 | train_step_timing in s: 2.056 | consumed_samples: 11056 | val_loss: 0.2101
[NeMo I 2025-10-07 01:29:23 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 691/998 | lr: 2.382e-05 | global_batch_size: 16 | global_step: 691 | reduced_train_loss: 0.2101 | train_step_timing in s: 1.251 | consumed_samples: 11072 | val_loss: 0.2101
[NeMo I 2025-10-07 01:29:27 nemo_logging:393] Successfully saved checkpoint from iteration     690 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=689-consumed_samples=11040.0-last.ckpt
[NeMo I 2025-10-07 01:29:27 nemo_logging:393] Async checkpoint save for step 690 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=689-consumed_samples=11040.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:29:31 nemo_logging:393] Async finalization time took 4.253 s
Training epoch 0, iteration 692/998 | lr: 2.367e-05 | global_batch_size: 16 | global_step: 692 | reduced_train_loss: 0.1313 | train_step_timing in s: 1.259 | consumed_samples: 11088 | val_loss: 0.2101
Training epoch 0, iteration 693/998 | lr: 2.353e-05 | global_batch_size: 16 | global_step: 693 | reduced_train_loss: 0.1355 | train_step_timing in s: 1.276 | consumed_samples: 11104 | val_loss: 0.2101
Training epoch 0, iteration 694/998 | lr: 2.339e-05 | global_batch_size: 16 | global_step: 694 | reduced_train_loss: 0.3077 | train_step_timing in s: 1.248 | consumed_samples: 11120 | val_loss: 0.2101
Training epoch 0, iteration 695/998 | lr: 2.325e-05 | global_batch_size: 16 | global_step: 695 | reduced_train_loss: 0.2902 | train_step_timing in s: 1.243 | consumed_samples: 11136 | val_loss: 0.2101
Training epoch 0, iteration 696/998 | lr: 2.311e-05 | global_batch_size: 16 | global_step: 696 | reduced_train_loss: 0.2144 | train_step_timing in s: 1.243 | consumed_samples: 11152 | val_loss: 0.2101
Training epoch 0, iteration 697/998 | lr: 2.297e-05 | global_batch_size: 16 | global_step: 697 | reduced_train_loss: 0.2352 | train_step_timing in s: 1.74 | consumed_samples: 11168 | val_loss: 0.2101
Training epoch 0, iteration 698/998 | lr: 2.284e-05 | global_batch_size: 16 | global_step: 698 | reduced_train_loss: 0.1326 | train_step_timing in s: 1.342 | consumed_samples: 11184 | val_loss: 0.2101
Training epoch 0, iteration 699/998 | lr: 2.27e-05 | global_batch_size: 16 | global_step: 699 | reduced_train_loss: 0.2398 | train_step_timing in s: 1.387 | consumed_samples: 11200 | val_loss: 0.2101
Epoch 0, global step 699: 'val_loss' reached 0.21013 (best 0.21013), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=699-consumed_samples=11200.0.ckpt' as top 2
[NeMo I 2025-10-07 01:30:07 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 699 : Start time: 1759800606.685s : Save duration: 1.030s
[NeMo I 2025-10-07 01:30:10 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=699-consumed_samples=11200.0.ckpt
[NeMo I 2025-10-07 01:30:11 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 700/998 | lr: 2.256e-05 | global_batch_size: 16 | global_step: 700 | reduced_train_loss: 0.5028 | train_step_timing in s: 1.412 | consumed_samples: 11216 | val_loss: 0.2101
[NeMo I 2025-10-07 01:30:14 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 701/998 | lr: 2.242e-05 | global_batch_size: 16 | global_step: 701 | reduced_train_loss: 0.2492 | train_step_timing in s: 1.223 | consumed_samples: 11232 | val_loss: 0.2101
[NeMo I 2025-10-07 01:30:18 nemo_logging:393] Successfully saved checkpoint from iteration     699 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=699-consumed_samples=11200.0.ckpt
[NeMo I 2025-10-07 01:30:19 nemo_logging:393] Async checkpoint save for step 700 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=699-consumed_samples=11200.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:30:26 nemo_logging:393] Async finalization time took 8.403 s
Training epoch 0, iteration 702/998 | lr: 2.228e-05 | global_batch_size: 16 | global_step: 702 | reduced_train_loss: 0.1178 | train_step_timing in s: 1.344 | consumed_samples: 11248 | val_loss: 0.2101
Training epoch 0, iteration 703/998 | lr: 2.214e-05 | global_batch_size: 16 | global_step: 703 | reduced_train_loss: 0.2257 | train_step_timing in s: 1.322 | consumed_samples: 11264 | val_loss: 0.2101
Training epoch 0, iteration 704/998 | lr: 2.201e-05 | global_batch_size: 16 | global_step: 704 | reduced_train_loss: 0.2577 | train_step_timing in s: 1.676 | consumed_samples: 11280 | val_loss: 0.2101
Training epoch 0, iteration 705/998 | lr: 2.187e-05 | global_batch_size: 16 | global_step: 705 | reduced_train_loss: 0.1306 | train_step_timing in s: 1.368 | consumed_samples: 11296 | val_loss: 0.2101
Training epoch 0, iteration 706/998 | lr: 2.173e-05 | global_batch_size: 16 | global_step: 706 | reduced_train_loss: 0.2256 | train_step_timing in s: 1.385 | consumed_samples: 11312 | val_loss: 0.2101
Training epoch 0, iteration 707/998 | lr: 2.16e-05 | global_batch_size: 16 | global_step: 707 | reduced_train_loss: 0.2275 | train_step_timing in s: 1.687 | consumed_samples: 11328 | val_loss: 0.2101
Training epoch 0, iteration 708/998 | lr: 2.146e-05 | global_batch_size: 16 | global_step: 708 | reduced_train_loss: 0.1109 | train_step_timing in s: 1.249 | consumed_samples: 11344 | val_loss: 0.2101
Training epoch 0, iteration 709/998 | lr: 2.132e-05 | global_batch_size: 16 | global_step: 709 | reduced_train_loss: 0.3228 | train_step_timing in s: 1.737 | consumed_samples: 11360 | val_loss: 0.2101
Training epoch 0, iteration 710/998 | lr: 2.119e-05 | global_batch_size: 16 | global_step: 710 | reduced_train_loss: 0.3008 | train_step_timing in s: 1.221 | consumed_samples: 11376 | val_loss: 0.2101
Training epoch 0, iteration 711/998 | lr: 2.105e-05 | global_batch_size: 16 | global_step: 711 | reduced_train_loss: 0.1444 | train_step_timing in s: 1.529 | consumed_samples: 11392 | val_loss: 0.2101
Training epoch 0, iteration 712/998 | lr: 2.092e-05 | global_batch_size: 16 | global_step: 712 | reduced_train_loss: 0.3091 | train_step_timing in s: 1.159 | consumed_samples: 11408 | val_loss: 0.2101
Training epoch 0, iteration 713/998 | lr: 2.079e-05 | global_batch_size: 16 | global_step: 713 | reduced_train_loss: 0.07397 | train_step_timing in s: 1.221 | consumed_samples: 11424 | val_loss: 0.2101
Training epoch 0, iteration 714/998 | lr: 2.065e-05 | global_batch_size: 16 | global_step: 714 | reduced_train_loss: 0.2009 | train_step_timing in s: 1.247 | consumed_samples: 11440 | val_loss: 0.2101
Training epoch 0, iteration 715/998 | lr: 2.052e-05 | global_batch_size: 16 | global_step: 715 | reduced_train_loss: 0.2628 | train_step_timing in s: 1.227 | consumed_samples: 11456 | val_loss: 0.2101
Training epoch 0, iteration 716/998 | lr: 2.038e-05 | global_batch_size: 16 | global_step: 716 | reduced_train_loss: 0.1543 | train_step_timing in s: 1.338 | consumed_samples: 11472 | val_loss: 0.2101
Training epoch 0, iteration 717/998 | lr: 2.025e-05 | global_batch_size: 16 | global_step: 717 | reduced_train_loss: 0.1026 | train_step_timing in s: 1.309 | consumed_samples: 11488 | val_loss: 0.2101
Training epoch 0, iteration 718/998 | lr: 2.012e-05 | global_batch_size: 16 | global_step: 718 | reduced_train_loss: 0.197 | train_step_timing in s: 1.237 | consumed_samples: 11504 | val_loss: 0.2101
Training epoch 0, iteration 719/998 | lr: 1.998e-05 | global_batch_size: 16 | global_step: 719 | reduced_train_loss: 0.3347 | train_step_timing in s: 1.239 | consumed_samples: 11520 | val_loss: 0.2101
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:36:09 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 720 : Start time: 1759800968.767s : Save duration: 1.037s
[NeMo I 2025-10-07 01:36:13 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=719-consumed_samples=11520.0-last.ckpt
Training epoch 0, iteration 720/998 | lr: 1.985e-05 | global_batch_size: 16 | global_step: 720 | reduced_train_loss: 0.2673 | train_step_timing in s: 1.78 | consumed_samples: 11536 | val_loss: 0.2064
[NeMo I 2025-10-07 01:36:15 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 721/998 | lr: 1.972e-05 | global_batch_size: 16 | global_step: 721 | reduced_train_loss: 0.06709 | train_step_timing in s: 1.266 | consumed_samples: 11552 | val_loss: 0.2064
[NeMo I 2025-10-07 01:36:18 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 722/998 | lr: 1.959e-05 | global_batch_size: 16 | global_step: 722 | reduced_train_loss: 0.1632 | train_step_timing in s: 1.229 | consumed_samples: 11568 | val_loss: 0.2064
[NeMo I 2025-10-07 01:36:24 nemo_logging:393] Successfully saved checkpoint from iteration     720 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=719-consumed_samples=11520.0-last.ckpt
[NeMo I 2025-10-07 01:36:24 nemo_logging:393] Async checkpoint save for step 720 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=719-consumed_samples=11520.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:36:25 nemo_logging:393] Async finalization time took 1.755 s
Training epoch 0, iteration 723/998 | lr: 1.946e-05 | global_batch_size: 16 | global_step: 723 | reduced_train_loss: 0.1338 | train_step_timing in s: 1.258 | consumed_samples: 11584 | val_loss: 0.2064
Training epoch 0, iteration 724/998 | lr: 1.933e-05 | global_batch_size: 16 | global_step: 724 | reduced_train_loss: 0.1177 | train_step_timing in s: 1.246 | consumed_samples: 11600 | val_loss: 0.2064
Training epoch 0, iteration 725/998 | lr: 1.92e-05 | global_batch_size: 16 | global_step: 725 | reduced_train_loss: 0.1605 | train_step_timing in s: 1.558 | consumed_samples: 11616 | val_loss: 0.2064
Training epoch 0, iteration 726/998 | lr: 1.907e-05 | global_batch_size: 16 | global_step: 726 | reduced_train_loss: 0.2027 | train_step_timing in s: 1.325 | consumed_samples: 11632 | val_loss: 0.2064
Training epoch 0, iteration 727/998 | lr: 1.894e-05 | global_batch_size: 16 | global_step: 727 | reduced_train_loss: 0.2768 | train_step_timing in s: 1.231 | consumed_samples: 11648 | val_loss: 0.2064
Training epoch 0, iteration 728/998 | lr: 1.881e-05 | global_batch_size: 16 | global_step: 728 | reduced_train_loss: 0.2417 | train_step_timing in s: 1.846 | consumed_samples: 11664 | val_loss: 0.2064
Training epoch 0, iteration 729/998 | lr: 1.868e-05 | global_batch_size: 16 | global_step: 729 | reduced_train_loss: 0.2066 | train_step_timing in s: 1.224 | consumed_samples: 11680 | val_loss: 0.2064
Training epoch 0, iteration 730/998 | lr: 1.855e-05 | global_batch_size: 16 | global_step: 730 | reduced_train_loss: 0.1779 | train_step_timing in s: 1.263 | consumed_samples: 11696 | val_loss: 0.2064
Training epoch 0, iteration 731/998 | lr: 1.842e-05 | global_batch_size: 16 | global_step: 731 | reduced_train_loss: 0.1752 | train_step_timing in s: 1.368 | consumed_samples: 11712 | val_loss: 0.2064
Training epoch 0, iteration 732/998 | lr: 1.829e-05 | global_batch_size: 16 | global_step: 732 | reduced_train_loss: 0.1311 | train_step_timing in s: 1.249 | consumed_samples: 11728 | val_loss: 0.2064
Training epoch 0, iteration 733/998 | lr: 1.816e-05 | global_batch_size: 16 | global_step: 733 | reduced_train_loss: 0.3888 | train_step_timing in s: 1.262 | consumed_samples: 11744 | val_loss: 0.2064
Training epoch 0, iteration 734/998 | lr: 1.804e-05 | global_batch_size: 16 | global_step: 734 | reduced_train_loss: 0.3722 | train_step_timing in s: 1.221 | consumed_samples: 11760 | val_loss: 0.2064
Training epoch 0, iteration 735/998 | lr: 1.791e-05 | global_batch_size: 16 | global_step: 735 | reduced_train_loss: 0.1559 | train_step_timing in s: 1.388 | consumed_samples: 11776 | val_loss: 0.2064
Training epoch 0, iteration 736/998 | lr: 1.778e-05 | global_batch_size: 16 | global_step: 736 | reduced_train_loss: 0.2149 | train_step_timing in s: 1.253 | consumed_samples: 11792 | val_loss: 0.2064
Training epoch 0, iteration 737/998 | lr: 1.766e-05 | global_batch_size: 16 | global_step: 737 | reduced_train_loss: 0.3853 | train_step_timing in s: 1.268 | consumed_samples: 11808 | val_loss: 0.2064
Training epoch 0, iteration 738/998 | lr: 1.753e-05 | global_batch_size: 16 | global_step: 738 | reduced_train_loss: 0.1572 | train_step_timing in s: 1.239 | consumed_samples: 11824 | val_loss: 0.2064
Training epoch 0, iteration 739/998 | lr: 1.741e-05 | global_batch_size: 16 | global_step: 739 | reduced_train_loss: 0.19 | train_step_timing in s: 1.723 | consumed_samples: 11840 | val_loss: 0.2064
Training epoch 0, iteration 740/998 | lr: 1.728e-05 | global_batch_size: 16 | global_step: 740 | reduced_train_loss: 0.2521 | train_step_timing in s: 1.176 | consumed_samples: 11856 | val_loss: 0.2064
Training epoch 0, iteration 741/998 | lr: 1.715e-05 | global_batch_size: 16 | global_step: 741 | reduced_train_loss: 0.3199 | train_step_timing in s: 1.288 | consumed_samples: 11872 | val_loss: 0.2064
Training epoch 0, iteration 742/998 | lr: 1.703e-05 | global_batch_size: 16 | global_step: 742 | reduced_train_loss: 0.1544 | train_step_timing in s: 1.262 | consumed_samples: 11888 | val_loss: 0.2064
Training epoch 0, iteration 743/998 | lr: 1.691e-05 | global_batch_size: 16 | global_step: 743 | reduced_train_loss: 0.1872 | train_step_timing in s: 1.368 | consumed_samples: 11904 | val_loss: 0.2064
Training epoch 0, iteration 744/998 | lr: 1.678e-05 | global_batch_size: 16 | global_step: 744 | reduced_train_loss: 0.1922 | train_step_timing in s: 1.155 | consumed_samples: 11920 | val_loss: 0.2064
Training epoch 0, iteration 745/998 | lr: 1.666e-05 | global_batch_size: 16 | global_step: 745 | reduced_train_loss: 0.3445 | train_step_timing in s: 1.236 | consumed_samples: 11936 | val_loss: 0.2064
Training epoch 0, iteration 746/998 | lr: 1.654e-05 | global_batch_size: 16 | global_step: 746 | reduced_train_loss: 0.2032 | train_step_timing in s: 1.572 | consumed_samples: 11952 | val_loss: 0.2064
Training epoch 0, iteration 747/998 | lr: 1.641e-05 | global_batch_size: 16 | global_step: 747 | reduced_train_loss: 0.1476 | train_step_timing in s: 1.538 | consumed_samples: 11968 | val_loss: 0.2064
Training epoch 0, iteration 748/998 | lr: 1.629e-05 | global_batch_size: 16 | global_step: 748 | reduced_train_loss: 0.159 | train_step_timing in s: 1.19 | consumed_samples: 11984 | val_loss: 0.2064
Training epoch 0, iteration 749/998 | lr: 1.617e-05 | global_batch_size: 16 | global_step: 749 | reduced_train_loss: 0.1766 | train_step_timing in s: 1.224 | consumed_samples: 12000 | val_loss: 0.2064
Epoch 0, global step 749: 'val_loss' reached 0.20641 (best 0.20641), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=749-consumed_samples=12000.0.ckpt' as top 2
[NeMo I 2025-10-07 01:38:25 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 749 : Start time: 1759801104.057s : Save duration: 1.003s
[NeMo I 2025-10-07 01:38:28 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=749-consumed_samples=12000.0.ckpt
[NeMo I 2025-10-07 01:38:28 nemo_logging:393] Async finalization time took 0.000 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 750/998 | lr: 1.605e-05 | global_batch_size: 16 | global_step: 750 | reduced_train_loss: 0.3538 | train_step_timing in s: 1.21 | consumed_samples: 12016 | val_loss: 0.2065
[NeMo I 2025-10-07 01:42:57 nemo_logging:393] Successfully saved checkpoint from iteration     749 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=749-consumed_samples=12000.0.ckpt
[NeMo I 2025-10-07 01:42:57 nemo_logging:393] Async checkpoint save for step 750 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.21-step=749-consumed_samples=12000.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:43:04 nemo_logging:393] Async finalization time took 7.742 s
Training epoch 0, iteration 751/998 | lr: 1.592e-05 | global_batch_size: 16 | global_step: 751 | reduced_train_loss: 0.1922 | train_step_timing in s: 1.832 | consumed_samples: 12032 | val_loss: 0.2065
Training epoch 0, iteration 752/998 | lr: 1.58e-05 | global_batch_size: 16 | global_step: 752 | reduced_train_loss: 0.2083 | train_step_timing in s: 1.355 | consumed_samples: 12048 | val_loss: 0.2065
Training epoch 0, iteration 753/998 | lr: 1.568e-05 | global_batch_size: 16 | global_step: 753 | reduced_train_loss: 0.273 | train_step_timing in s: 1.219 | consumed_samples: 12064 | val_loss: 0.2065
Training epoch 0, iteration 754/998 | lr: 1.556e-05 | global_batch_size: 16 | global_step: 754 | reduced_train_loss: 0.2568 | train_step_timing in s: 1.217 | consumed_samples: 12080 | val_loss: 0.2065
Training epoch 0, iteration 755/998 | lr: 1.544e-05 | global_batch_size: 16 | global_step: 755 | reduced_train_loss: 0.1239 | train_step_timing in s: 1.299 | consumed_samples: 12096 | val_loss: 0.2065
Training epoch 0, iteration 756/998 | lr: 1.532e-05 | global_batch_size: 16 | global_step: 756 | reduced_train_loss: 0.2414 | train_step_timing in s: 1.207 | consumed_samples: 12112 | val_loss: 0.2065
Training epoch 0, iteration 757/998 | lr: 1.52e-05 | global_batch_size: 16 | global_step: 757 | reduced_train_loss: 0.1418 | train_step_timing in s: 1.383 | consumed_samples: 12128 | val_loss: 0.2065
Training epoch 0, iteration 758/998 | lr: 1.509e-05 | global_batch_size: 16 | global_step: 758 | reduced_train_loss: 0.2415 | train_step_timing in s: 1.228 | consumed_samples: 12144 | val_loss: 0.2065
Training epoch 0, iteration 759/998 | lr: 1.497e-05 | global_batch_size: 16 | global_step: 759 | reduced_train_loss: 0.158 | train_step_timing in s: 1.65 | consumed_samples: 12160 | val_loss: 0.2065
Training epoch 0, iteration 760/998 | lr: 1.485e-05 | global_batch_size: 16 | global_step: 760 | reduced_train_loss: 0.1541 | train_step_timing in s: 1.364 | consumed_samples: 12176 | val_loss: 0.2065
Training epoch 0, iteration 761/998 | lr: 1.473e-05 | global_batch_size: 16 | global_step: 761 | reduced_train_loss: 0.263 | train_step_timing in s: 1.226 | consumed_samples: 12192 | val_loss: 0.2065
Training epoch 0, iteration 762/998 | lr: 1.462e-05 | global_batch_size: 16 | global_step: 762 | reduced_train_loss: 0.2277 | train_step_timing in s: 1.239 | consumed_samples: 12208 | val_loss: 0.2065
Training epoch 0, iteration 763/998 | lr: 1.45e-05 | global_batch_size: 16 | global_step: 763 | reduced_train_loss: 0.115 | train_step_timing in s: 1.275 | consumed_samples: 12224 | val_loss: 0.2065
Training epoch 0, iteration 764/998 | lr: 1.438e-05 | global_batch_size: 16 | global_step: 764 | reduced_train_loss: 0.2292 | train_step_timing in s: 1.394 | consumed_samples: 12240 | val_loss: 0.2065
Training epoch 0, iteration 765/998 | lr: 1.427e-05 | global_batch_size: 16 | global_step: 765 | reduced_train_loss: 0.2878 | train_step_timing in s: 1.288 | consumed_samples: 12256 | val_loss: 0.2065
Training epoch 0, iteration 766/998 | lr: 1.415e-05 | global_batch_size: 16 | global_step: 766 | reduced_train_loss: 0.2611 | train_step_timing in s: 1.969 | consumed_samples: 12272 | val_loss: 0.2065
Training epoch 0, iteration 767/998 | lr: 1.404e-05 | global_batch_size: 16 | global_step: 767 | reduced_train_loss: 0.08597 | train_step_timing in s: 1.275 | consumed_samples: 12288 | val_loss: 0.2065
Training epoch 0, iteration 768/998 | lr: 1.392e-05 | global_batch_size: 16 | global_step: 768 | reduced_train_loss: 0.1271 | train_step_timing in s: 1.334 | consumed_samples: 12304 | val_loss: 0.2065
Training epoch 0, iteration 769/998 | lr: 1.381e-05 | global_batch_size: 16 | global_step: 769 | reduced_train_loss: 0.199 | train_step_timing in s: 1.277 | consumed_samples: 12320 | val_loss: 0.2065
Training epoch 0, iteration 770/998 | lr: 1.369e-05 | global_batch_size: 16 | global_step: 770 | reduced_train_loss: 0.2054 | train_step_timing in s: 1.852 | consumed_samples: 12336 | val_loss: 0.2065
Training epoch 0, iteration 771/998 | lr: 1.358e-05 | global_batch_size: 16 | global_step: 771 | reduced_train_loss: 0.1502 | train_step_timing in s: 1.252 | consumed_samples: 12352 | val_loss: 0.2065
Training epoch 0, iteration 772/998 | lr: 1.347e-05 | global_batch_size: 16 | global_step: 772 | reduced_train_loss: 0.1979 | train_step_timing in s: 1.246 | consumed_samples: 12368 | val_loss: 0.2065
Training epoch 0, iteration 773/998 | lr: 1.335e-05 | global_batch_size: 16 | global_step: 773 | reduced_train_loss: 0.08281 | train_step_timing in s: 1.217 | consumed_samples: 12384 | val_loss: 0.2065
Training epoch 0, iteration 774/998 | lr: 1.324e-05 | global_batch_size: 16 | global_step: 774 | reduced_train_loss: 0.1451 | train_step_timing in s: 1.293 | consumed_samples: 12400 | val_loss: 0.2065
Training epoch 0, iteration 775/998 | lr: 1.313e-05 | global_batch_size: 16 | global_step: 775 | reduced_train_loss: 0.2268 | train_step_timing in s: 1.697 | consumed_samples: 12416 | val_loss: 0.2065
Training epoch 0, iteration 776/998 | lr: 1.302e-05 | global_batch_size: 16 | global_step: 776 | reduced_train_loss: 0.1127 | train_step_timing in s: 1.533 | consumed_samples: 12432 | val_loss: 0.2065
Training epoch 0, iteration 777/998 | lr: 1.291e-05 | global_batch_size: 16 | global_step: 777 | reduced_train_loss: 0.1941 | train_step_timing in s: 1.669 | consumed_samples: 12448 | val_loss: 0.2065
Training epoch 0, iteration 778/998 | lr: 1.279e-05 | global_batch_size: 16 | global_step: 778 | reduced_train_loss: 0.08866 | train_step_timing in s: 1.335 | consumed_samples: 12464 | val_loss: 0.2065
Training epoch 0, iteration 779/998 | lr: 1.268e-05 | global_batch_size: 16 | global_step: 779 | reduced_train_loss: 0.137 | train_step_timing in s: 1.219 | consumed_samples: 12480 | val_loss: 0.2065
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:49:41 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 780 : Start time: 1759801780.410s : Save duration: 1.068s
[NeMo I 2025-10-07 01:49:44 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=779-consumed_samples=12480.0-last.ckpt
Training epoch 0, iteration 780/998 | lr: 1.257e-05 | global_batch_size: 16 | global_step: 780 | reduced_train_loss: 0.1269 | train_step_timing in s: 1.289 | consumed_samples: 12496 | val_loss: 0.2039
[NeMo I 2025-10-07 01:49:46 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 781/998 | lr: 1.246e-05 | global_batch_size: 16 | global_step: 781 | reduced_train_loss: 0.1754 | train_step_timing in s: 1.684 | consumed_samples: 12512 | val_loss: 0.2039
[NeMo I 2025-10-07 01:49:50 nemo_logging:393] Successfully saved checkpoint from iteration     780 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=779-consumed_samples=12480.0-last.ckpt
[NeMo I 2025-10-07 01:49:50 nemo_logging:393] Async checkpoint save for step 780 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=779-consumed_samples=12480.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:49:51 nemo_logging:393] Async finalization time took 1.407 s
Training epoch 0, iteration 782/998 | lr: 1.236e-05 | global_batch_size: 16 | global_step: 782 | reduced_train_loss: 0.1522 | train_step_timing in s: 1.249 | consumed_samples: 12528 | val_loss: 0.2039
Training epoch 0, iteration 783/998 | lr: 1.225e-05 | global_batch_size: 16 | global_step: 783 | reduced_train_loss: 0.2634 | train_step_timing in s: 1.958 | consumed_samples: 12544 | val_loss: 0.2039
Training epoch 0, iteration 784/998 | lr: 1.214e-05 | global_batch_size: 16 | global_step: 784 | reduced_train_loss: 0.1237 | train_step_timing in s: 1.34 | consumed_samples: 12560 | val_loss: 0.2039
Training epoch 0, iteration 785/998 | lr: 1.203e-05 | global_batch_size: 16 | global_step: 785 | reduced_train_loss: 0.2282 | train_step_timing in s: 1.32 | consumed_samples: 12576 | val_loss: 0.2039
Training epoch 0, iteration 786/998 | lr: 1.192e-05 | global_batch_size: 16 | global_step: 786 | reduced_train_loss: 0.2428 | train_step_timing in s: 1.685 | consumed_samples: 12592 | val_loss: 0.2039
Training epoch 0, iteration 787/998 | lr: 1.182e-05 | global_batch_size: 16 | global_step: 787 | reduced_train_loss: 0.2269 | train_step_timing in s: 1.249 | consumed_samples: 12608 | val_loss: 0.2039
Training epoch 0, iteration 788/998 | lr: 1.171e-05 | global_batch_size: 16 | global_step: 788 | reduced_train_loss: 0.1433 | train_step_timing in s: 1.326 | consumed_samples: 12624 | val_loss: 0.2039
Training epoch 0, iteration 789/998 | lr: 1.16e-05 | global_batch_size: 16 | global_step: 789 | reduced_train_loss: 0.1813 | train_step_timing in s: 2.153 | consumed_samples: 12640 | val_loss: 0.2039
Training epoch 0, iteration 790/998 | lr: 1.15e-05 | global_batch_size: 16 | global_step: 790 | reduced_train_loss: 0.193 | train_step_timing in s: 1.638 | consumed_samples: 12656 | val_loss: 0.2039
Training epoch 0, iteration 791/998 | lr: 1.139e-05 | global_batch_size: 16 | global_step: 791 | reduced_train_loss: 0.1224 | train_step_timing in s: 1.26 | consumed_samples: 12672 | val_loss: 0.2039
Training epoch 0, iteration 792/998 | lr: 1.129e-05 | global_batch_size: 16 | global_step: 792 | reduced_train_loss: 0.1245 | train_step_timing in s: 1.293 | consumed_samples: 12688 | val_loss: 0.2039
Training epoch 0, iteration 793/998 | lr: 1.118e-05 | global_batch_size: 16 | global_step: 793 | reduced_train_loss: 0.3391 | train_step_timing in s: 1.258 | consumed_samples: 12704 | val_loss: 0.2039
Training epoch 0, iteration 794/998 | lr: 1.108e-05 | global_batch_size: 16 | global_step: 794 | reduced_train_loss: 0.1291 | train_step_timing in s: 1.215 | consumed_samples: 12720 | val_loss: 0.2039
Training epoch 0, iteration 795/998 | lr: 1.097e-05 | global_batch_size: 16 | global_step: 795 | reduced_train_loss: 0.2071 | train_step_timing in s: 1.237 | consumed_samples: 12736 | val_loss: 0.2039
Training epoch 0, iteration 796/998 | lr: 1.087e-05 | global_batch_size: 16 | global_step: 796 | reduced_train_loss: 0.1376 | train_step_timing in s: 1.268 | consumed_samples: 12752 | val_loss: 0.2039
Training epoch 0, iteration 797/998 | lr: 1.077e-05 | global_batch_size: 16 | global_step: 797 | reduced_train_loss: 0.2831 | train_step_timing in s: 1.212 | consumed_samples: 12768 | val_loss: 0.2039
Training epoch 0, iteration 798/998 | lr: 1.067e-05 | global_batch_size: 16 | global_step: 798 | reduced_train_loss: 0.2458 | train_step_timing in s: 1.282 | consumed_samples: 12784 | val_loss: 0.2039
Training epoch 0, iteration 799/998 | lr: 1.056e-05 | global_batch_size: 16 | global_step: 799 | reduced_train_loss: 0.2401 | train_step_timing in s: 1.394 | consumed_samples: 12800 | val_loss: 0.2039
Epoch 0, global step 799: 'val_loss' reached 0.20391 (best 0.20391), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=799-consumed_samples=12800.0.ckpt' as top 2
[NeMo I 2025-10-07 01:51:15 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 799 : Start time: 1759801874.598s : Save duration: 1.038s
[NeMo I 2025-10-07 01:51:18 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=799-consumed_samples=12800.0.ckpt
[NeMo I 2025-10-07 01:51:19 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 800/998 | lr: 1.046e-05 | global_batch_size: 16 | global_step: 800 | reduced_train_loss: 0.3793 | train_step_timing in s: 1.33 | consumed_samples: 12816 | val_loss: 0.2039
[NeMo I 2025-10-07 01:51:23 nemo_logging:393] Successfully saved checkpoint from iteration     799 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=799-consumed_samples=12800.0.ckpt
[NeMo I 2025-10-07 01:51:23 nemo_logging:393] Async checkpoint save for step 800 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=799-consumed_samples=12800.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:51:30 nemo_logging:393] Async finalization time took 7.831 s
Training epoch 0, iteration 801/998 | lr: 1.036e-05 | global_batch_size: 16 | global_step: 801 | reduced_train_loss: 0.1589 | train_step_timing in s: 1.234 | consumed_samples: 12832 | val_loss: 0.2039
Training epoch 0, iteration 802/998 | lr: 1.026e-05 | global_batch_size: 16 | global_step: 802 | reduced_train_loss: 0.2245 | train_step_timing in s: 1.172 | consumed_samples: 12848 | val_loss: 0.2039
Training epoch 0, iteration 803/998 | lr: 1.016e-05 | global_batch_size: 16 | global_step: 803 | reduced_train_loss: 0.193 | train_step_timing in s: 1.272 | consumed_samples: 12864 | val_loss: 0.2039
Training epoch 0, iteration 804/998 | lr: 1.006e-05 | global_batch_size: 16 | global_step: 804 | reduced_train_loss: 0.1749 | train_step_timing in s: 1.324 | consumed_samples: 12880 | val_loss: 0.2039
Training epoch 0, iteration 805/998 | lr: 9.962e-06 | global_batch_size: 16 | global_step: 805 | reduced_train_loss: 0.07181 | train_step_timing in s: 1.689 | consumed_samples: 12896 | val_loss: 0.2039
Training epoch 0, iteration 806/998 | lr: 9.863e-06 | global_batch_size: 16 | global_step: 806 | reduced_train_loss: 0.172 | train_step_timing in s: 1.323 | consumed_samples: 12912 | val_loss: 0.2039
Training epoch 0, iteration 807/998 | lr: 9.764e-06 | global_batch_size: 16 | global_step: 807 | reduced_train_loss: 0.1222 | train_step_timing in s: 1.254 | consumed_samples: 12928 | val_loss: 0.2039
Training epoch 0, iteration 808/998 | lr: 9.666e-06 | global_batch_size: 16 | global_step: 808 | reduced_train_loss: 0.3556 | train_step_timing in s: 1.778 | consumed_samples: 12944 | val_loss: 0.2039
Training epoch 0, iteration 809/998 | lr: 9.569e-06 | global_batch_size: 16 | global_step: 809 | reduced_train_loss: 0.2459 | train_step_timing in s: 1.171 | consumed_samples: 12960 | val_loss: 0.2039
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 01:56:36 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 810 : Start time: 1759802195.714s : Save duration: 0.568s
[NeMo I 2025-10-07 01:56:39 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=809-consumed_samples=12960.0-last.ckpt
Training epoch 0, iteration 810/998 | lr: 9.471e-06 | global_batch_size: 16 | global_step: 810 | reduced_train_loss: 0.2067 | train_step_timing in s: 1.405 | consumed_samples: 12976 | val_loss: 0.2042
[NeMo I 2025-10-07 01:56:41 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 811/998 | lr: 9.375e-06 | global_batch_size: 16 | global_step: 811 | reduced_train_loss: 0.3013 | train_step_timing in s: 1.233 | consumed_samples: 12992 | val_loss: 0.2042
[NeMo I 2025-10-07 01:56:45 nemo_logging:393] Successfully saved checkpoint from iteration     810 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=809-consumed_samples=12960.0-last.ckpt
[NeMo I 2025-10-07 01:56:45 nemo_logging:393] Async checkpoint save for step 810 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=809-consumed_samples=12960.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 01:56:45 nemo_logging:393] Async finalization time took 1.513 s
Training epoch 0, iteration 812/998 | lr: 9.278e-06 | global_batch_size: 16 | global_step: 812 | reduced_train_loss: 0.065 | train_step_timing in s: 1.666 | consumed_samples: 13008 | val_loss: 0.2042
Training epoch 0, iteration 813/998 | lr: 9.183e-06 | global_batch_size: 16 | global_step: 813 | reduced_train_loss: 0.1644 | train_step_timing in s: 1.277 | consumed_samples: 13024 | val_loss: 0.2042
Training epoch 0, iteration 814/998 | lr: 9.087e-06 | global_batch_size: 16 | global_step: 814 | reduced_train_loss: 0.1183 | train_step_timing in s: 2.403 | consumed_samples: 13040 | val_loss: 0.2042
Training epoch 0, iteration 815/998 | lr: 8.992e-06 | global_batch_size: 16 | global_step: 815 | reduced_train_loss: 0.3288 | train_step_timing in s: 1.383 | consumed_samples: 13056 | val_loss: 0.2042
Training epoch 0, iteration 816/998 | lr: 8.898e-06 | global_batch_size: 16 | global_step: 816 | reduced_train_loss: 0.1457 | train_step_timing in s: 1.272 | consumed_samples: 13072 | val_loss: 0.2042
Training epoch 0, iteration 817/998 | lr: 8.804e-06 | global_batch_size: 16 | global_step: 817 | reduced_train_loss: 0.2003 | train_step_timing in s: 1.678 | consumed_samples: 13088 | val_loss: 0.2042
Training epoch 0, iteration 818/998 | lr: 8.71e-06 | global_batch_size: 16 | global_step: 818 | reduced_train_loss: 0.3336 | train_step_timing in s: 1.332 | consumed_samples: 13104 | val_loss: 0.2042
Training epoch 0, iteration 819/998 | lr: 8.617e-06 | global_batch_size: 16 | global_step: 819 | reduced_train_loss: 0.1903 | train_step_timing in s: 1.271 | consumed_samples: 13120 | val_loss: 0.2042
Training epoch 0, iteration 820/998 | lr: 8.524e-06 | global_batch_size: 16 | global_step: 820 | reduced_train_loss: 0.0981 | train_step_timing in s: 1.207 | consumed_samples: 13136 | val_loss: 0.2042
Training epoch 0, iteration 821/998 | lr: 8.432e-06 | global_batch_size: 16 | global_step: 821 | reduced_train_loss: 0.09295 | train_step_timing in s: 1.283 | consumed_samples: 13152 | val_loss: 0.2042
Training epoch 0, iteration 822/998 | lr: 8.341e-06 | global_batch_size: 16 | global_step: 822 | reduced_train_loss: 0.1344 | train_step_timing in s: 1.286 | consumed_samples: 13168 | val_loss: 0.2042
Training epoch 0, iteration 823/998 | lr: 8.249e-06 | global_batch_size: 16 | global_step: 823 | reduced_train_loss: 0.2271 | train_step_timing in s: 1.292 | consumed_samples: 13184 | val_loss: 0.2042
Training epoch 0, iteration 824/998 | lr: 8.158e-06 | global_batch_size: 16 | global_step: 824 | reduced_train_loss: 0.2796 | train_step_timing in s: 1.266 | consumed_samples: 13200 | val_loss: 0.2042
Training epoch 0, iteration 825/998 | lr: 8.068e-06 | global_batch_size: 16 | global_step: 825 | reduced_train_loss: 0.1247 | train_step_timing in s: 1.219 | consumed_samples: 13216 | val_loss: 0.2042
Training epoch 0, iteration 826/998 | lr: 7.978e-06 | global_batch_size: 16 | global_step: 826 | reduced_train_loss: 0.1867 | train_step_timing in s: 1.208 | consumed_samples: 13232 | val_loss: 0.2042
Training epoch 0, iteration 827/998 | lr: 7.889e-06 | global_batch_size: 16 | global_step: 827 | reduced_train_loss: 0.4962 | train_step_timing in s: 1.344 | consumed_samples: 13248 | val_loss: 0.2042
Training epoch 0, iteration 828/998 | lr: 7.8e-06 | global_batch_size: 16 | global_step: 828 | reduced_train_loss: 0.1336 | train_step_timing in s: 1.284 | consumed_samples: 13264 | val_loss: 0.2042
Training epoch 0, iteration 829/998 | lr: 7.711e-06 | global_batch_size: 16 | global_step: 829 | reduced_train_loss: 0.1856 | train_step_timing in s: 1.739 | consumed_samples: 13280 | val_loss: 0.2042
Training epoch 0, iteration 830/998 | lr: 7.623e-06 | global_batch_size: 16 | global_step: 830 | reduced_train_loss: 0.1807 | train_step_timing in s: 1.272 | consumed_samples: 13296 | val_loss: 0.2042
Training epoch 0, iteration 831/998 | lr: 7.535e-06 | global_batch_size: 16 | global_step: 831 | reduced_train_loss: 0.2317 | train_step_timing in s: 1.324 | consumed_samples: 13312 | val_loss: 0.2042
Training epoch 0, iteration 832/998 | lr: 7.448e-06 | global_batch_size: 16 | global_step: 832 | reduced_train_loss: 0.1787 | train_step_timing in s: 2.064 | consumed_samples: 13328 | val_loss: 0.2042
Training epoch 0, iteration 833/998 | lr: 7.362e-06 | global_batch_size: 16 | global_step: 833 | reduced_train_loss: 0.2394 | train_step_timing in s: 1.272 | consumed_samples: 13344 | val_loss: 0.2042
Training epoch 0, iteration 834/998 | lr: 7.275e-06 | global_batch_size: 16 | global_step: 834 | reduced_train_loss: 0.2042 | train_step_timing in s: 1.751 | consumed_samples: 13360 | val_loss: 0.2042
Training epoch 0, iteration 835/998 | lr: 7.19e-06 | global_batch_size: 16 | global_step: 835 | reduced_train_loss: 0.3253 | train_step_timing in s: 1.754 | consumed_samples: 13376 | val_loss: 0.2042
Training epoch 0, iteration 836/998 | lr: 7.104e-06 | global_batch_size: 16 | global_step: 836 | reduced_train_loss: 0.248 | train_step_timing in s: 1.218 | consumed_samples: 13392 | val_loss: 0.2042
Training epoch 0, iteration 837/998 | lr: 7.019e-06 | global_batch_size: 16 | global_step: 837 | reduced_train_loss: 0.1976 | train_step_timing in s: 1.288 | consumed_samples: 13408 | val_loss: 0.2042
Training epoch 0, iteration 838/998 | lr: 6.935e-06 | global_batch_size: 16 | global_step: 838 | reduced_train_loss: 0.1955 | train_step_timing in s: 1.785 | consumed_samples: 13424 | val_loss: 0.2042
Training epoch 0, iteration 839/998 | lr: 6.851e-06 | global_batch_size: 16 | global_step: 839 | reduced_train_loss: 0.4728 | train_step_timing in s: 1.699 | consumed_samples: 13440 | val_loss: 0.2042
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 02:03:39 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 840 : Start time: 1759802617.167s : Save duration: 1.849s
[NeMo I 2025-10-07 02:03:44 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=839-consumed_samples=13440.0-last.ckpt
Training epoch 0, iteration 840/998 | lr: 6.768e-06 | global_batch_size: 16 | global_step: 840 | reduced_train_loss: 0.3154 | train_step_timing in s: 1.704 | consumed_samples: 13456 | val_loss: 0.2044
[NeMo I 2025-10-07 02:03:45 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 841/998 | lr: 6.685e-06 | global_batch_size: 16 | global_step: 841 | reduced_train_loss: 0.2379 | train_step_timing in s: 1.22 | consumed_samples: 13472 | val_loss: 0.2044
[NeMo I 2025-10-07 02:03:49 nemo_logging:393] Successfully saved checkpoint from iteration     840 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=839-consumed_samples=13440.0-last.ckpt
[NeMo I 2025-10-07 02:03:49 nemo_logging:393] Async checkpoint save for step 840 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=839-consumed_samples=13440.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:03:53 nemo_logging:393] Async finalization time took 4.992 s
Training epoch 0, iteration 842/998 | lr: 6.602e-06 | global_batch_size: 16 | global_step: 842 | reduced_train_loss: 0.2174 | train_step_timing in s: 1.372 | consumed_samples: 13488 | val_loss: 0.2044
Training epoch 0, iteration 843/998 | lr: 6.521e-06 | global_batch_size: 16 | global_step: 843 | reduced_train_loss: 0.1492 | train_step_timing in s: 1.25 | consumed_samples: 13504 | val_loss: 0.2044
Training epoch 0, iteration 844/998 | lr: 6.439e-06 | global_batch_size: 16 | global_step: 844 | reduced_train_loss: 0.4315 | train_step_timing in s: 1.26 | consumed_samples: 13520 | val_loss: 0.2044
Training epoch 0, iteration 845/998 | lr: 6.358e-06 | global_batch_size: 16 | global_step: 845 | reduced_train_loss: 0.1473 | train_step_timing in s: 1.373 | consumed_samples: 13536 | val_loss: 0.2044
Training epoch 0, iteration 846/998 | lr: 6.277e-06 | global_batch_size: 16 | global_step: 846 | reduced_train_loss: 0.1682 | train_step_timing in s: 1.242 | consumed_samples: 13552 | val_loss: 0.2044
Training epoch 0, iteration 847/998 | lr: 6.197e-06 | global_batch_size: 16 | global_step: 847 | reduced_train_loss: 0.1022 | train_step_timing in s: 1.288 | consumed_samples: 13568 | val_loss: 0.2044
Training epoch 0, iteration 848/998 | lr: 6.118e-06 | global_batch_size: 16 | global_step: 848 | reduced_train_loss: 0.07224 | train_step_timing in s: 2.199 | consumed_samples: 13584 | val_loss: 0.2044
Training epoch 0, iteration 849/998 | lr: 6.039e-06 | global_batch_size: 16 | global_step: 849 | reduced_train_loss: 0.1824 | train_step_timing in s: 1.247 | consumed_samples: 13600 | val_loss: 0.2044
Epoch 0, global step 849: 'val_loss' reached 0.20440 (best 0.20391), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=849-consumed_samples=13600.0.ckpt' as top 2
[NeMo I 2025-10-07 02:04:29 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 849 : Start time: 1759802669.096s : Save duration: 0.562s
[NeMo I 2025-10-07 02:04:32 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=849-consumed_samples=13600.0.ckpt
[NeMo I 2025-10-07 02:04:32 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 850/998 | lr: 5.96e-06 | global_batch_size: 16 | global_step: 850 | reduced_train_loss: 0.3793 | train_step_timing in s: 1.55 | consumed_samples: 13616 | val_loss: 0.2044
[NeMo I 2025-10-07 02:04:37 nemo_logging:393] Successfully saved checkpoint from iteration     849 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=849-consumed_samples=13600.0.ckpt
[NeMo I 2025-10-07 02:04:37 nemo_logging:393] Async checkpoint save for step 850 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=849-consumed_samples=13600.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:04:44 nemo_logging:393] Async finalization time took 7.809 s
Training epoch 0, iteration 851/998 | lr: 5.882e-06 | global_batch_size: 16 | global_step: 851 | reduced_train_loss: 0.1531 | train_step_timing in s: 1.338 | consumed_samples: 13632 | val_loss: 0.2044
Training epoch 0, iteration 852/998 | lr: 5.804e-06 | global_batch_size: 16 | global_step: 852 | reduced_train_loss: 0.1331 | train_step_timing in s: 1.348 | consumed_samples: 13648 | val_loss: 0.2044
Training epoch 0, iteration 853/998 | lr: 5.727e-06 | global_batch_size: 16 | global_step: 853 | reduced_train_loss: 0.2174 | train_step_timing in s: 1.393 | consumed_samples: 13664 | val_loss: 0.2044
Training epoch 0, iteration 854/998 | lr: 5.651e-06 | global_batch_size: 16 | global_step: 854 | reduced_train_loss: 0.1752 | train_step_timing in s: 1.205 | consumed_samples: 13680 | val_loss: 0.2044
Training epoch 0, iteration 855/998 | lr: 5.574e-06 | global_batch_size: 16 | global_step: 855 | reduced_train_loss: 0.1378 | train_step_timing in s: 1.84 | consumed_samples: 13696 | val_loss: 0.2044
Training epoch 0, iteration 856/998 | lr: 5.499e-06 | global_batch_size: 16 | global_step: 856 | reduced_train_loss: 0.08305 | train_step_timing in s: 1.272 | consumed_samples: 13712 | val_loss: 0.2044
Training epoch 0, iteration 857/998 | lr: 5.423e-06 | global_batch_size: 16 | global_step: 857 | reduced_train_loss: 0.1151 | train_step_timing in s: 1.301 | consumed_samples: 13728 | val_loss: 0.2044
Training epoch 0, iteration 858/998 | lr: 5.349e-06 | global_batch_size: 16 | global_step: 858 | reduced_train_loss: 0.103 | train_step_timing in s: 1.786 | consumed_samples: 13744 | val_loss: 0.2044
Training epoch 0, iteration 859/998 | lr: 5.274e-06 | global_batch_size: 16 | global_step: 859 | reduced_train_loss: 0.1496 | train_step_timing in s: 1.251 | consumed_samples: 13760 | val_loss: 0.2044
Training epoch 0, iteration 860/998 | lr: 5.201e-06 | global_batch_size: 16 | global_step: 860 | reduced_train_loss: 0.1825 | train_step_timing in s: 1.246 | consumed_samples: 13776 | val_loss: 0.2044
Training epoch 0, iteration 861/998 | lr: 5.127e-06 | global_batch_size: 16 | global_step: 861 | reduced_train_loss: 0.1528 | train_step_timing in s: 1.251 | consumed_samples: 13792 | val_loss: 0.2044
Training epoch 0, iteration 862/998 | lr: 5.055e-06 | global_batch_size: 16 | global_step: 862 | reduced_train_loss: 0.1143 | train_step_timing in s: 1.281 | consumed_samples: 13808 | val_loss: 0.2044
Training epoch 0, iteration 863/998 | lr: 4.982e-06 | global_batch_size: 16 | global_step: 863 | reduced_train_loss: 0.1229 | train_step_timing in s: 1.288 | consumed_samples: 13824 | val_loss: 0.2044
Training epoch 0, iteration 864/998 | lr: 4.911e-06 | global_batch_size: 16 | global_step: 864 | reduced_train_loss: 0.23 | train_step_timing in s: 2.17 | consumed_samples: 13840 | val_loss: 0.2044
Training epoch 0, iteration 865/998 | lr: 4.839e-06 | global_batch_size: 16 | global_step: 865 | reduced_train_loss: 0.1732 | train_step_timing in s: 2.064 | consumed_samples: 13856 | val_loss: 0.2044
Training epoch 0, iteration 866/998 | lr: 4.769e-06 | global_batch_size: 16 | global_step: 866 | reduced_train_loss: 0.1057 | train_step_timing in s: 1.266 | consumed_samples: 13872 | val_loss: 0.2044
Training epoch 0, iteration 867/998 | lr: 4.698e-06 | global_batch_size: 16 | global_step: 867 | reduced_train_loss: 0.1352 | train_step_timing in s: 1.734 | consumed_samples: 13888 | val_loss: 0.2044
Training epoch 0, iteration 868/998 | lr: 4.628e-06 | global_batch_size: 16 | global_step: 868 | reduced_train_loss: 0.2135 | train_step_timing in s: 1.221 | consumed_samples: 13904 | val_loss: 0.2044
Training epoch 0, iteration 869/998 | lr: 4.559e-06 | global_batch_size: 16 | global_step: 869 | reduced_train_loss: 0.1088 | train_step_timing in s: 1.275 | consumed_samples: 13920 | val_loss: 0.2044
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 02:10:44 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 870 : Start time: 1759803043.501s : Save duration: 0.562s
[NeMo I 2025-10-07 02:10:47 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=869-consumed_samples=13920.0-last.ckpt
Training epoch 0, iteration 870/998 | lr: 4.49e-06 | global_batch_size: 16 | global_step: 870 | reduced_train_loss: 0.2204 | train_step_timing in s: 1.268 | consumed_samples: 13936 | val_loss: 0.2029
[NeMo I 2025-10-07 02:10:48 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 871/998 | lr: 4.422e-06 | global_batch_size: 16 | global_step: 871 | reduced_train_loss: 0.2271 | train_step_timing in s: 1.556 | consumed_samples: 13952 | val_loss: 0.2029
[NeMo I 2025-10-07 02:10:52 nemo_logging:393] Successfully saved checkpoint from iteration     870 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=869-consumed_samples=13920.0-last.ckpt
[NeMo I 2025-10-07 02:10:53 nemo_logging:393] Async checkpoint save for step 870 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=869-consumed_samples=13920.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:10:53 nemo_logging:393] Async finalization time took 1.732 s
Training epoch 0, iteration 872/998 | lr: 4.354e-06 | global_batch_size: 16 | global_step: 872 | reduced_train_loss: 0.145 | train_step_timing in s: 1.225 | consumed_samples: 13968 | val_loss: 0.2029
Training epoch 0, iteration 873/998 | lr: 4.287e-06 | global_batch_size: 16 | global_step: 873 | reduced_train_loss: 0.177 | train_step_timing in s: 1.252 | consumed_samples: 13984 | val_loss: 0.2029
Training epoch 0, iteration 874/998 | lr: 4.22e-06 | global_batch_size: 16 | global_step: 874 | reduced_train_loss: 0.2062 | train_step_timing in s: 1.151 | consumed_samples: 14000 | val_loss: 0.2029
Training epoch 0, iteration 875/998 | lr: 4.154e-06 | global_batch_size: 16 | global_step: 875 | reduced_train_loss: 0.1775 | train_step_timing in s: 1.74 | consumed_samples: 14016 | val_loss: 0.2029
Training epoch 0, iteration 876/998 | lr: 4.088e-06 | global_batch_size: 16 | global_step: 876 | reduced_train_loss: 0.08901 | train_step_timing in s: 1.528 | consumed_samples: 14032 | val_loss: 0.2029
Training epoch 0, iteration 877/998 | lr: 4.023e-06 | global_batch_size: 16 | global_step: 877 | reduced_train_loss: 0.2596 | train_step_timing in s: 1.248 | consumed_samples: 14048 | val_loss: 0.2029
Training epoch 0, iteration 878/998 | lr: 3.958e-06 | global_batch_size: 16 | global_step: 878 | reduced_train_loss: 0.2435 | train_step_timing in s: 1.324 | consumed_samples: 14064 | val_loss: 0.2029
Training epoch 0, iteration 879/998 | lr: 3.894e-06 | global_batch_size: 16 | global_step: 879 | reduced_train_loss: 0.1808 | train_step_timing in s: 1.368 | consumed_samples: 14080 | val_loss: 0.2029
Training epoch 0, iteration 880/998 | lr: 3.83e-06 | global_batch_size: 16 | global_step: 880 | reduced_train_loss: 0.2347 | train_step_timing in s: 1.739 | consumed_samples: 14096 | val_loss: 0.2029
Training epoch 0, iteration 881/998 | lr: 3.767e-06 | global_batch_size: 16 | global_step: 881 | reduced_train_loss: 0.08851 | train_step_timing in s: 1.533 | consumed_samples: 14112 | val_loss: 0.2029
Training epoch 0, iteration 882/998 | lr: 3.704e-06 | global_batch_size: 16 | global_step: 882 | reduced_train_loss: 0.119 | train_step_timing in s: 1.345 | consumed_samples: 14128 | val_loss: 0.2029
Training epoch 0, iteration 883/998 | lr: 3.641e-06 | global_batch_size: 16 | global_step: 883 | reduced_train_loss: 0.08972 | train_step_timing in s: 1.328 | consumed_samples: 14144 | val_loss: 0.2029
Training epoch 0, iteration 884/998 | lr: 3.58e-06 | global_batch_size: 16 | global_step: 884 | reduced_train_loss: 0.1707 | train_step_timing in s: 1.531 | consumed_samples: 14160 | val_loss: 0.2029
Training epoch 0, iteration 885/998 | lr: 3.518e-06 | global_batch_size: 16 | global_step: 885 | reduced_train_loss: 0.1408 | train_step_timing in s: 1.534 | consumed_samples: 14176 | val_loss: 0.2029
Training epoch 0, iteration 886/998 | lr: 3.458e-06 | global_batch_size: 16 | global_step: 886 | reduced_train_loss: 0.1226 | train_step_timing in s: 1.695 | consumed_samples: 14192 | val_loss: 0.2029
Training epoch 0, iteration 887/998 | lr: 3.398e-06 | global_batch_size: 16 | global_step: 887 | reduced_train_loss: 0.1536 | train_step_timing in s: 1.265 | consumed_samples: 14208 | val_loss: 0.2029
Training epoch 0, iteration 888/998 | lr: 3.338e-06 | global_batch_size: 16 | global_step: 888 | reduced_train_loss: 0.09931 | train_step_timing in s: 1.33 | consumed_samples: 14224 | val_loss: 0.2029
Training epoch 0, iteration 889/998 | lr: 3.279e-06 | global_batch_size: 16 | global_step: 889 | reduced_train_loss: 0.1877 | train_step_timing in s: 1.282 | consumed_samples: 14240 | val_loss: 0.2029
Training epoch 0, iteration 890/998 | lr: 3.22e-06 | global_batch_size: 16 | global_step: 890 | reduced_train_loss: 0.1299 | train_step_timing in s: 1.391 | consumed_samples: 14256 | val_loss: 0.2029
Training epoch 0, iteration 891/998 | lr: 3.162e-06 | global_batch_size: 16 | global_step: 891 | reduced_train_loss: 0.2934 | train_step_timing in s: 1.325 | consumed_samples: 14272 | val_loss: 0.2029
Training epoch 0, iteration 892/998 | lr: 3.104e-06 | global_batch_size: 16 | global_step: 892 | reduced_train_loss: 0.1359 | train_step_timing in s: 1.39 | consumed_samples: 14288 | val_loss: 0.2029
Training epoch 0, iteration 893/998 | lr: 3.047e-06 | global_batch_size: 16 | global_step: 893 | reduced_train_loss: 0.1612 | train_step_timing in s: 1.706 | consumed_samples: 14304 | val_loss: 0.2029
Training epoch 0, iteration 894/998 | lr: 2.99e-06 | global_batch_size: 16 | global_step: 894 | reduced_train_loss: 0.2096 | train_step_timing in s: 1.211 | consumed_samples: 14320 | val_loss: 0.2029
Training epoch 0, iteration 895/998 | lr: 2.934e-06 | global_batch_size: 16 | global_step: 895 | reduced_train_loss: 0.1452 | train_step_timing in s: 1.251 | consumed_samples: 14336 | val_loss: 0.2029
Training epoch 0, iteration 896/998 | lr: 2.879e-06 | global_batch_size: 16 | global_step: 896 | reduced_train_loss: 0.1216 | train_step_timing in s: 1.207 | consumed_samples: 14352 | val_loss: 0.2029
Training epoch 0, iteration 897/998 | lr: 2.823e-06 | global_batch_size: 16 | global_step: 897 | reduced_train_loss: 0.1024 | train_step_timing in s: 1.343 | consumed_samples: 14368 | val_loss: 0.2029
Training epoch 0, iteration 898/998 | lr: 2.769e-06 | global_batch_size: 16 | global_step: 898 | reduced_train_loss: 0.1739 | train_step_timing in s: 1.709 | consumed_samples: 14384 | val_loss: 0.2029
Training epoch 0, iteration 899/998 | lr: 2.715e-06 | global_batch_size: 16 | global_step: 899 | reduced_train_loss: 0.1754 | train_step_timing in s: 1.216 | consumed_samples: 14400 | val_loss: 0.2029
Epoch 0, global step 899: 'val_loss' reached 0.20289 (best 0.20289), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=899-consumed_samples=14400.0.ckpt' as top 2
[NeMo I 2025-10-07 02:13:13 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 899 : Start time: 1759803193.268s : Save duration: 0.556s
[NeMo I 2025-10-07 02:13:17 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=899-consumed_samples=14400.0.ckpt
[NeMo I 2025-10-07 02:13:17 nemo_logging:393] Async finalization time took 0.000 s
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
Training epoch 0, iteration 900/998 | lr: 2.661e-06 | global_batch_size: 16 | global_step: 900 | reduced_train_loss: 0.06851 | train_step_timing in s: 1.319 | consumed_samples: 14416 | val_loss: 0.2032
[NeMo I 2025-10-07 02:17:44 nemo_logging:393] Successfully saved checkpoint from iteration     899 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=899-consumed_samples=14400.0.ckpt
[NeMo I 2025-10-07 02:17:44 nemo_logging:393] Async checkpoint save for step 900 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=899-consumed_samples=14400.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:17:51 nemo_logging:393] Async finalization time took 8.157 s
Training epoch 0, iteration 901/998 | lr: 2.608e-06 | global_batch_size: 16 | global_step: 901 | reduced_train_loss: 0.1329 | train_step_timing in s: 1.326 | consumed_samples: 14432 | val_loss: 0.2032
Training epoch 0, iteration 902/998 | lr: 2.556e-06 | global_batch_size: 16 | global_step: 902 | reduced_train_loss: 0.1237 | train_step_timing in s: 1.557 | consumed_samples: 14448 | val_loss: 0.2032
Training epoch 0, iteration 903/998 | lr: 2.504e-06 | global_batch_size: 16 | global_step: 903 | reduced_train_loss: 0.08551 | train_step_timing in s: 1.248 | consumed_samples: 14464 | val_loss: 0.2032
Training epoch 0, iteration 904/998 | lr: 2.452e-06 | global_batch_size: 16 | global_step: 904 | reduced_train_loss: 0.2796 | train_step_timing in s: 1.164 | consumed_samples: 14480 | val_loss: 0.2032
Training epoch 0, iteration 905/998 | lr: 2.401e-06 | global_batch_size: 16 | global_step: 905 | reduced_train_loss: 0.245 | train_step_timing in s: 1.743 | consumed_samples: 14496 | val_loss: 0.2032
Training epoch 0, iteration 906/998 | lr: 2.351e-06 | global_batch_size: 16 | global_step: 906 | reduced_train_loss: 0.1253 | train_step_timing in s: 1.152 | consumed_samples: 14512 | val_loss: 0.2032
Training epoch 0, iteration 907/998 | lr: 2.301e-06 | global_batch_size: 16 | global_step: 907 | reduced_train_loss: 0.2421 | train_step_timing in s: 1.534 | consumed_samples: 14528 | val_loss: 0.2032
Training epoch 0, iteration 908/998 | lr: 2.252e-06 | global_batch_size: 16 | global_step: 908 | reduced_train_loss: 0.1432 | train_step_timing in s: 1.529 | consumed_samples: 14544 | val_loss: 0.2032
Training epoch 0, iteration 909/998 | lr: 2.203e-06 | global_batch_size: 16 | global_step: 909 | reduced_train_loss: 0.2439 | train_step_timing in s: 1.246 | consumed_samples: 14560 | val_loss: 0.2032
Training epoch 0, iteration 910/998 | lr: 2.154e-06 | global_batch_size: 16 | global_step: 910 | reduced_train_loss: 0.1597 | train_step_timing in s: 1.227 | consumed_samples: 14576 | val_loss: 0.2032
Training epoch 0, iteration 911/998 | lr: 2.107e-06 | global_batch_size: 16 | global_step: 911 | reduced_train_loss: 0.1117 | train_step_timing in s: 1.74 | consumed_samples: 14592 | val_loss: 0.2032
Training epoch 0, iteration 912/998 | lr: 2.059e-06 | global_batch_size: 16 | global_step: 912 | reduced_train_loss: 0.1824 | train_step_timing in s: 1.245 | consumed_samples: 14608 | val_loss: 0.2032
Training epoch 0, iteration 913/998 | lr: 2.013e-06 | global_batch_size: 16 | global_step: 913 | reduced_train_loss: 0.3073 | train_step_timing in s: 1.263 | consumed_samples: 14624 | val_loss: 0.2032
Training epoch 0, iteration 914/998 | lr: 1.966e-06 | global_batch_size: 16 | global_step: 914 | reduced_train_loss: 0.1805 | train_step_timing in s: 1.16 | consumed_samples: 14640 | val_loss: 0.2032
Training epoch 0, iteration 915/998 | lr: 1.921e-06 | global_batch_size: 16 | global_step: 915 | reduced_train_loss: 0.259 | train_step_timing in s: 2.381 | consumed_samples: 14656 | val_loss: 0.2032
Training epoch 0, iteration 916/998 | lr: 1.876e-06 | global_batch_size: 16 | global_step: 916 | reduced_train_loss: 0.2097 | train_step_timing in s: 1.306 | consumed_samples: 14672 | val_loss: 0.2032
Training epoch 0, iteration 917/998 | lr: 1.831e-06 | global_batch_size: 16 | global_step: 917 | reduced_train_loss: 0.2295 | train_step_timing in s: 3.05 | consumed_samples: 14688 | val_loss: 0.2032
Training epoch 0, iteration 918/998 | lr: 1.787e-06 | global_batch_size: 16 | global_step: 918 | reduced_train_loss: 0.1382 | train_step_timing in s: 1.385 | consumed_samples: 14704 | val_loss: 0.2032
Training epoch 0, iteration 919/998 | lr: 1.743e-06 | global_batch_size: 16 | global_step: 919 | reduced_train_loss: 0.1411 | train_step_timing in s: 1.255 | consumed_samples: 14720 | val_loss: 0.2032
Training epoch 0, iteration 920/998 | lr: 1.7e-06 | global_batch_size: 16 | global_step: 920 | reduced_train_loss: 0.1806 | train_step_timing in s: 1.235 | consumed_samples: 14736 | val_loss: 0.2032
Training epoch 0, iteration 921/998 | lr: 1.658e-06 | global_batch_size: 16 | global_step: 921 | reduced_train_loss: 0.2205 | train_step_timing in s: 1.154 | consumed_samples: 14752 | val_loss: 0.2032
Training epoch 0, iteration 922/998 | lr: 1.616e-06 | global_batch_size: 16 | global_step: 922 | reduced_train_loss: 0.2414 | train_step_timing in s: 1.33 | consumed_samples: 14768 | val_loss: 0.2032
Training epoch 0, iteration 923/998 | lr: 1.574e-06 | global_batch_size: 16 | global_step: 923 | reduced_train_loss: 0.3084 | train_step_timing in s: 1.265 | consumed_samples: 14784 | val_loss: 0.2032
Training epoch 0, iteration 924/998 | lr: 1.533e-06 | global_batch_size: 16 | global_step: 924 | reduced_train_loss: 0.207 | train_step_timing in s: 1.259 | consumed_samples: 14800 | val_loss: 0.2032
Training epoch 0, iteration 925/998 | lr: 1.493e-06 | global_batch_size: 16 | global_step: 925 | reduced_train_loss: 0.3415 | train_step_timing in s: 1.372 | consumed_samples: 14816 | val_loss: 0.2032
Training epoch 0, iteration 926/998 | lr: 1.453e-06 | global_batch_size: 16 | global_step: 926 | reduced_train_loss: 0.3625 | train_step_timing in s: 1.173 | consumed_samples: 14832 | val_loss: 0.2032
Training epoch 0, iteration 927/998 | lr: 1.414e-06 | global_batch_size: 16 | global_step: 927 | reduced_train_loss: 0.1943 | train_step_timing in s: 1.309 | consumed_samples: 14848 | val_loss: 0.2032
Training epoch 0, iteration 928/998 | lr: 1.375e-06 | global_batch_size: 16 | global_step: 928 | reduced_train_loss: 0.3604 | train_step_timing in s: 1.379 | consumed_samples: 14864 | val_loss: 0.2032
Training epoch 0, iteration 929/998 | lr: 1.336e-06 | global_batch_size: 16 | global_step: 929 | reduced_train_loss: 0.0661 | train_step_timing in s: 1.844 | consumed_samples: 14880 | val_loss: 0.2032
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 02:24:25 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 930 : Start time: 1759803865.178s : Save duration: 0.583s
[NeMo I 2025-10-07 02:24:29 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=929-consumed_samples=14880.0-last.ckpt
Training epoch 0, iteration 930/998 | lr: 1.299e-06 | global_batch_size: 16 | global_step: 930 | reduced_train_loss: 0.1716 | train_step_timing in s: 1.396 | consumed_samples: 14896 | val_loss: 0.2038
[NeMo I 2025-10-07 02:24:30 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 931/998 | lr: 1.262e-06 | global_batch_size: 16 | global_step: 931 | reduced_train_loss: 0.2858 | train_step_timing in s: 1.388 | consumed_samples: 14912 | val_loss: 0.2038
[NeMo I 2025-10-07 02:24:34 nemo_logging:393] Successfully saved checkpoint from iteration     930 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=929-consumed_samples=14880.0-last.ckpt
[NeMo I 2025-10-07 02:24:34 nemo_logging:393] Async checkpoint save for step 930 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=929-consumed_samples=14880.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:24:35 nemo_logging:393] Async finalization time took 1.533 s
Training epoch 0, iteration 932/998 | lr: 1.225e-06 | global_batch_size: 16 | global_step: 932 | reduced_train_loss: 0.1569 | train_step_timing in s: 1.207 | consumed_samples: 14928 | val_loss: 0.2038
Training epoch 0, iteration 933/998 | lr: 1.189e-06 | global_batch_size: 16 | global_step: 933 | reduced_train_loss: 0.1006 | train_step_timing in s: 1.335 | consumed_samples: 14944 | val_loss: 0.2038
Training epoch 0, iteration 934/998 | lr: 1.153e-06 | global_batch_size: 16 | global_step: 934 | reduced_train_loss: 0.2503 | train_step_timing in s: 1.362 | consumed_samples: 14960 | val_loss: 0.2038
Training epoch 0, iteration 935/998 | lr: 1.118e-06 | global_batch_size: 16 | global_step: 935 | reduced_train_loss: 0.178 | train_step_timing in s: 1.382 | consumed_samples: 14976 | val_loss: 0.2038
Training epoch 0, iteration 936/998 | lr: 1.083e-06 | global_batch_size: 16 | global_step: 936 | reduced_train_loss: 0.2164 | train_step_timing in s: 1.737 | consumed_samples: 14992 | val_loss: 0.2038
Training epoch 0, iteration 937/998 | lr: 1.049e-06 | global_batch_size: 16 | global_step: 937 | reduced_train_loss: 0.247 | train_step_timing in s: 1.734 | consumed_samples: 15008 | val_loss: 0.2038
Training epoch 0, iteration 938/998 | lr: 1.016e-06 | global_batch_size: 16 | global_step: 938 | reduced_train_loss: 0.2607 | train_step_timing in s: 1.326 | consumed_samples: 15024 | val_loss: 0.2038
Training epoch 0, iteration 939/998 | lr: 9.831e-07 | global_batch_size: 16 | global_step: 939 | reduced_train_loss: 0.178 | train_step_timing in s: 1.561 | consumed_samples: 15040 | val_loss: 0.2038
Training epoch 0, iteration 940/998 | lr: 9.507e-07 | global_batch_size: 16 | global_step: 940 | reduced_train_loss: 0.2243 | train_step_timing in s: 1.204 | consumed_samples: 15056 | val_loss: 0.2038
Training epoch 0, iteration 941/998 | lr: 9.188e-07 | global_batch_size: 16 | global_step: 941 | reduced_train_loss: 0.2479 | train_step_timing in s: 1.318 | consumed_samples: 15072 | val_loss: 0.2038
Training epoch 0, iteration 942/998 | lr: 8.875e-07 | global_batch_size: 16 | global_step: 942 | reduced_train_loss: 0.07736 | train_step_timing in s: 1.238 | consumed_samples: 15088 | val_loss: 0.2038
Training epoch 0, iteration 943/998 | lr: 8.567e-07 | global_batch_size: 16 | global_step: 943 | reduced_train_loss: 0.316 | train_step_timing in s: 1.28 | consumed_samples: 15104 | val_loss: 0.2038
Training epoch 0, iteration 944/998 | lr: 8.265e-07 | global_batch_size: 16 | global_step: 944 | reduced_train_loss: 0.2983 | train_step_timing in s: 1.339 | consumed_samples: 15120 | val_loss: 0.2038
Training epoch 0, iteration 945/998 | lr: 7.968e-07 | global_batch_size: 16 | global_step: 945 | reduced_train_loss: 0.1222 | train_step_timing in s: 1.742 | consumed_samples: 15136 | val_loss: 0.2038
Training epoch 0, iteration 946/998 | lr: 7.676e-07 | global_batch_size: 16 | global_step: 946 | reduced_train_loss: 0.2925 | train_step_timing in s: 1.238 | consumed_samples: 15152 | val_loss: 0.2038
Training epoch 0, iteration 947/998 | lr: 7.39e-07 | global_batch_size: 16 | global_step: 947 | reduced_train_loss: 0.3858 | train_step_timing in s: 1.212 | consumed_samples: 15168 | val_loss: 0.2038
Training epoch 0, iteration 948/998 | lr: 7.109e-07 | global_batch_size: 16 | global_step: 948 | reduced_train_loss: 0.116 | train_step_timing in s: 1.812 | consumed_samples: 15184 | val_loss: 0.2038
Training epoch 0, iteration 949/998 | lr: 6.834e-07 | global_batch_size: 16 | global_step: 949 | reduced_train_loss: 0.3188 | train_step_timing in s: 1.325 | consumed_samples: 15200 | val_loss: 0.2038
Epoch 0, global step 949: 'val_loss' reached 0.20380 (best 0.20289), saving model to '/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=949-consumed_samples=15200.0.ckpt' as top 2
[NeMo I 2025-10-07 02:25:54 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 949 : Start time: 1759803953.857s : Save duration: 0.551s
[NeMo I 2025-10-07 02:25:57 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=949-consumed_samples=15200.0.ckpt
[NeMo I 2025-10-07 02:25:57 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 950/998 | lr: 6.564e-07 | global_batch_size: 16 | global_step: 950 | reduced_train_loss: 0.166 | train_step_timing in s: 1.237 | consumed_samples: 15216 | val_loss: 0.2038
[NeMo I 2025-10-07 02:26:01 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 951/998 | lr: 6.299e-07 | global_batch_size: 16 | global_step: 951 | reduced_train_loss: 0.2593 | train_step_timing in s: 1.249 | consumed_samples: 15232 | val_loss: 0.2038
[NeMo I 2025-10-07 02:26:05 nemo_logging:393] Successfully saved checkpoint from iteration     949 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=949-consumed_samples=15200.0.ckpt
[NeMo I 2025-10-07 02:26:05 nemo_logging:393] Async checkpoint save for step 950 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=949-consumed_samples=15200.0.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:26:13 nemo_logging:393] Async finalization time took 9.069 s
Training epoch 0, iteration 952/998 | lr: 6.04e-07 | global_batch_size: 16 | global_step: 952 | reduced_train_loss: 0.07948 | train_step_timing in s: 2.0 | consumed_samples: 15248 | val_loss: 0.2038
Training epoch 0, iteration 953/998 | lr: 5.786e-07 | global_batch_size: 16 | global_step: 953 | reduced_train_loss: 0.1668 | train_step_timing in s: 1.216 | consumed_samples: 15264 | val_loss: 0.2038
Training epoch 0, iteration 954/998 | lr: 5.538e-07 | global_batch_size: 16 | global_step: 954 | reduced_train_loss: 0.1162 | train_step_timing in s: 1.405 | consumed_samples: 15280 | val_loss: 0.2038
Training epoch 0, iteration 955/998 | lr: 5.295e-07 | global_batch_size: 16 | global_step: 955 | reduced_train_loss: 0.2014 | train_step_timing in s: 1.26 | consumed_samples: 15296 | val_loss: 0.2038
Training epoch 0, iteration 956/998 | lr: 5.057e-07 | global_batch_size: 16 | global_step: 956 | reduced_train_loss: 0.1557 | train_step_timing in s: 1.703 | consumed_samples: 15312 | val_loss: 0.2038
Training epoch 0, iteration 957/998 | lr: 4.825e-07 | global_batch_size: 16 | global_step: 957 | reduced_train_loss: 0.2028 | train_step_timing in s: 1.261 | consumed_samples: 15328 | val_loss: 0.2038
Training epoch 0, iteration 958/998 | lr: 4.598e-07 | global_batch_size: 16 | global_step: 958 | reduced_train_loss: 0.2119 | train_step_timing in s: 1.281 | consumed_samples: 15344 | val_loss: 0.2038
Training epoch 0, iteration 959/998 | lr: 4.377e-07 | global_batch_size: 16 | global_step: 959 | reduced_train_loss: 0.3546 | train_step_timing in s: 1.274 | consumed_samples: 15360 | val_loss: 0.2038
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 02:31:13 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 960 : Start time: 1759804272.541s : Save duration: 0.576s
[NeMo I 2025-10-07 02:31:16 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=959-consumed_samples=15360.0-last.ckpt
Training epoch 0, iteration 960/998 | lr: 4.161e-07 | global_batch_size: 16 | global_step: 960 | reduced_train_loss: 0.09752 | train_step_timing in s: 1.447 | consumed_samples: 15376 | val_loss: 0.2036
[NeMo I 2025-10-07 02:31:17 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 961/998 | lr: 3.951e-07 | global_batch_size: 16 | global_step: 961 | reduced_train_loss: 0.07791 | train_step_timing in s: 1.226 | consumed_samples: 15392 | val_loss: 0.2036
[NeMo I 2025-10-07 02:31:22 nemo_logging:393] Successfully saved checkpoint from iteration     960 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=959-consumed_samples=15360.0-last.ckpt
[NeMo I 2025-10-07 02:31:22 nemo_logging:393] Async checkpoint save for step 960 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=959-consumed_samples=15360.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:31:22 nemo_logging:393] Async finalization time took 1.515 s
Training epoch 0, iteration 962/998 | lr: 3.746e-07 | global_batch_size: 16 | global_step: 962 | reduced_train_loss: 0.206 | train_step_timing in s: 1.288 | consumed_samples: 15408 | val_loss: 0.2036
Training epoch 0, iteration 963/998 | lr: 3.546e-07 | global_batch_size: 16 | global_step: 963 | reduced_train_loss: 0.1685 | train_step_timing in s: 1.838 | consumed_samples: 15424 | val_loss: 0.2036
Training epoch 0, iteration 964/998 | lr: 3.352e-07 | global_batch_size: 16 | global_step: 964 | reduced_train_loss: 0.09806 | train_step_timing in s: 1.368 | consumed_samples: 15440 | val_loss: 0.2036
Training epoch 0, iteration 965/998 | lr: 3.164e-07 | global_batch_size: 16 | global_step: 965 | reduced_train_loss: 0.2444 | train_step_timing in s: 1.382 | consumed_samples: 15456 | val_loss: 0.2036
Training epoch 0, iteration 966/998 | lr: 2.981e-07 | global_batch_size: 16 | global_step: 966 | reduced_train_loss: 0.1634 | train_step_timing in s: 1.289 | consumed_samples: 15472 | val_loss: 0.2036
Training epoch 0, iteration 967/998 | lr: 2.803e-07 | global_batch_size: 16 | global_step: 967 | reduced_train_loss: 0.1777 | train_step_timing in s: 1.387 | consumed_samples: 15488 | val_loss: 0.2036
Training epoch 0, iteration 968/998 | lr: 2.631e-07 | global_batch_size: 16 | global_step: 968 | reduced_train_loss: 0.2235 | train_step_timing in s: 1.361 | consumed_samples: 15504 | val_loss: 0.2036
Training epoch 0, iteration 969/998 | lr: 2.464e-07 | global_batch_size: 16 | global_step: 969 | reduced_train_loss: 0.3314 | train_step_timing in s: 1.371 | consumed_samples: 15520 | val_loss: 0.2036
Training epoch 0, iteration 970/998 | lr: 2.302e-07 | global_batch_size: 16 | global_step: 970 | reduced_train_loss: 0.3669 | train_step_timing in s: 1.253 | consumed_samples: 15536 | val_loss: 0.2036
Training epoch 0, iteration 971/998 | lr: 2.146e-07 | global_batch_size: 16 | global_step: 971 | reduced_train_loss: 0.1286 | train_step_timing in s: 1.216 | consumed_samples: 15552 | val_loss: 0.2036
Training epoch 0, iteration 972/998 | lr: 1.996e-07 | global_batch_size: 16 | global_step: 972 | reduced_train_loss: 0.09057 | train_step_timing in s: 1.274 | consumed_samples: 15568 | val_loss: 0.2036
Training epoch 0, iteration 973/998 | lr: 1.851e-07 | global_batch_size: 16 | global_step: 973 | reduced_train_loss: 0.2478 | train_step_timing in s: 1.218 | consumed_samples: 15584 | val_loss: 0.2036
Training epoch 0, iteration 974/998 | lr: 1.711e-07 | global_batch_size: 16 | global_step: 974 | reduced_train_loss: 0.1696 | train_step_timing in s: 1.571 | consumed_samples: 15600 | val_loss: 0.2036
Training epoch 0, iteration 975/998 | lr: 1.577e-07 | global_batch_size: 16 | global_step: 975 | reduced_train_loss: 0.06268 | train_step_timing in s: 2.114 | consumed_samples: 15616 | val_loss: 0.2036
Training epoch 0, iteration 976/998 | lr: 1.449e-07 | global_batch_size: 16 | global_step: 976 | reduced_train_loss: 0.2469 | train_step_timing in s: 1.539 | consumed_samples: 15632 | val_loss: 0.2036
Training epoch 0, iteration 977/998 | lr: 1.325e-07 | global_batch_size: 16 | global_step: 977 | reduced_train_loss: 0.1516 | train_step_timing in s: 2.054 | consumed_samples: 15648 | val_loss: 0.2036
Training epoch 0, iteration 978/998 | lr: 1.208e-07 | global_batch_size: 16 | global_step: 978 | reduced_train_loss: 0.2575 | train_step_timing in s: 1.217 | consumed_samples: 15664 | val_loss: 0.2036
Training epoch 0, iteration 979/998 | lr: 1.095e-07 | global_batch_size: 16 | global_step: 979 | reduced_train_loss: 0.1225 | train_step_timing in s: 1.222 | consumed_samples: 15680 | val_loss: 0.2036
Training epoch 0, iteration 980/998 | lr: 9.887e-08 | global_batch_size: 16 | global_step: 980 | reduced_train_loss: 0.1723 | train_step_timing in s: 2.219 | consumed_samples: 15696 | val_loss: 0.2036
Training epoch 0, iteration 981/998 | lr: 8.874e-08 | global_batch_size: 16 | global_step: 981 | reduced_train_loss: 0.09216 | train_step_timing in s: 1.365 | consumed_samples: 15712 | val_loss: 0.2036
Training epoch 0, iteration 982/998 | lr: 7.916e-08 | global_batch_size: 16 | global_step: 982 | reduced_train_loss: 0.2915 | train_step_timing in s: 1.33 | consumed_samples: 15728 | val_loss: 0.2036
Training epoch 0, iteration 983/998 | lr: 7.012e-08 | global_batch_size: 16 | global_step: 983 | reduced_train_loss: 0.1752 | train_step_timing in s: 1.332 | consumed_samples: 15744 | val_loss: 0.2036
Training epoch 0, iteration 984/998 | lr: 6.163e-08 | global_batch_size: 16 | global_step: 984 | reduced_train_loss: 0.1463 | train_step_timing in s: 1.167 | consumed_samples: 15760 | val_loss: 0.2036
Training epoch 0, iteration 985/998 | lr: 5.369e-08 | global_batch_size: 16 | global_step: 985 | reduced_train_loss: 0.2961 | train_step_timing in s: 1.214 | consumed_samples: 15776 | val_loss: 0.2036
Training epoch 0, iteration 986/998 | lr: 4.629e-08 | global_batch_size: 16 | global_step: 986 | reduced_train_loss: 0.1715 | train_step_timing in s: 1.391 | consumed_samples: 15792 | val_loss: 0.2036
Training epoch 0, iteration 987/998 | lr: 3.945e-08 | global_batch_size: 16 | global_step: 987 | reduced_train_loss: 0.3391 | train_step_timing in s: 1.824 | consumed_samples: 15808 | val_loss: 0.2036
Training epoch 0, iteration 988/998 | lr: 3.315e-08 | global_batch_size: 16 | global_step: 988 | reduced_train_loss: 0.2228 | train_step_timing in s: 1.247 | consumed_samples: 15824 | val_loss: 0.2036
Training epoch 0, iteration 989/998 | lr: 2.739e-08 | global_batch_size: 16 | global_step: 989 | reduced_train_loss: 0.187 | train_step_timing in s: 1.333 | consumed_samples: 15840 | val_loss: 0.2036
Validation: iteration 1/0
Validation: iteration 2/0
Validation: iteration 3/0
Validation: iteration 4/0
Validation: iteration 5/0
Validation: iteration 6/0
Validation: iteration 7/0
Validation: iteration 8/0
Validation: iteration 9/0
Validation: iteration 10/0
Validation: iteration 11/0
Validation: iteration 12/0
Validation: iteration 13/0
Validation: iteration 14/0
Validation: iteration 15/0
Validation: iteration 16/0
Validation: iteration 17/0
Validation: iteration 18/0
Validation: iteration 19/0
Validation: iteration 20/0
Validation: iteration 21/0
Validation: iteration 22/0
Validation: iteration 23/0
Validation: iteration 24/0
Validation: iteration 25/0
Validation: iteration 26/0
Validation: iteration 27/0
Validation: iteration 28/0
Validation: iteration 29/0
Validation: iteration 30/0
Validation: iteration 31/0
Validation: iteration 32/0
Validation: iteration 33/0
Validation: iteration 34/0
[NeMo I 2025-10-07 02:37:55 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 990 : Start time: 1759804675.236s : Save duration: 0.556s
[NeMo I 2025-10-07 02:38:00 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=989-consumed_samples=15840.0-last.ckpt
Training epoch 0, iteration 990/998 | lr: 2.219e-08 | global_batch_size: 16 | global_step: 990 | reduced_train_loss: 0.2074 | train_step_timing in s: 1.365 | consumed_samples: 15856 | val_loss: 0.2036
[NeMo I 2025-10-07 02:38:01 nemo_logging:393] Async finalization time took 0.000 s
Training epoch 0, iteration 991/998 | lr: 1.753e-08 | global_batch_size: 16 | global_step: 991 | reduced_train_loss: 0.1774 | train_step_timing in s: 1.24 | consumed_samples: 15872 | val_loss: 0.2036
[NeMo I 2025-10-07 02:38:05 nemo_logging:393] Successfully saved checkpoint from iteration     990 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=989-consumed_samples=15840.0-last.ckpt
[NeMo I 2025-10-07 02:38:05 nemo_logging:393] Async checkpoint save for step 990 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=989-consumed_samples=15840.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:38:09 nemo_logging:393] Async finalization time took 4.404 s
Training epoch 0, iteration 992/998 | lr: 1.342e-08 | global_batch_size: 16 | global_step: 992 | reduced_train_loss: 0.3272 | train_step_timing in s: 1.249 | consumed_samples: 15888 | val_loss: 0.2036
Training epoch 0, iteration 993/998 | lr: 9.863e-09 | global_batch_size: 16 | global_step: 993 | reduced_train_loss: 0.3391 | train_step_timing in s: 3.68 | consumed_samples: 15904 | val_loss: 0.2036
Training epoch 0, iteration 994/998 | lr: 6.849e-09 | global_batch_size: 16 | global_step: 994 | reduced_train_loss: 0.07043 | train_step_timing in s: 1.866 | consumed_samples: 15920 | val_loss: 0.2036
Training epoch 0, iteration 995/998 | lr: 4.383e-09 | global_batch_size: 16 | global_step: 995 | reduced_train_loss: 0.1211 | train_step_timing in s: 1.272 | consumed_samples: 15936 | val_loss: 0.2036
Training epoch 0, iteration 996/998 | lr: 2.466e-09 | global_batch_size: 16 | global_step: 996 | reduced_train_loss: 0.2594 | train_step_timing in s: 1.203 | consumed_samples: 15952 | val_loss: 0.2036
Training epoch 0, iteration 997/998 | lr: 1.096e-09 | global_batch_size: 16 | global_step: 997 | reduced_train_loss: 0.2076 | train_step_timing in s: 1.362 | consumed_samples: 15968 | val_loss: 0.2036
Training epoch 0, iteration 998/998 | lr: 2.74e-10 | global_batch_size: 16 | global_step: 998 | reduced_train_loss: 0.3471 | train_step_timing in s: 1.23 | consumed_samples: 15984 | val_loss: 0.2036
`Trainer.fit` stopped: `max_steps=999` reached.
[NeMo I 2025-10-07 02:38:45 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 999 : Start time: 1759804724.696s : Save duration: 0.435s
[NeMo I 2025-10-07 02:38:48 nemo_logging:393] Scheduled async checkpoint save for /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=998-consumed_samples=15984.0-last.ckpt
[NeMo I 2025-10-07 02:38:48 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now
[NeMo I 2025-10-07 02:38:49 nemo_logging:393] Successfully saved checkpoint from iteration     999 to /data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=998-consumed_samples=15984.0-last.ckpt
[NeMo I 2025-10-07 02:38:49 nemo_logging:393] Async checkpoint save for step 999 (/data/nvidia-pipeline/finetuned-models/gemma2-9b-instruct-squad/default/2025-10-06_22-49-22/checkpoints/model_name=0--val_loss=0.20-step=998-consumed_samples=15984.0-last.ckpt) finalized successfully.
[NeMo I 2025-10-07 02:38:52 nemo_logging:393] Async finalization time took 4.002 s
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO comm 0x62be01d0 rank 0 nranks 1 cudaDev 0 busId 5000 - Destroy COMPLETE
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO comm 0x61887a80 rank 0 nranks 1 cudaDev 0 busId 5000 - Destroy COMPLETE
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO comm 0x64491bb0 rank 0 nranks 1 cudaDev 0 busId 5000 - Destroy COMPLETE
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO comm 0x62827e60 rank 0 nranks 1 cudaDev 0 busId 5000 - Destroy COMPLETE
nemo-gpu-lora-job-worker-0-0:76:76 [0] NCCL INFO comm 0x3c5f1210 rank 0 nranks 1 cudaDev 0 busId 5000 - Destroy COMPLETE
