# JobSet is a Kubernetes API for managing a group of jobs as a single unit.
# This manifest defines a JobSet to run a distributed deep learning job.
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: nemo-gpu-lora-job
  namespace: default
spec:
  # The number of replicated jobs to create. For a simple job, this can be 1.
  # For distributed training (e.g., with MPI), you would set this to the desired number of replicas.
  replicatedJobs:
    - name: worker
      replicas: 1
      template:
        spec:
          backoffLimit: 0
          template:
            metadata:
              annotations:
                gke-gcsfuse/volumes: "true"
            spec:

              # The imagePullSecrets field has been added to use the ngc-secret.
              imagePullSecrets:
                - name: ngc-secret

              restartPolicy: Never
              containers:
                - name: nemo-container
                  image: nvcr.io/nvidia/nemo:25.07.02
                  env:

                    - name: HF_TOKEN
                      value: "<<HF_TOKEN>>"
                    - name: NCCL_DEBUG
                      value: "INFO"
                    - name: HUGGING_FACE_HUB_TOKEN
                      value: "<<HF_TOKEN>>"
                    - name: MASTER_ADDR
                      value: "nemo-gpu-lora-job-worker-0-0.nemo-gpu-lora-job.default.svc.cluster.local"
                    - name: MASTER_PORT
                      value: "3389"
                    - name: MAX_STEPS
                      value: "50"
                    - name: NNODES
                      value: "1"
                    - name: WORLD_SIZE
                      value: "1"
                    - name: GPUS_PER_NODE
                      value: "1"
                    - name: NEMO_MODELS_CACHE
                      value: "/data/nvidia-pipeline/nemo-models/"
                    - name: NEMO_EXPERIMENT_NAME
                      value: "llama3-instruct-lora-squad"
                    - name: EXPERIMENT_ROOT_DIR
                      value: "/data/nvidia-pipeline/finetuned-models/llama3-8b-instruct-dolly"
                    - name: EXPLICIT_LOG_DIR
                      value: "/data/nvidia-pipeline/finetuned-models/llama3-8b-instruct-dolly"
                    - name: CUDA_DEVICE_MAX_CONNECTIONS
                      value: "1"
                    - name: GLOO_SOCKET_IFNAME
                      value: "eth0"

                  resources:
                    limits:
                      # This limit requests exactly 4 NVIDIA GPUs for the container.
                      nvidia.com/gpu: "1"
                    requests:
                      # It's good practice to set requests equal to limits for GPUs.
                      nvidia.com/gpu: "1"

                  # This volumeMounts section has been added to mount the PVC.
                  volumeMounts:

                    - name: gcs-fuse-csi-ephemeral
                      mountPath: /data

                    - name: dshm
                      mountPath: /dev/shm
                  #command: ["/bin/bash", "-c", "tail -f /dev/null"]

                  command: ["/bin/sh", "-c"]
                  args:
                    - |
                       torchrun \
                       --nproc-per-node=${GPUS_PER_NODE} \
                       --nnodes=${NNODES} \
                       --rdzv_backend=c10d \
                       --rdzv_id=123 \
                       --rdzv-endpoint=${MASTER_ADDR}:${MASTER_PORT} \
                       --master_addr="${MASTER_ADDR}" \
                       --master_port="${MASTER_PORT}" \
                        /usr/local/bin/nemo llm finetune -y --factory llama31_8b peft=lora data=squad trainer.devices=1 trainer.max_steps=${MAX_STEPS} trainer.num_nodes=${NNODES} log.log_dir=${EXPLICIT_LOG_DIR} \
                        trainer.max_steps="${MAX_STEPS}" trainer.devices="${GPUS_PER_NODE}"

              # This volumes section has been added to link the PVC to the Pod.
              volumes:

                - name: gcs-fuse-csi-ephemeral
                  csi:
                    driver: gcsfuse.csi.storage.gke.io
                    readOnly: false
                    volumeAttributes:
                      bucketName: pmotgi-g4-checkpoints
                      mountOptions: "implicit-dirs"

                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: "128Gi"