apiVersion: v1
kind: Pod
metadata:
  name: genai-perf-client
spec:
  
  imagePullSecrets:
    - name: ngc-secret
  containers:
  - name: perf-client-container
    image: nvcr.io/nvidia/vllm:25.09-py3
    env:
      - name: HF_TOKEN
        value: "<HF_TOKEN>"
    command: ["/bin/sh", "-c"]
    args:
      - |
           pip install genai-perf && \
           genai-perf profile --endpoint-type completions -m meta/llama-3.1-8b-instruct --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Llama-3.1-8B-Instruct && \
           genai-perf profile --endpoint-type completions -m llama3-8b-instruct-lora_vhf-squad-v2 --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Llama-3.1-8B-Instruct

---
#Sample output of how one can trigger benchmark on NIM endpoints.

#modelname- matches to model name within NIM and because there is a case difference, we need to pass tokenizer with actual model name from HF.

  #  76  genai-perf profile --endpoint-type completions -m meta/llama-3.1-8b-instruct --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Llama-3.1-8B-Instruct
  #  77  genai-perf profile --endpoint-type completions -m llama3-8b-instruct-lora_vhf-squad-v2 --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Llama-3.1-8B-Instruct


# Sample output

# root@genai-perf-client:/workspace# genai-perf profile --endpoint-type completions -m meta/llama-3.1-8b-instruct --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Llama-3.1-8B-
# Instruct
# [2025-09-22 18:45:41] INFO     Profiling these models: meta/llama-3.1-8b-instruct         create_config.py:58
# [2025-09-22 18:45:41] WARNING  Skipping unreachable metrics URL:                            subcommand.py:223
#                                http://localhost:9400/metrics                                                 
# [2025-09-22 18:45:41] INFO     Model name 'meta/llama-3.1-8b-instruct' cannot be  perf_analyzer_config.py:157
#                                used to create artifact directory. Instead,                                   
#                                'meta_llama-3.1-8b-instruct' will be used.                                    
# [2025-09-22 18:45:41] INFO     Creating tokenizer for: meta-llama/Llama-3.1-8B-Instruct     subcommand.py:190
# [2025-09-22 18:45:51] INFO     Running Perf Analyzer : 'perf_analyzer -m                     subcommand.py:98
#                                meta/llama-3.1-8b-instruct --async --stability-percentage 999                 
#                                --request-count 10 -i http -u                                                 
#                                nim-llama-3-1-8b-lora-service:8000 --concurrency-range 1                      
#                                --service-kind openai --endpoint v1/completions --input-data                  
#                                artifacts/meta_llama-3.1-8b-instruct-openai-completions-concu                 
#                                rrency1/inputs.json --profile-export-file                                     
#                                artifacts/meta_llama-3.1-8b-instruct-openai-completions-concu                 
#                                rrency1/profile_export.json'                                                  
# [2025-09-22 18:45:56] INFO     Loading response data from                           profile_data_parser.py:66
#                                'artifacts/meta_llama-3.1-8b-instruct-openai-complet                          
#                                ions-concurrency1/profile_export.json'                                        
# [2025-09-22 18:45:56] INFO     Parsing total 10 requests.                      llm_profile_data_parser.py:124
# Progress: 100%|███████████████████████████████████████████████████████| 10/10 [00:00<00:00, XX requests/s]
#                                NVIDIA GenAI-Perf | LLM Metrics                                
# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓
# ┃                            Statistic ┃    avg ┃    min ┃    max ┃    p99 ┃    p90 ┃    p75 ┃
# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩
# │             Time To First Token (ms) │ 
# │            Time To Second Token (ms) │ 
# │                 Request Latency (ms) │     Note: perf numbers will be reflected here  
# │             Inter Token Latency (ms) │ │         for your runs
# │     Output Token Throughput Per User │ │
# │                    (tokens/sec/user) │        │        │        │        │        │        │
# │      Output Sequence Length (tokens) │  │
# │       Input Sequence Length (tokens) │  │
# │ Output Token Throughput (tokens/sec) │  │
# │         Request Throughput (per sec) │  │
# │                Request Count (count) │ 
# └──────────────────────────────────────┴────────┴────────┴────────┴────────┴────────┴────────┘
# [2025-09-22 18:45:56] INFO     Generating                                                 json_exporter.py:64
#                                artifacts/meta_llama-3.1-8b-instruct-openai-completions-co                    
#                                ncurrency1/profile_export_genai_perf.json                                     
# [2025-09-22 18:45:56] INFO     Generating                                                  csv_exporter.py:75
#                                artifacts/meta_llama-3.1-8b-instruct-openai-completions-con                   
#                                currency1/profile_export_genai_perf.csv                                       
# root@genai-perf-client:/workspace# genai-perf profile --endpoint-type completions -m llama3-8b-instruct-lora_vhf-squad-v2 --streaming --concurrency 1 --url  nim-llama-3-1-8b-lora-service:8000 --tokenizer=meta-llama/Lla
# ma-3.1-8B-Instruct
# [2025-09-22 18:52:39] INFO     Profiling these models:                                    create_config.py:58
#                                llama3-8b-instruct-lora_vhf-squad-v2                                          
# [2025-09-22 18:52:39] WARNING  Skipping unreachable metrics URL:                            subcommand.py:223
#                                http://localhost:9400/metrics                                                 
# [2025-09-22 18:52:39] INFO     Creating tokenizer for: meta-llama/Llama-3.1-8B-Instruct     subcommand.py:190
# [2025-09-22 18:52:51] INFO     Running Perf Analyzer : 'perf_analyzer -m                     subcommand.py:98
#                                llama3-8b-instruct-lora_vhf-squad-v2 --async                                  
#                                --stability-percentage 999 --request-count 10 -i http -u                      
#                                nim-llama-3-1-8b-lora-service:8000 --concurrency-range 1                      
#                                --service-kind openai --endpoint v1/completions --input-data                  
#                                artifacts/llama3-8b-instruct-lora_vhf-squad-v2-openai-complet                 
#                                ions-concurrency1/inputs.json --profile-export-file                           
#                                artifacts/llama3-8b-instruct-lora_vhf-squad-v2-openai-complet                 
#                                ions-concurrency1/profile_export.json'                                        
# [2025-09-22 18:52:58] INFO     Loading response data from                           profile_data_parser.py:66
#                                'artifacts/llama3-8b-instruct-lora_vhf-squad-v2-open                          
#                                ai-completions-concurrency1/profile_export.json'                              
# [2025-09-22 18:52:58] INFO     Parsing total 10 requests.                      llm_profile_data_parser.py:124
# Progress: 100%|███████████████████████████████████████████████████████| 10/10 [00:00<00:00, XXrequests/s]
#                                  NVIDIA GenAI-Perf | LLM Metrics                                  
# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓
# ┃                            Statistic ┃    avg ┃    min ┃      max ┃      p99 ┃    p90 ┃    p75 ┃
# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩
# │             Time To First Token (ms) │  │
# │            Time To Second Token (ms) │   │
# │                 Request Latency (ms) │  │  Note:
# │             Inter Token Latency (ms) │   │ perf numbers in the table will be reflected as per
# │     Output Token Throughput Per User │ │   your runs
# │                    (tokens/sec/user) │        │        │          │          │        │        │
# │      Output Sequence Length (tokens) │  │
# │       Input Sequence Length (tokens) │  │
# │ Output Token Throughput (tokens/sec) │   
# │         Request Throughput (per sec) │                                                         │
# │                Request Count (count) │                                                         │
# └──────────────────────────────────────┴────────┴────────┴──────────┴──────────┴────────┴────────┘
# [2025-09-22 18:52:58] INFO     Generating                                                 json_exporter.py:64
#                                artifacts/llama3-8b-instruct-lora_vhf-squad-v2-openai-comp                    
#                                letions-concurrency1/profile_export_genai_perf.json                           
# [2025-09-22 18:52:58] INFO     Generating                                                  csv_exporter.py:75
#                                artifacts/llama3-8b-instruct-lora_vhf-squad-v2-openai-compl                   
#                                etions-concurrency1/profile_export_genai_perf.csv                           